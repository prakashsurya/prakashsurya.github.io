<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>www.prakashsurya.com</title>
    <link>https://www.prakashsurya.com/</link>
    <description>Recent content on www.prakashsurya.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Apr 2018 00:00:00 -0800</lastBuildDate>
    
	<atom:link href="https://www.prakashsurya.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Ubuntu-Based Appliance Build Architecture</title>
      <link>https://www.prakashsurya.com/post/2018-04-23-ubuntu-based-appliance-build-architecture/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2018-04-23-ubuntu-based-appliance-build-architecture/</guid>
      <description>Agenda 1. Overview of current illumos-based build architecture. 2. Problems with current build architecture. 3. Goals of new Ubuntu-based build architecture. 4. Overview of new build architecture. 5. Details of new build architecture. .footnote[*Press &amp;ldquo;p&amp;rdquo; for notes, and &amp;ldquo;c&amp;rdquo; for split view.]
class: middle, center
1 &amp;ndash; Overview of current build architecture Overview of current build architecture  Converts source code, into virtual machine (VM) artifacts
 OVA, VHD, qcow2, etc.</description>
    </item>
    
    <item>
      <title>Principles of OS Debugging</title>
      <link>https://www.prakashsurya.com/post/2017-12-01-principles-of-os-debugging/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-12-01-principles-of-os-debugging/</guid>
      <description>The most important bugs occur at customer sites, cause downtime for mission-critical machines, and cannot be deterministically reproduced
 Customers should not and will not perform experiments and debugging tasks for us
 We must be able to perform root-cause analysis from a single crash dump and deliver an appropriate fix
 Debugging support for new subsystems is required at the same time as project integration
  The above points were taken from slide 5, of Mike Shapiro&amp;rsquo;s presentation &amp;ldquo;A Brief Tour of the Modular Debugger&amp;rdquo; ca.</description>
    </item>
    
    <item>
      <title>Performance Testing Results for ZFS on Linux #6566</title>
      <link>https://www.prakashsurya.com/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/</guid>
      <description>The following are links to the Jupyter notebooks that describe the performance testing that I did for ZFS on Linux #6566, and the results of that testing:
 Max Rate Submit on HDDs Max Rate Submit on SSDs Fixed Rate Submit on HDDs Fixed Rate Submit on SSDs  Additionally, a compressed tarball with all the raw data used to generate those Jupyter notebooks can be found here.</description>
    </item>
    
    <item>
      <title>ZIL Performance: How I Doubled Sync Write Speed</title>
      <link>https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/</guid>
      <description>Agenda 1. What is the ZIL? 2. How is it used? How does it work? 3. The problem to be fixed; the solution. 4. Details on the changes I made. 5. Performance testing and results. .footnote[*Press &amp;ldquo;p&amp;rdquo; for notes, and &amp;ldquo;c&amp;rdquo; for split view.]
???
 Here&amp;rsquo;s a brief overview of what I plan to discuss.
 It&amp;rsquo;s broken up into roughly 3 parts:
 First I&amp;rsquo;ll give some background, and discuss:</description>
    </item>
    
    <item>
      <title>Generating Code Coverage Reports for ZFS on Linux</title>
      <link>https://www.prakashsurya.com/post/2017-09-28-generating-code-coverage-reports-for-zfs-on-linux/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-28-generating-code-coverage-reports-for-zfs-on-linux/</guid>
      <description>Introduction This is another post about collecting code coverage data for the ZFS on Linux project. We&amp;rsquo;ve recently added a new make target to the project, so I wanted to highlight how easy it is to use this to generate static HTML based code coverage reports, and/or to generate a report that can be used with other tools and services (e.g. codecov.io).
Examples Before I get into the specifics for how to run the tests and generate the coverage report, I want to show off the results.</description>
    </item>
    
    <item>
      <title>ZFS on Linux Code Coverage</title>
      <link>https://www.prakashsurya.com/post/2017-09-26-zfs-on-linux-code-coverage/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-26-zfs-on-linux-code-coverage/</guid>
      <description>Branches + Pull Requests  Code coverage data is collected for:
 All commits merged to a branch (e.g. master)
 All pull requests for the &amp;ldquo;zfs&amp;rdquo; project
  Code coverage collected after running all tests
 ztest, zfstest, zfsstress, etc.  Data generated using make code-coverage-capture &amp;hellip;
 Emits .info file and static HTML pages  .info file uploaded to codecov.io
 ZFS on Linux + Codecov</description>
    </item>
    
    <item>
      <title>Python &#43; Jupyter for Performance Testing</title>
      <link>https://www.prakashsurya.com/post/2017-09-19-python-plus-jupyter-for-performance-testing/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-19-python-plus-jupyter-for-performance-testing/</guid>
      <description>Setting the stage.  Working on performance improvement to ZFS (sync writes)
 To verify my changes, I needed to:
 Measure the performance of the system without my changes.
 Measure the performance of the system with my changes.
 Analyze the difference(s) in performance with and without my changes.
 Collect tangential information from the system, to support (or refute) my conclusions.
   Visualizations required?  While not strictly required, visualizations are often powerful.</description>
    </item>
    
    <item>
      <title>Code Coverage for ZFS on Linux</title>
      <link>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</link>
      <pubDate>Mon, 18 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</guid>
      <description>I&amp;rsquo;ve been working with Brian Behlendorf on getting code coverage information for the ZFS on Linux. The goal was to get code coverage data for pull requests, as well as branches; this way, we can get a sense of how well tested any given PR is by the automated tests, prior to landing it. There&amp;rsquo;s still some wrinkles that need to be ironed out, but we&amp;rsquo;ve mostly achieved that goal by leveraging codecov.</description>
    </item>
    
    <item>
      <title>Using &#34;gcov&#34; with ZFS on Linux Kernel Modules</title>
      <link>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</guid>
      <description>Building a &amp;ldquo;gcov&amp;rdquo; Enabled Linux Kernel In order to extract &amp;ldquo;gcov&amp;rdquo; data from the Linux kernel, and/or Linux kernel modules, a &amp;ldquo;gcov&amp;rdquo; enabled Linux kernel is needed. Since my current development environment is based on Ubuntu 17.04, and the fact that Ubuntu doesn&amp;rsquo;t provide a pre-built kernel with &amp;ldquo;gcov&amp;rdquo; enabled, I had to build the kernel from source. This was actually pretty simple, and most of that process is already documented here.</description>
    </item>
    
    <item>
      <title>Performance Testing Results for OpenZFS #447</title>
      <link>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</guid>
      <description>The following are links to the Jupyter notebooks that describe the performance testing that I did for OpenZFS #447, and the results of that testing:
 Max Rate Submit on HDDs Max Rate Submit on SSDs Fixed Rate Submit on HDDs Fixed Rate Submit on SSDs  Additionally, a compressed tarball with all the raw data used to generate those Jupyter notebooks can be found here.</description>
    </item>
    
    <item>
      <title>Using Python and Jupyter for Performance Testing and Analysis</title>
      <link>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</guid>
      <description>Introduction I recently worked on some changes to the OpenZFS ZIL (see here), and in the context of working on that project, I discovered some new tools that helped me run my performance tests and analyze their results. What follows is some notes on the tools that I used, and how I used them.
Quick Overview Before I dive into the details of how I used these tools, I wanted to quickly go over what the tools were:</description>
    </item>
    
    <item>
      <title>Building and Using &#34;crash&#34; on Ubuntu 16.04</title>
      <link>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</guid>
      <description>Introduction I&amp;rsquo;ve been working on the ZFS on Linux project recently, and had a need to use crash on the Ubuntu 16.04 based VM I was using. The following is some notes regarding the steps I had to take, in order to build, install, and ultimately run the utility against the &amp;ldquo;live&amp;rdquo; system.
Build and Install &amp;ldquo;crash&amp;rdquo; First, I had to install the build dependencies:
$ sudo apt-get install -y \ git build-essential libncurses5-dev zlib1g-dev bison  Then I could checkout the source code, build, and install:</description>
    </item>
    
    <item>
      <title>Using BCC&#39;s &#34;trace&#34; Instead of &#34;printk&#34;</title>
      <link>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</guid>
      <description>Introduction Recently I&amp;rsquo;ve been working on porting some changes that I made to the OpenZFS ZIL over to the ZFS on Linux codebase; see here for the OpenZFS pull request, and here for the ZFS on Linux pull request.
In my initial port, I was running into a problem where the automated tests would trigger a &amp;ldquo;hang&amp;rdquo; as a result of the readmmap program calling msync:
$ pstree -p 2337 test-runner.</description>
    </item>
    
    <item>
      <title>OpenZFS: Isolating ZIL Disk Activity</title>
      <link>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</guid>
      <description>I recently completed a project to improve the performance of the OpenZFS ZIL (see here for more details); i.e. improving the performance of synchronous activity on OpenZFS, such as writes using the O_SYNC flag. As part of that work, I had to run some performance testing and benchmarking of my code changes (and the system as a whole), to ensure the system was behaving as I expected.
Early on in my benchmarking exercises, I became confused by the data that I was gathering.</description>
    </item>
    
    <item>
      <title>Links</title>
      <link>https://www.prakashsurya.com/link/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/link/</guid>
      <description>OpenZFS Related  17 Oct 2017 &amp;raquo; Bryan Cantrill &amp;raquo; ARC after dark 21 Apr 2017 &amp;raquo; Serapheim Dimitropoulos &amp;raquo; ZFS Storage Pool Checkpoint 12 Apr 2015 &amp;raquo; Ryan Zezeski &amp;raquo; More Efficient TLB Shootdowns 06 Jun 2014 &amp;raquo; Matt Ahrens &amp;raquo; ZFS RAIDZ stripe width 13 Dec 2012 &amp;raquo; Adam Leventhal &amp;raquo; ZFS fundamentals: transaction groups 06 Oct 2008 &amp;raquo; George Wilson &amp;raquo; Jack Adams Interviews George Wilson on ZFS 15 Dec 2008 &amp;raquo; Matt Ahrens &amp;raquo; New Scrub Code ?</description>
    </item>
    
    <item>
      <title>Running `sshd` on Windows using Cygwin</title>
      <link>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</guid>
      <description>Introduction As part of our effort to support Delphix in the Azure cloud environment, we&amp;rsquo;re writing some automation to convert our .iso install media into a VHD image, leveraging Jenkins and Packer in the process.
Essentially, we want to use a Windows server as a Jenkins &amp;ldquo;slave&amp;rdquo;, and run Packer from within a Jenkins job that will run on that Windows system.
In order to do that, the Jenkins &amp;ldquo;master&amp;rdquo; needs to connect with the Windows system, such that it can configure the system to act as a Jenkins slave.</description>
    </item>
    
    <item>
      <title>OpenZFS: Notes on ZIL Transactions</title>
      <link>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</guid>
      <description>Introduction The OpenZFS Intent Log (ZIL) is used to ensure POSIX compliance of certain system calls (that modify the state of a ZFS dataset), and protect against data loss in the face of failure scenarios such as: an operating system crash, power loss, etc. Specifically, it&amp;rsquo;s used as a performance optimization so that applications can be assured that their given system call, and any &amp;ldquo;user data&amp;rdquo; associated with it, will not be &amp;ldquo;lost&amp;rdquo;, without having to wait for an entire transaction group (TXG) to be synced out (which can take on the order of seconds, on a moderately loaded system).</description>
    </item>
    
    <item>
      <title>OpenZFS: Refresher on `zpool reguid` Using Examples</title>
      <link>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</guid>
      <description>Introduction The zpool reguid command can be used to regenerate the GUID for an OpenZFS pool, which is useful when using device level copies to generate multiple pools all with the same contents.
Example using File VDEVs As a contrived example, lets create a zpool backed by a single file vdev:
# mkdir /tmp/tank1 # mkfile -n 256m /tmp/tank1/vdev # zpool create tank1 /tmp/tank1/vdev # zpool list tank1 NAME SIZE ALLOC FREE EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT tank1 240M 78K 240M - 1% 0% 1.</description>
    </item>
    
    <item>
      <title>Creating a Custom Amazon EC2 AMI from ISO (using OI Hipster)</title>
      <link>https://www.prakashsurya.com/post/2017-02-06-creating-a-custom-amazon-ec2-ami-from-iso/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-06-creating-a-custom-amazon-ec2-ami-from-iso/</guid>
      <description>Preface In this post, I&amp;rsquo;ll pick up from where I left off last time, and demonstrate one potential way to convert the installation ISO media generated in that post, into an AMI that can be used to create new VMs in the Amazon EC2 environment. It&amp;rsquo;s important to note a couple things before we start:
 While I&amp;rsquo;ll be generating an AMI based on OI Hipster, this process should be applicable to any Linux or FreeBSD based operating system as well (and quite possibly Windows too, but I don&amp;rsquo;t know much about that platform).</description>
    </item>
    
    <item>
      <title>Creating Custom Installation Media for OI Hipster</title>
      <link>https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/</guid>
      <description>Preface This post is a write up of my notes for creating custom installation media for OpenIndiana Hipster, using a custom/patched version of illumos. It assumes that OI Hipster has already been installed on a machine (e.g. installed on a VM using their provided installation media); and this server will be used to build our custom version of illumos, as well as the custom OI installation media. The goal of this exercise is to create a &amp;ldquo;Live DVD&amp;rdquo; that can be used to install our custom version of illumos.</description>
    </item>
    
    <item>
      <title>Effective Communication of Code Changes</title>
      <link>https://www.prakashsurya.com/post/2016-03-22-effective-communication-of-code-changes/</link>
      <pubDate>Tue, 22 Mar 2016 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2016-03-22-effective-communication-of-code-changes/</guid>
      <description>Agenda 1. What&amp;rsquo;s the problem? 2. Story Time 3. My Proposed Solution .footnote[*Press &amp;ldquo;p&amp;rdquo; for presenter&amp;rsquo;s notes, and &amp;ldquo;c&amp;rdquo; for split view.]
???
Agenda  Give a brief introduction of the topics that will be discussed.  Code is our Enemy 
&amp;gt; Code is bad. It rots. It requires periodic maintenance. It has bugs that &amp;gt; need to be found. New features mean old code has to be adapted.</description>
    </item>
    
    <item>
      <title>OpenZFS: Artificial Disk Latency Using zinject</title>
      <link>https://www.prakashsurya.com/post/2015-12-07-openzfs-artificial-disk-latency-using-zinject/</link>
      <pubDate>Mon, 07 Dec 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-12-07-openzfs-artificial-disk-latency-using-zinject/</guid>
      <description>About a year ago I had the opportunity to work on a small extension to the OpenZFS zinject command with colleagues Matt Ahrens and Frank Salzmann, during one of our Delphix engineering wide hackathon events. Now that it&amp;rsquo;s in the process of landing in the upstream OpenZFS repository, I wanted to take a minute to show it off.
To describe the new functionality, I&amp;rsquo;ll defer to the help message:</description>
    </item>
    
    <item>
      <title>OpenZFS: Reducing ARC Lock Contention</title>
      <link>https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/</link>
      <pubDate>Mon, 23 Mar 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/</guid>
      <description>tl;dr; Cached random read performance of 8K blocks was improved by 225% by reducing internal lock contention within the OpenZFS ARC on illumos.
Introduction Locks are a pain. Even worse, is a single lock serializing all of the page cache accesses for a filesystem. While that&amp;rsquo;s not quite the situation the OpenZFS ARC was in earlier this year, it was pretty close.
For those unfamiliar, the OpenZFS file system is built atop its very own page cache based on the paper by Nimrod Megiddo and Dharmendra S.</description>
    </item>
    
    <item>
      <title>OpenZFS Developer Summit 2014: OpenZFS on illumos</title>
      <link>https://www.prakashsurya.com/post/2015-01-06-openzfs-developer-summit-2014-openzfs-on-illumos/</link>
      <pubDate>Tue, 06 Jan 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-01-06-openzfs-developer-summit-2014-openzfs-on-illumos/</guid>
      <description>The OpenZFS project is growing! The second annual OpenZFS developer summit concluded just under two months ago, and overall I thought it went very well. There was roughly 70 attendees, twice as many as the previous year, and the talks given were very engaging and interesting.
I gave a short talk about ZFS on the illumos platform, and also touched briefly on some of my subjective opinions coming from a ZFS on Linux background.</description>
    </item>
    
    <item>
      <title>OpenZFS on illumos</title>
      <link>https://www.prakashsurya.com/post/2014-11-10-openzfs-on-illumos/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2014-11-10-openzfs-on-illumos/</guid>
      <description>Where OpenZFS Originated  2001 &amp;ndash; Started at Sun
 2005 &amp;ndash; Released through OpenSolaris
 2010 &amp;ndash; illumos spawned; fork of OpenSolaris
 2013 &amp;ndash; OpenZFS created
 OpenZFS&amp;rsquo;s &amp;ldquo;home&amp;rdquo; is in illumos:
 Due to history, but also OS integration: grub, mdd, fma, etc  But OpenZFS is growing beyond illumos
  Development model on illumos  Committer access is granted to &amp;ldquo;advocates&amp;rdquo;
 Advocates rely on &amp;ldquo;reviewers&amp;rdquo; to verify changes</description>
    </item>
    
  </channel>
</rss>