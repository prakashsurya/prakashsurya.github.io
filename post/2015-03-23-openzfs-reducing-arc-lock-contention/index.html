<!DOCTYPE html>
<html>
  <head>
    <meta name="generator" content="Hugo 0.18.1" />
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="canonical" href="https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/">
<link rel="stylesheet" type="text/css" href="/css/hack.css">
<link rel="stylesheet" type="text/css" href="/css/custom.css">

<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-91378771-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


    <title>
  OpenZFS: Reducing ARC Lock Contention &raquo; www.prakashsurya.com
</title>
  </head>
  <body class="hack container">
    <header>
      <nav>
  
    <a class="active" href="/">Home</a>
  
    <a class="active" href="/post/">Posts</a>
  
    <a class="active" href="/link/">Links</a>
  
</nav>

    </header>
    <main>
  <h1>OpenZFS: Reducing ARC Lock Contention</h1>
  <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#real-world-problems-with-the-arc">Real World Problems with the ARC</a></li>
<li><a href="#performance-before">Performance: Before</a></li>
<li><a href="#remove-the-lists">Remove the Lists?</a></li>
<li><a href="#solution-use-more-locks">Solution: Use More Locks!</a></li>
<li><a href="#performance-after">Performance: After</a></li>
<li><a href="#commit-details">Commit Details</a></li>
<li><a href="#appendix">Appendix</a></li>
</ul></li>
</ul>
</nav>
  

<p><strong>tl;dr;</strong> Cached random read performance of 8K blocks was improved by
225% by reducing internal lock contention within the OpenZFS ARC on
illumos.</p>

<h2 id="introduction">Introduction</h2>

<p>Locks are a pain. Even worse, is a single lock serializing all of the
page cache accesses for a filesystem. While that&rsquo;s not <em>quite</em> the
situation the OpenZFS ARC was in earlier this year, it was pretty close.</p>

<p>For those unfamiliar, the OpenZFS file system is built atop its very own
page cache based on the paper by Nimrod Megiddo and Dharmendra S. Modha,
&ldquo;ARC: A Self-Tuning, Low Overhead Replacement Cache&rdquo;. While the paper
provides a very good basis from which actual code can be written to
implement its concepts, there&rsquo;s a number of real world problems it fails
to address. For this article, we&rsquo;ll be looking at one of these problems:
concurrency.</p>

<h2 id="real-world-problems-with-the-arc">Real World Problems with the ARC</h2>

<p>The paper describes a global data structure from which all page accesses
are performed; the problem is, it doesn&rsquo;t describe a mechanism to do
this safely in a highly concurrent environment. Since any modern file
system obviously needs to support multi-threaded concurrent operations,
locks had to be added to protect the global data used to manage and
implement the ARC. How might one do this?</p>

<p>One way is to is to wrap all operations modifying the global structures
with a single mutually exclusive lock. This satisfies the requirement
that all accesses are safe to perform concurrently from many threads,
while also be fairly simple to understand and implement. To my surprise,
this is how the OpenZFS ARC was structured for nearly the past decade</p>

<h2 id="performance-before">Performance: Before</h2>

<p>As one might expect, the main drawback with this simplistic approach is
performance. While safe, on systems with many CPUs, this approach
causes terrible lock contention for multi-threaded cached workloads. To
highlight this performance issue, I used FIO to perform a cached random
read workload using 32 threads. Each thread would use its own unique
file, reading 8K blocks at random 8K aligned offsets from its file
[<a href="#appendix-1">Appendix 1</a>]. The aggregate bandwidth was measured while
varying the number of virtual CPUs allocated to the virtual machine,
and the results are as follows:</p>

<table>
<thead>
<tr>
<th># of CPUS</th>
<th>Bandwidth</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>840893 KB/s</td>
</tr>

<tr>
<td>2</td>
<td>1559.5 MB/s</td>
</tr>

<tr>
<td>4</td>
<td>3022.1 MB/s</td>
</tr>

<tr>
<td>8</td>
<td>5104.9 MB/s</td>
</tr>

<tr>
<td>16</td>
<td>3854.3 MB/s</td>
</tr>

<tr>
<td>32</td>
<td>3036.2 MB/s</td>
</tr>
</tbody>
</table>

<p>See [<a href="#appendix-2">Appendix 2</a>] for more detailed performance metrics.</p>

<p>As one can see, the performance scales terribly when increasing the
number of CPUs in the system. Not only does performance fail to increase
with more CPUs, it actually gets much worse with more CPUs! Clearly this
is a bad situation to be in; especially so, with large CPU count systems
becoming increasingly more common.</p>

<p>Looking at a snippet of lockstat profiling data, it&rsquo;s clear which locks
are to blame (the following was taken from a run on a system with 32
CPUs):</p>

<pre><code>Count indv cuml rcnt     nsec Lock                   Caller
-------------------------------------------------------------------------------
4191777  47%  47% 0.00    67709 ARC_mfu+0x58           remove_reference+0x63
4103160  46%  92% 0.00    75366 ARC_mfu+0x58           add_reference+0x8d
   1877   0%  92% 0.00   436330 buf_hash_table+0x750   arc_buf_add_ref+0x6a
</code></pre>

<p>The two locks causing the bottleneck are used to protect the ARC&rsquo;s
internal lists. These lists contain all ARC buffers residing in the
cache that aren&rsquo;t actively used by consumers of the ARC (i.e. buffers
with a reference count of zero). Every time a buffer is added or removed
from one of these lists, the list&rsquo;s lock must be held. This means the
lists&rsquo; lock must be acquired for all of the following operations: a read
from disk, an ARC cache hit, when evicting a buffer, when dirtying a
buffer, when all references to a buffer are released by consumers, etc.</p>

<h2 id="remove-the-lists">Remove the Lists?</h2>

<p>Before I dive into the solution that was taken, I should make it clear
why the lists are needed in the first place. These lists allow for
iterating over the buffers from the least recently accessed buffer to
the most recently accessed buffer, or vice versa. The ability to do this
iteration is critical to implementing the ARC as described by the paper.
It allows for the least recently accessed buffer to be selected in
constant-time during the eviction process [<a href="#appendix-3">Appendix 3</a>].</p>

<h2 id="solution-use-more-locks">Solution: Use More Locks!</h2>

<p>To alleviate the issue, we really needed a way to perform this iteration
without causing so much locking overhead to the performance critical
code paths (e.g. during a ARC cache hit). The solution we decided on is
to split each of the ARC&rsquo;s internal lists into a variable number of
sublists, each sublist having its own lock and maintaining a time
ordering within itself (i.e. the buffers in each sublist are ordered
based on access time, but there&rsquo;s no ordering of buffers between the
different sublists). Thus, a new data structure was created to
encapsulate these semantics, a &ldquo;multilist&rdquo; [<a href="#appendix-4">Appendix 4</a>].</p>

<p>On the surface this seems like a pretty trivial change, but there&rsquo;s a
key aspect that make it more complex to implement than one might expect.
Due to the sublists not maintaining a strict ordering between each
other, selecting the least (or most) recently accessed buffer is no
longer a constant-time operation. In general, it becomes a linear-time
operation as each sublist must be iterated, comparing the elements in
each sublists with each other.</p>

<p>A linear-time lookup of the least recently accessed buffer is
insufficient, so a clever domain-specific workaround was used to allow
for this lookup to maintain its constant-time complexity. Rather than
selecting the absolute least recently accessed buffer during eviction,
a buffer that is &ldquo;old enough&rdquo; is selected instead; this selection
happening in constant-time.</p>

<p>We do this by evenly inserting buffers into each sublist, and then
during eviction, a randomly selected sublist is chosen. This sublist&rsquo;s
least recently accessed buffer is selected and evicted. This process
will select a buffer that&rsquo;s &ldquo;old enough&rdquo; based on the fact that buffers
are evenly inserted into each of the sublists, so each sublist&rsquo;s least
recently accessed buffer will have an access time proportional to all
other sublists&rsquo; least recently accessed buffer [<a href="#appendix-5">Appendix
5</a>].</p>

<h2 id="performance-after">Performance: After</h2>

<p>As with any performance related change, empirical results are necessary
to give the change any sort of credibility. So, the workload that was
previously used to exemplify the problem [<a href="#appendix-1">Appendix 1</a>],
was used again to ensure the expected performance gains were achieved.
The following table contains aggregate bandwidth numbers with the fix in
place (&ldquo;Bandwidth After&rdquo; column), as well as the aggregate bandwidth
numbers without the fix (&ldquo;Bandwidth Before&rdquo; column, the same results
previously shown):</p>

<table>
<thead>
<tr>
<th># of CPUS</th>
<th>Bandwidth After</th>
<th>Bandwidth Before</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>833163 KB/s</td>
<td>840893 KB/s</td>
</tr>

<tr>
<td>2</td>
<td>1520.2 MB/s</td>
<td>1559.5 MB/s</td>
</tr>

<tr>
<td>4</td>
<td>2985.3 MB/s</td>
<td>3022.1 MB/s</td>
</tr>

<tr>
<td>8</td>
<td>5913.2 MB/s</td>
<td>5104.9 MB/s</td>
</tr>

<tr>
<td>16</td>
<td>8938.9 MB/s</td>
<td>3854.3 MB/s</td>
</tr>

<tr>
<td>32</td>
<td>9896.6 MB/s</td>
<td>3036.2 MB/s</td>
</tr>
</tbody>
</table>

<p>As expected, performance scales much better as more CPUs are added to
the system, and the overall performance saw a 225% improvement when
using 32 CPUs (the largest CPU count I had available to test). Also,
the performance difference when using a small number of CPUs was
negligible.</p>

<p>Lockstat was used again, and contention is clearly much less than before
(as before, this snippet was taken from a run on a system with 32 CPUs):</p>

<pre><code>Count indv cuml rcnt     nsec Lock                   Caller
------------------------------------------------------------------------------
142396   3%   3% 0.00     2785 0xffffff09168112b0     rrw_exit+0x23
136499   3%   6% 0.00     3012 0xffffff09168112b0     rrw_enter_read+0x2
 82009   2%   8% 0.00     3359 0xffffff09168110d0     rrw_enter_read+0x27
</code></pre>

<p>Success!</p>

<h2 id="commit-details">Commit Details</h2>

<p>For those interested, this fix was committed to illumos on January 12th,
2015 via this commit:</p>

<pre><code>commit 244781f10dcd82684fd8163c016540667842f203
Author:     Prakash Surya
AuthorDate: Mon Jan 12 19:52:19 2015 -0800
Commit:     Christopher Siden
CommitDate: Mon Jan 12 19:52:19 2015 -0800

    5497 lock contention on arcs_mtx
    Reviewed by: George Wilson
    Reviewed by: Matthew Ahrens
    Reviewed by: Richard Elling
    Approved by: Dan McDonald
</code></pre>

<p>It&rsquo;s also in the process of being ported and merged into FreeBSD and
OpenZFS on Linux (at least, it was at the time of this writing).</p>

<h2 id="appendix">Appendix</h2>

<ol>
<li><p><a name="appendix-1"></a>The FIO script used to run the performance
benchmarks is below. Keep in mind, this script was run a number of
times prior to gathering the performance data, as I was specifically
targeting a fully cached workload and needed to ensure the files
were cached in the ARC.</p>

<pre><code>[global]
group_reporting=1
fallocate=none
ioengine=psync
numjobs=32
iodepth=1
bs=8k
filesize=512m
size=512m
randrepeat=0
use_os_rand=1

[32threads]
rw=randread
time_based
runtime=1m
directory=/tank/fish
</code></pre></li>

<li><p><a name="appendix-2"></a>In addition to the run used to gather the
aggregate bandwidth numbers, the FIO script was run a couple more
times to gather more useful debugging and performance metrics.
Specifically, it was run with lockstat profiling enabled, which was
used to pinpoint the lock(s) causing the poor scaling. As well as
another iteration with flame graph profiling enabled to pinpoint the
specific functions of interest (additionally to provide
corroborating evidence to back up the lockstat data). Unfortunately,
it looks like I can&rsquo;t upload SVG or TAR files; so if anybody is
interested in the full lockstat output or the flamegraphs, drop me
an email and we can work something out (probably just email the TAR
archive directly as it&rsquo;s only 227K compressed).</p></li>

<li><p><a name="appendix-3"></a>The lists can also be iterated starting
from the most recently accessed buffer to the least recently
accessed buffer too. This iteration direction is used when
populating the L2ARC device prior to the cache being &ldquo;warmed up&rdquo;.</p></li>

<li><p><a name="appendix-4"></a>This concept is not original to me, as
FreeBSD has included a similar fix to the ARC in their platform for
some time. FreeBSD&rsquo;s usage of this concept actually gave me
confidence that it would work out in practice, prior to actually
having a working solution.</p></li>

<li><p><a id="appendix-5"></a>I realize &ldquo;old enough&rdquo; is vague, but
essentially what we&rsquo;re doing here is using a heuristic to determine
what buffer would be the best candidate to be evicted from the
cache. So, we&rsquo;re modifying the previous heuristic from always
selecting the absolutely oldest buffer, to being a randomly selected
buffer out of a group of &ldquo;old&rdquo; buffers. The group of old buffers
being composed of the tails of all the sublists.</p></li>
</ol>

</main>
    <footer>
  <hr />

<div class="footer">
  <div id="footer-left">
    <a href="mailto:me@prakashsurya.com">me@prakashsurya.com</a>
  </div>

  <div id="footer-center">
    Published: 23 Mar 2015
  </div>

  <div id="footer-right">
    Last Modified: 23 Mar 2015
  </div>
</div>

</footer>
  </body>
</html>
