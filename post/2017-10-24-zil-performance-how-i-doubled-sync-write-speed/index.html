<!DOCTYPE html>
<html>
  <head>
    <meta name="generator" content="Hugo 0.18.1" />
<meta charset="utf-8">
<link rel="canonical" href="https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/">
<script src="/js/mermaid.min.js"></script>
<link rel="stylesheet" href="/css/mermaid.css">
<script src="/js/remark.min.js"></script>
<style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
  @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
  @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

  body { font-family: 'Droid Serif'; }
  h1, h2, h3 {
    font-family: 'Yanone Kaffeesatz';
    font-weight: 400;
    margin-bottom: 0;
  }

  .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
  .remark-slide-content h1 { font-size: 3em; }
  .remark-slide-content h2 { font-size: 2em; }
  .remark-slide-content h3 { font-size: 1.6em; }
  .remark-slide-content a, a > code { text-decoration: none; }
  .footnote { position: absolute; bottom: 3em; font-size: .6em; }
  .pull-left { float: left; width: 47%; }
  .pull-right { float: right; width: 47%; }
  .pull-right ~ p { clear: both; }

   

  @page {
     
    size: 908px 681px;
    margin: 0;
  }

  @media print {
    .remark-slide-scaler {
      width: 100% !important;
      height: 100% !important;
      transform: scale(1) !important;
      top: 0 !important;
      left: 0 !important;
    }
  }
</style>

<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-91378771-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


    <title>ZIL Performance: How I Doubled Sync Write Speed &raquo www.prakashsurya.com</title>
  </head>
  <body>
    <main>
  <textarea id="source">
class: center, middle
# ZIL Performance: How I Doubled Sync Write Speed
---
# Agenda

### 1. What is the ZIL?

### 2. How is it used? How does it work?

### 3. The problem to be fixed; the solution.

### 4. Details on the changes I made.

### 5. Performance testing and results.

.footnote[&lt;sup&gt;\*&lt;/sup&gt;Press &#34;p&#34; for notes, and &#34;c&#34; for split view.]

---

class: middle, center

# 1 &amp;ndash; What is the ZIL?

---

# What is the ZIL?

 - ZIL: Acronym for (Z)FS (I)ntent (L)og

    - Logs synchronous operations to disk, before `spa_sync()`

    - What constitutes a &#34;synchronous operation&#34;?

       - most _modifying_ ZPL operations:

          - e.g. `zfs_create`, `zfs_unlink`, `zfs_write` (some), etc.

       - doesn&#39;t include non-modifying ZPL operations:

          - e.g. `zfs_read`, `zfs_seek`, etc.

???

# What is the ZIL?

 - ZIL: Acronym for (Z)FS (I)ntent (L)og

    - Logs synchronous operations to disk, before `spa_sync()`

    - What constitutes a &#34;synchronous operation&#34;?

       - most _modifying_ ZPL operations:

          - e.g. `zfs_create`, `zfs_unlink`, `zfs_write` (sync), etc.

       - doesn&#39;t include non-modifying ZPL operations:

          - e.g. `zfs_read`, `zfs_seek`, etc.

---

# When is the ZIL used?

 - Always&lt;sup&gt;\*&lt;/sup&gt;

    - ZPL operations (`itx`&#39;s) logged via in-memory lists

    - lists of in-memory `itx`&#39;s written to disk via `zil_commit()`

    - `zil_commit()` called for:

       - _any_ sync write

       - other sync operations (e.g. create, unlink), **and** `sync=always`

       - _some_ reads (`sync=always` or `FRSYNC` set)

.footnote[&lt;sup&gt;\*&lt;/sup&gt;Except when dataset configured with: `sync=disabled`]

???

# When is the ZIL used?

 - Always&lt;sup&gt;\*&lt;/sup&gt;

    - ZPL operations (`itx`&#39;s) logged via in-memory lists

    - lists of in-memory `itx`&#39;s written to disk via `zil_commit()`

    - `zil_commit()` called for:

       - _any_ sync write

       - other sync operations (e.g. create, unlink), **and** `sync=always`

       - _some_ reads (`sync=always` or `FRSYNC` set)

 - Caveat: None of this applies if dataset configured with: `sync=disabled`

    - in-memory `itx`&#39;s aren&#39;t created

    - `zil_commit` doesn&#39;t do writes to disk

---

# What is the SLOG?

 - SLOG: Acronym for (S)eperate (LOG) Device

    - An SLOG is not necessary

    - An SLOG can be used to improve latency of ZIL writes

 - Conceptually, SLOG is different than the ZIL

 - ZIL is used, even if no SLOG attached

???

# What is the SLOG?

 - SLOG: Acronym for (S)eperate (LOG) Device

    - An SLOG is not necessary

       - By default (no SLOG), ZIL will write to main pool VDEVs

    - An SLOG can be used to improve latency of ZIL writes

       - When attached, ZIL writes to SLOG instead of main pool

 - Conceptually, SLOG is different than the ZIL

    - Difference between SLOG and ZIL, similar difference of DMU and VDEV

    - ZIL is mechanism for writing, SLOG is device written to

 - ZIL is used, even if no SLOG attached

---

# Why does the ZIL exist?

 - Writes in ZFS are &#34;write-back&#34;

 - Without the ZIL, sync operations inherit latency of `spa_sync()`&lt;sup&gt;\*&lt;/sup&gt;

    - `spa_sync()` can take tens of seconds (or more) to complete

 - Further, with the ZIL, write amplification can be mitigated

 - ZIL is essentially a performance optimization

.footnote[&lt;sup&gt;\*&lt;/sup&gt;All operations inherit this latency, but only sync operations wait for completion]
???

# Why does the ZIL exist?

 - Writes in ZFS are &#34;write-back&#34;

    - Data is first written and stored in-memory, in DMU layer...

    - At some later point, data for whole pool written to disk via `spa_sync()`.

 - Without ZIL, all sync operations inherit latency of `spa_sync()`

    - `spa_sync()` can take tens of seconds (or more) to complete

    - We don&#39;t want each sync operation to take this log

 - Further, with the ZIL, write amplification can be mitigated

    - A single ZPL operation, can cause many writes to occur

       - e.g. 1 block write to a file causes additional writes to update
         indirect blocks

    - ZIL allows operation to &#34;complete&#34; with minimal data written

 - ZIL is essentially a performance optimization

    - Strictly speaking, correctness could be achieved without it...

       - e.g. all sync ops could just wait for `spa_sync()` to complete

    - But practically speaking, it&#39;s necessary...

       - Performance would be unreasonably bad without it...

       - due to reasons just described

---

# ZIL On-Disk Format

 - Each dataset has it&#39;s own unique ZIL on-disk

 - ZIL stored on-disk as a singly linked list of ZIL blocks (`lwb`&#39;s)

&lt;hr style=&#34;visibility:hidden;&#34; /&gt;

.center[![](zil-on-disk-format.svg)]

???

# ZIL On-Disk Format

 - Each dataset has it&#39;s own unique ZIL on-disk

 - ZIL stored on-disk as a singly linked list of ZIL blocks (`lwb`&#39;s)

    - each ZIL block contains a pointer to the &#34;next&#34; ZIL block

    - no indirect blocks, so no write amplification

    - checksums calculated (and verified) differently

       - &#34;next&#34; checksum is &#34;current&#34; checksum, plus one
```
        error = zio_alloc_zil(...);
        if (error == 0) {
                ASSERT3U(bp-&gt;blk_birth, ==, txg);
                bp-&gt;blk_cksum = lwb-&gt;lwb_blk.blk_cksum;
                bp-&gt;blk_cksum.zc_word[ZIL_ZC_SEQ]&#43;&#43;;

                /*
                 * Allocate a new log write block (lwb).
                 */
                nlwb = zil_alloc_lwb(zilog, bp, slog, txg);
        }
```

---

class: middle, center

# 2 &amp;ndash; How is the ZIL used?

---

# How is the ZIL used?

 - ZPL will generally interact with the ZIL in two phases:

    1. Log the operation(s) &amp;mdash; `zil_itx_assign`

    2. Commit the operation(s) &amp;mdash; `zil_commit`

???

# How is the ZIL used?

 - ZPL will generally interact with the ZIL in two phases:

    1. Log the operation(s) &amp;ndash; via `zil_itx_assign`

        - Tells the ZIL the operation occurred

        - Only called when _new_ ZPL operations occur

    2. Commit the operation(s)

        - Causes the ZIL to write log record of operation to disk

        - Only called if sync write, or `sync=always`

---

# Example: `zfs_write`

 - `zfs_write` &amp;rarr; `zfs_log_write`

 - `zfs_log_write`

    &amp;rarr; `zil_itx_create`

    &amp;rarr; `zil_itx_assign`

 - `zfs_write` &amp;rarr; `zil_commit`

 - Most ZPL operations have a corresponding `zfs_log_*` function

    - `zfs_log_create`
    - `zfs_log_remove`
    - `zfs_log_link`
    - `zfs_log_symlink`
    - `zfs_log_truncate`
    - `zfs_log_setattr`
    - ...

???

# Example: `zfs_write`

 - `zfs_write` first calls `zfs_log_write`

 - Within `zfs_log_write` we call:

    - `zil_itx_create` ...

    - and then, `zil_itx_assign`

 - These `zil_itx_*` functions register the write operation with the ZIL

 - Later, `zfs_write` will call `zil_commit` (if it&#39;s a sync write)

    - This causes ZIL to write previously generated write `itx` to disk

 - Most ZPL operations have a corresponding `zfs_log_*` function

    - All `zfs_log_*` functions similarly call `zil_itx_{create,assign}`

---

# Example: `zfs_fsync`

 - `zfs_fsync` &amp;rarr; `zil_commit`

    - no _new_ operations to log... no `zfs_log_fysnc` function

???

# Example: `zfs_fsync`

 - As another example, `zfs_fsync` doesn&#39;t have a &#34;log&#34; function

    - `fsync` doesn&#39;t create any new modifications...

    - instead, it only ensures prior modifying operations complete.

    - as a result, there isn&#39;t a `zfs_log_fsync` function

---

# Contract between ZIL and ZPL.

 - Parameters to `zil_commit`: ZIL pointer, object number

    - These uniquely identify an object whose data is to be committed

 - When `zil_commit` returns:

    - Operations _relevant_ to the object specified, will be _persistent_
      on disk

    - relevant &amp;ndash; all operations that would modify that object

    - persistent &amp;ndash; Log block(s) written (completed) &amp;rarr; disk flushed

 - Interface of `zil_commit` doesn&#39;t specify _which_ operation(s) to commit

???

# Contract between ZIL and ZPL.

 - Parameters to `zil_commit`: ZIL pointer, object number

    - These uniquely identify an object whose data is to be committed

 - When `zil_commit` returns:

    - Operations _relevant_ to the object specified, will be _persistent_
      on disk

    - relevant &amp;ndash; all operations that would modify that object

       - &#34;sync&#34; and &#34;async&#34;...

       - e.g. &#34;async&#34; writes written along with &#34;sync&#34; writes

    - persistent &amp;ndash; Log block(s) written (completed) &amp;rarr; disk flushed

       - All &#34;prior/dependent&#34; log blocks written and completed

       - We can&#39;t issue the flush before the write completes...

          - or else, disk flush may not contain log block&#39;s contents.

             - concurrent write &#43; flush can be reordered

          - if flush doesn&#39;t contain log block(s), flush was meaningless

 - Interface of `zil_commit` doesn&#39;t specify _which_ operation(s) to commit

    - `zil_commit` doesn&#39;t know which operation(s) the caller cares about...

       - thus, it must write all operations for the object

       - e.g. multiple threads writing to same object, but different offsets...

       - all offsets must be written before `zil_commit` returns

---

class: middle, center

# 2 &amp;ndash; How does the ZIL work?

---

# How does the ZIL work?

 - In memory ZIL contains per-txg `itxg_t` structures

 - Each `itxg_t` contains:

    - A single list of sync operations (for all objects)

    - Object specific lists of async operations


???

 - In memory ZIL contains per-txg `itxg_t` structures

    - An `itxg_t` exists for each DMU TXG not yet synced to disk

    - Each `itx` is specific to a TXG...

       - assigned to corresponding `itxg_t` for that TXG

 - Each `itxg_t` contains:

    - A single list of sync operations (for all objects)

    - Object specific lists of async operations

---

# Example: itx lists

&lt;br /&gt;

![](itx-lists.svg)

---

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

???

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

---

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

     1. find all relavant `itx`&#39;s, move them to the &#34;commit list&#34;

???

# How are itx&#39;s written to disk?

 - ~~`zil_commit` handles the process of writing `itx_t`&#39;s to disk:~~

     1. find all relavant `itx`&#39;s, move them to the &#34;commit list&#34;

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-1-01.svg)

???

# Example: `zil_commit` Object B

 - We&#39;ll start with the same itx lists from before...

 - Except now, we also have any empty commit list

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-1-02.svg)

???

# Example: `zil_commit` Object B

 - We&#39;re calling `zil_commit` for object &#34;B&#34;...

 - So the first step is to move object B&#39;s async `itx`&#39;s to the sync list

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-1-03.svg)


???

# Example: `zil_commit` Object B

 - Nothing to say...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-1-04.svg)

???

# Example: `zil_commit` Object B

 - Now we move the entire sync list to the commit list

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-1-05.svg)

???

# Example: `zil_commit` Object B

 - Finally, all of the `itx`&#39;s that were linked off the sync list, are
   not linked off the commit list

 - So why do we do this?

    - Why do we move the `itx`&#39;s to a new list, vs. simply using the
      sync list?

    - We do this so the thread that writes these `itx`&#39;s out to disk...

       - can work with a list that won&#39;t change due to concurrent activity

    - If there&#39;s concurrent ZPL operations occurring...

       - those operations may insert new `itx`&#39;s onto the sync list.

    - We create a new &#34;commit list&#34; so the list of `itx`s to write out...

       - is isolated from this concurrent ZPL activity

---

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

     1. Move async itx&#39;s for object being commited, to the sync list

     2. Write all commit list `itx`&#39;s to disk

???

# How are itx&#39;s written to disk?

 - ~~`zil_commit` handles the process of writing `itx_t`&#39;s to disk:~~

     1. ~~Move async itx&#39;s for object being commited, to the sync list~~

     2. Write all commit list `itx`&#39;s to disk

        - We do this by iterating over the `itx`&#39;s in the commit list

        - For each `itx`:

           1. If insufficient space in the currently &#34;open&#34; ZIL block:

              - Allocate a new (empty) ZIL block...

              - issue write of old block

           2. Copy the `itx` into the &#34;open&#34; ZIL block

        - Finally, issue last &#34;open&#34; ZIL block to disk

           - this also means, allocated another block...

           - to be the next &#34;open&#34; ZIL block

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-01.svg)

???

# Example: `zil_commit` Object B

 - Starting with the commit list from before...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-02.svg)

???

# Example: `zil_commit` Object B

 - Select the first `itx` in the list...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-03.svg)

???

# Example: `zil_commit` Object B

 - allocate our first ZIL block...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-04.svg)

???

# Example: `zil_commit` Object B

 - and copy the `itx` into the `lwb`&#39;s buffer.

 - The `lwb` still hasn&#39;t been issued to disk yet...

    - it may be used for the next `itx`.

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-05.svg)

???

# Example: `zil_commit` Object B

 - Now we move on to the next `itx` in the list...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-06.svg)

???

# Example: `zil_commit` Object B

 - This `itx` doesn&#39;t fit in the currently &#34;open&#34; lwb

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-07.svg)

???

# Example: `zil_commit` Object B

 - so we must allocate a new `lwb`...

    - and issue the &#34;open&#34; one to disk

 - The newly allocated `lwb` becomes the new &#34;open&#34; lwb

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-08.svg)

???

# Example: `zil_commit` Object B

 - Now we can copy the `itx` into the newly allocated, and &#34;open&#34;, `lwb`

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-09.svg)

???

# Example: `zil_commit` Object B

 - And move on to the last `itx` in the list

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-10.svg)

???

# Example: `zil_commit` Object B

 - This fits in the currently &#34;open&#34; `lwb`...

 - so it can simply be copied into place

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-2-11.svg)

???

# Example: `zil_commit` Object B

 - And finally, we can issue the currently &#34;open&#34; lwb to disk

    - To do this, we must also allocate a new `lwb`...

       - to be the &#34;next open&#34; `lwb`

    - This &#34;next open&#34; lwb will used for the next &#34;batch&#34; of `itx`&#39;s

---

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

     1. Move async itx&#39;s for object being commited, to the sync list

     2. Write all commit list `itx`&#39;s to disk

     3. Wait for all ZIL block writes to complete

???

# How are itx&#39;s written to disk?

 - ~~`zil_commit` handles the process of writing `itx_t`&#39;s to disk:~~

     1. ~~Move async itx&#39;s for object being commited, to the sync list~~

     2. ~~Write all commit list `itx`&#39;s to disk~~

     3. Wait for all ZIL block writes to complete

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-3-01.svg)

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-3-02.svg)

???

# Example: `zil_commit` Object B

 - The `lwb`&#39;s can complete in any order

 - In this example, &#34;lwb 2&#34; completes first...

---

# Example: `zil_commit` Object B

&lt;br /&gt;

![](zil-commit-3-03.svg)

???

# Example: `zil_commit` Object B

 - Then &#34;lwb 1&#34;...

 - At this point, all `lwb`&#39;s have completed.

---

# How are itx&#39;s written to disk?

 - `zil_commit` handles the process of writing `itx_t`&#39;s to disk:

     1. Move async itx&#39;s for object being commited, to the sync list

     2. Write all commit list `itx`&#39;s to disk

     3. Wait for all ZIL block writes to complete

     4. Flush VDEVs and notify waiting threads

???

# How are itx&#39;s written to disk?

 - ~~`zil_commit` handles the process of writing `itx_t`&#39;s to disk:~~

     1. ~~Move async itx&#39;s for object being commited, to the sync list~~

     2. ~~Write all commit list `itx`&#39;s to disk~~

     3. ~~Wait for all ZIL block writes to complete~~

     4. Flush VDEVs and notify waiting threads

---

class: middle, center

# 2 &amp;ndash; ZIL Block Sizing &#43; Performance

---

# ZIL Block Sizing &#43; Performance

 - ZIL blocks must be &#34;pre-allocated&#34;, due to on-disk format

    - Block size chosen at time of allocation

 - Allocated block size can dramatically impact performance:

    - &#34;too big&#34; &amp;ndash; wasted space

    - &#34;too small&#34; &amp;ndash; too many (small) IOPs issued to disk

    - &#34;just right&#34; &amp;ndash; large IOPs filled with `itx`&#39;s

???

# ZIL Block Sizing &#43; Performance

 - ZIL blocks must be &#34;pre-allocated&#34;, due to on-disk format

    - Each ZIL block contains pointer to next ZIL block on disk...

       - Thus, &#34;next&#34; block allocated when the &#34;current&#34; is issued to disk

    - Block size chosen at time of allocation

 - Allocated block size can dramatically impact performance:

    - Because blocks &#34;pre-allocated&#34;...

       - the size of &#34;next&#34; block is decided before number of `itx`&#39;s is known

    - &#34;too big&#34; &amp;ndash; wasted space

       - Block only partially filled with `itx`&#39;s

       - Can result in small, but fast, SLOG filling to capacity

          - full SLOG can cause poor performance

    - &#34;too small&#34; &amp;ndash; too many (small) IOPs issued to disk

       - Can cause disk saturation and poor performance

    - &#34;just right&#34; &amp;ndash; large IOPs filled with `itx`&#39;s

       - ZIL blocks fully utilized with `itx`&#39;s (no wasted SLOG space)

       - Fewer IOPs for same number of `itx`&#39;s

          - e.g. 15 8K writes using 1 128K lwb vs. 4 32K lwb&#39;s

---

class: middle, center

# 3 &amp;ndash; Problem

---

# Problem

 1. `itx`&#39;s grouped and written in &#34;batches&#34;

    - The commit list constitutes a batch

    - Batch size proportional to sync workload on system

 2. Waiting threads only notified when _all_ ZIL blocks in batch complete

 3. Only a single batch processed at a time

---

# Example Batch

&lt;br /&gt;

![](problem-01.svg)

???

# Example Batch

 - Example batch taken from prior slides

---

# Example &#34;itx S1&#34;

&lt;br /&gt;

![](problem-02.svg)

???

# Example &#34;itx S1&#34;

 - Thread waiting for &#34;itx S1&#34;

    - e.g. a single sync write

---

# Example &#34;itx S1&#34;

&lt;br /&gt;

![](problem-03.svg)

???

# Example &#34;itx S1&#34;

 - Must wait for **all** lwb&#39;s to be written and completed

---

# Implications

 1. `zil_commit` latency proportional to system workload, _not_ disk latency

 2. Disk &#34;anomalies&#34; &amp;rarr; larger batches &amp;rarr; increased `zil_commit` latency

 3. New calls to `zil_commit` wait for &#34;current&#34; batch, _and_ &#34;next&#34; batch

???

# Implications

 1. `zil_commit` latency proportional to system workload, _not_ disk latency

    - Fast SLOG may not compensate for large workload

 2. Disk &#34;anomalies&#34; &amp;rarr; larger batches &amp;rarr; increased `zil_commit` latency

    - E.g. Temporary network delays when using SAN storage

 3. New calls to `zil_commit` wait for &#34;current&#34; batch, _and_ &#34;next&#34; batch

    - Average `zil_commit` latency equal to latency of 1.5 batches

---

class: middle, center

# 3 &amp;ndash; Solution

---

# Solution

 - Remove concept of &#34;batches&#34;:

    1. Allow `zil_commit` to issue new ZIL block writes immediately

    2. Notify threads immediately when _dependent_ `itx`&#39;s on disk

???

# Solution

 - Remove concept of &#34;batches&#34;:

    1. Allow `zil_commit` to issue new ZIL block writes immediately

       - Rather than waiting for &#34;current&#34; batch to complete

    2. Notify threads immediately when _dependent_ `itx`&#39;s on disk

       - Rather than when **all** writes complete (dependent or not)

---

# Example &#34;Batch&#34;

&lt;br /&gt;

![](solution-01.svg)

???

# Example &#34;Batch&#34;

 - Same example as before

---

# Example &#34;itx S1&#34;

&lt;br /&gt;

![](solution-02.svg)

???

# Example &#34;itx S1&#34;

 - Thread waiting for &#34;itx S1&#34; (just like before)

---

# Example &#34;itx S1&#34;

&lt;br /&gt;

![](solution-03.svg)

???

# Example &#34;itx S1&#34;

 - Must wait for _only_ &#34;lwb 1&#34; to complete

 - Previously, would have also waited for &#34;lwb 2&#34;

    - Might not seem significant for a small example with 2 `lwb`&#39;s...

    - But it can be significant with 100&#39;s of `lwb`&#39;s on a real system

---

class: middle, center

# 4 &amp;ndash; Changes to VDEV Flush

---

# Details

 - A ZIL block is not &#34;persistent&#34; until the VDEV is flushed

 - Prior mechanics:

    - Single VDEV flush for each VDEV, after batch completes

    - 1 flush per many lwb&#39;s

 - New mechanics:

    - VDEV flush issued after each ZIL block written

    - 1 flush per 1 lwb

???

# Details

 - A ZIL block is not &#34;persistent&#34; until the VDEV is flushed

 - Prior mechanics:

    - Single VDEV flush for each VDEV, after batch completes

    - 1 flush per many lwb&#39;s

 - New mechanics:

    - VDEV flush issued after each ZIL block written

    - 1 flush per 1 lwb

---

# Example: Before

&lt;br /&gt;

.center[![](flush-sequence-before.svg)]

???

# Example: Before

 - 2 `lwb`&#39;s are written to VDEV 1...

    - but only a single flush is issued to it.

 - Additionally, all flushes are issued at the very end.

---

# Example: After

&lt;br /&gt;

.center[![](flush-sequence-after.svg)]

???

# Example: After

 - The same 2 `lwb`&#39;s are written to VDEV 1...

    - but 2 flushes are issued to it.

 - Additionally, the flushes are intertwined with other activity.

---

class: middle, center

# 4 &amp;ndash; Changes to ZIL Block ZIO Tree

---

# Details

 - ZIL blocks issued to disk using ZIOs

 - Prior mechanics:

    - &#34;root&#34; ZIO created for each batch

       - &#34;write&#34; ZIOs, for all `lwb`&#39;s in batch, are children of root ZIO

    - &#34;flush&#34; ZIOs issued separately after root ZIO completes

 - New mechanics:

    - &#34;root&#34; ZIO created for each lwb

       - &#34;write&#34; and &#34;flush&#34; ZIOs are child of root ZIO

    - &#34;next&#34; lwb root ZIO become parent of &#34;current&#34; lwb root ZIO

???

# Details

 - ZIL blocks issued to disk using ZIOs

    - ZIOs represented as directed acyclic graph (DAG)

    - Parent ZIOs cannot complete until all children complete

 - Prior mechanics:

    - &#34;root&#34; ZIO created for each batch

       - &#34;write&#34; ZIOs, for all `lwb`&#39;s in batch, are children of root ZIO

    - &#34;flush&#34; ZIOs issued separately after root ZIO completes

 - New mechanics:

    - &#34;root&#34; ZIO created for each lwb

       - &#34;write&#34; and &#34;flush&#34; ZIOs are child of root ZIO

    - &#34;next&#34; lwb root ZIO become parent of &#34;current&#34; lwb root ZIO

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-01.svg)]

???

# Example: Before

 - First, create the batch root ZIO

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-02.svg)]

???

# Example: Before

 - Then the lwb &#34;write&#34; ZIOs are created as children

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-03.svg)]

???

# Example: Before

 - Nothing to say...

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-04.svg)]

???

# Example: Before

 - Nothing to say...

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-05.svg)]

???

# Example: Before

 - After all lwb &#34;write&#34; ZIOs complete...

 - we create the root ZIO for the VDEV flush commands

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-06.svg)]

???

# Example: Before

 - And issue the flush to the first VDEV

---

# Example: Before

&lt;br /&gt;

.center[![](zio-tree-before-07.svg)]

???

# Example: Before

 - And now the second VDEV

 - Once both of these flushes complete, we&#39;re done...

 - and notify the waiting threads

---

# Example: After

![](zio-tree-after-01.svg)

???

# Example: After

 - Now, the ZIO tree is drastically different, as we&#39;ll soon see

 - In the new code, first we create a root ZIO...

 - that is specific to the lwb being written

---

# Example: After

![](zio-tree-after-02.svg)

???

# Example: After

 - Then we create a &#34;write&#34; ZIO for the same lwb...

 - It&#39;s this ZIO that contains the actual data that will be written..

 - Containing the `itx`&#39;s that need to be persisted

---

# Example: After

![](zio-tree-after-03.svg)

???

# Example: After

 - Then, after the &#34;write&#34; ZIO completes...

 - we&#39;ll issue the flush ZIO

 - This flush must not be issued until the &#34;write&#34; completes

---

# Example: After

![](zio-tree-after-04.svg)

???

# Example: After

 - The next lwb &#34;root&#34; ZIO will become a parent of the prior lwb &#34;root&#34;

---

# Example: After

![](zio-tree-after-05.svg)

???

# Example: After

 - And like before, it&#39;ll have a &#34;write&#34; ZIO as a child

---

# Example: After

![](zio-tree-after-06.svg)

???

# Example: After

 - But, for the sake of example...

    - let&#39;s say the next `lwb` is issued before this &#34;write&#34; completes.

 - In this case, the &#34;write&#34; will still be outstanding...

    - but a new `lwb` can be created and issued.

 - This new `lwb` will, again, be a parent of the prior `lwb`

---

# Example: After

![](zio-tree-after-07.svg)

???

# Example: After

 - And this `lwb` will also have a child &#34;write&#34; ZIO

---

# Example: After

![](zio-tree-after-08.svg)

???

# Example: After

 - It&#39;s possible, that the &#34;write&#34; for &#34;lwb 3&#34; will complete before &#34;lwb 2&#34;

 - In which case, &#34;lwb 3&#34; will issue the flush...

    - even though &#34;lwb 2&#34; is still outstanding

 - It&#39;s important to note...

    - &#34;lwb 3&#34; can&#39;t &#34;complete&#34; until &#34;lwb 2&#34; is complete

    - this is enforced by the ZIO layer

       - a &#34;parent&#34; ZIO can&#39;t complete until all children complete

---

# Example: After

![](zio-tree-after-09.svg)

???

# Example: After

 - Then, later, when the &#34;write&#34; for &#34;lwb 2&#34; completes...

    - The flush will be issued

 - Since &#34;lwb 3&#34; may have already completed it&#39;s IO...

    - it&#39;s possible for &#34;lwb 2&#34; and &#34;lwb 3&#34; to complete &#34;simultaneously&#34;

---

# Example: After

![:scale 100%](zio-tree-after-10.svg)

???

# Example: After

 - This pattern will continue...

    - as long as there&#39;s `lwb`&#39;s to issue to disk

---

# Example: After

![:scale 100%](zio-tree-after-11.svg)

???

# Example: After

 - As lwb ZIOs complete...

    - they&#39;ll simply &#34;drop off&#34; from the bottom of the tree

---

# Example: After

![:scale 100%](zio-tree-after-12.svg)

???

# Example: After

 - Nothing to say...

---

# Example: After

![](zio-tree-after-13.svg)

???

# Example: After

 - Here, both &#34;lwb 1&#34; and &#34;lwb 2&#34; have completed

    - so they&#39;ve been removed from the tree

---

# Example: After

![](zio-tree-after-14.svg)

---

class: middle, center

# 4 &amp;ndash; Changes to Waiter Notification

---

# Details: Before

 - 2 condition variables (CV), for &#34;current&#34; and &#34;next&#34; batch

 - Threads that called `zil_commit`:

    - Assigned to &#34;next batch&#34;, wait on next batch&#39;s CV

 - When &#34;current&#34; batch completes

    1. All threads waiting on &#34;current&#34; signalled, they&#39;d return

    2. One thread waiting on &#34;next&#34; signalled, becomes &#34;writer&#34;

    3. &#34;next&#34; and &#34;current&#34; CV swapped

 - Ultimately, these two CVs are the source of original problem

---

# Example: Before

.center[![](waiter-sequence-before-01.svg)]

---

# Example: Before

.center[![](waiter-sequence-before-02.svg)]

---

# Details: After

 - Each time a process calls `zil_commit`:

    - A new CV is allocated for this specific process to wait on

    - A new `TX_COMMIT` itx is inserted into the ZIL itx tree

       - The &#34;commit itx&#34; has a pointer to the process&#39;s CV

 - When a commit itx is copied to an lwb:

    - No data copied into the lwb&#39;s buffer

    - Instead, itx&#39;s CV added to lwb&#39;s list of CVs

 - When lwb&#39;s ZIO completes, list of CVs iterated and signalled

    - This is how we map which lwb a process is waiting for

---

# Example: After

.center[![](waiter-sequence-after-01.svg)]

---

# Example: After

.center[![](waiter-sequence-after-02.svg)]

---

class: middle, center

# 5 &amp;ndash; Performance testing and results.

---

# Details

 - Two `fio` workloads used to drive a sync write workload

    1. `fio` was trying to perform sync writes as fast as it could

    2. `fio` was trying to perform 64 sync writes per second

 - IOPs and latency measured with and without my changes

    - Other metrics also observed (`iostat`, flamegraphs, lwb info, etc.)

 - 1, 2, 4, and 8 disk zpools; tested both SSD and HDD

 - Full details can be found [here][perf-results]

---

class: middle, center

# 5 &amp;ndash; Max Rate Workload &amp;ndash; HDDs

---

background-image: url(max-rate-hdd-iops-pctchange.png)
background-size: 95%

# % Change IOPs &amp;ndash; Max Rate &amp;ndash; HDDs

---

background-image: url(max-rate-hdd-lat-pctchange.png)
background-size: 95%

# % Change Latency &amp;ndash; Max Rate &amp;ndash; HDDs

---

class: middle, center

# 5 &amp;ndash; Max Rate Workload &amp;ndash; SSDs

---

background-image: url(max-rate-ssd-iops-pctchange.png)
background-size: 95%

# % Change IOPs &amp;ndash; Max Rate &amp;ndash; SSDs

---

background-image: url(max-rate-ssd-lat-pctchange.png)
background-size: 95%

# % Change Latency &amp;ndash; Max Rate &amp;ndash; SSDs

---

class: middle, center

# 5 &amp;ndash; Fixed Rate Workload &amp;ndash; HDDs

---

background-image: url(fixed-rate-hdd-iops-pctchange.png)
background-size: 95%

# % Change IOPs &amp;ndash; Fixed Rate &amp;ndash; HDDs

---

background-image: url(fixed-rate-hdd-lat-pctchange.png)
background-size: 95%

# % Change Latency &amp;ndash; Fixed Rate &amp;ndash; HDDs

---

class: middle, center

# 5 &amp;ndash; Fixed Rate Workload &amp;ndash; SSDs

---

background-image: url(fixed-rate-ssd-iops-pctchange.png)
background-size: 95%

# % Change IOPs &amp;ndash; Fixed Rate &amp;ndash; SSDs

---

background-image: url(fixed-rate-ssd-lat-pctchange.png)
background-size: 95%

# % Change Latency &amp;ndash; Fixed Rate &amp;ndash; SSDs

[perf-results]: https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/

---
class: center, middle
# End
  </textarea>
  <script>
    

    remark.macros.scale = function (percentage) {
      var url = this;
      return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };

    var slideshow = remark.create({ ratio: '4:3' });

    

    mermaid.initialize({ startOnLoad: false, cloneCssStyles: false });
    slideshow.on('afterShowSlide', function(s) {
      var diagrams = document.querySelectorAll('.mermaid');
      var i;
      for (i=0; i<diagrams.length; i++) {
        if (diagrams[i].offsetWidth > 0) {
          mermaid.init(undefined, diagrams[i]);
        }
      }
    });
  </script>
</main>
  </body>
</html>
