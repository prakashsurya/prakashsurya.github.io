<!DOCTYPE html>
<html>
  <head>
    <meta name="generator" content="Hugo 0.18.1" />
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="canonical" href="https://www.prakashsurya.com/post/">
<link rel="stylesheet" type="text/css" href="/css/hack.css">
<link rel="stylesheet" type="text/css" href="/css/custom.css">

<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-91378771-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


    <title>Posts &raquo www.prakashsurya.com</title>
  </head>
  <body class="hack container">
    <header>
      <nav>
  
    <a class="active" href="/">Home</a>
  
    <a class="active" href="/post/">Posts</a>
  
    <a class="active" href="/link/">Links</a>
  
</nav>

    </header>
    <main>
  <h1>Posts</h1>
  
    <h2>
      24 Oct 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/">ZIL Performance: How I Doubled Sync Write Speed</a>
      
        (slides)
      
    </h2>
    <p>Agenda  What is the ZIL? Why does it exist?
 How is it used? How does it work?
 The problem to be fixed; the solution.
 Details on the changes I made.
 Performance testing and results.
  class: middle, center
1 &ndash; What is the ZIL? What is the ZIL?  ZIL: Acronym for (Z)FS (I)ntent (L)og
 Log of all operations ZFS intends to write via spa_sync()</p>
  
    <h2>
      28 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-28-generating-code-coverage-reports-for-zfs-on-linux/">Generating Code Coverage Reports for ZFS on Linux</a>
      
    </h2>
    <p>Introduction This is another post about collecting code coverage data for the ZFS on Linux project. We&rsquo;ve recently added a new make target to the project, so I wanted to highlight how easy it is to use this to generate static HTML based code coverage reports, and/or to generate a report that can be used with other tools and services (e.g. codecov.io).
Examples Before I get into the specifics for how to run the tests and generate the coverage report, I want to show off the results.</p>
  
    <h2>
      26 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-26-zfs-on-linux-code-coverage/">ZFS on Linux Code Coverage</a>
      
        (slides)
      
    </h2>
    <p>Branches + Pull Requests  Code coverage data is collected for:
 All commits merged to a branch (e.g. master)
 All pull requests for the &ldquo;zfs&rdquo; project
  Code coverage collected after running all tests
 ztest, zfstest, zfsstress, etc.  Data generated using make code-coverage-capture &hellip;
 Emits .info file and static HTML pages  .info file uploaded to codecov.io
 ZFS on Linux + Codecov</p>
  
    <h2>
      19 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-19-python-plus-jupyter-for-performance-testing/">Python &#43; Jupyter for Performance Testing</a>
      
        (slides)
      
    </h2>
    <p>Setting the stage.  Working on performance improvement to ZFS (sync writes)
 To verify my changes, I needed to:
 Measure the performance of the system without my changes.
 Measure the performance of the system with my changes.
 Analyze the difference(s) in performance with and without my changes.
 Collect tangential information from the system, to support (or refute) my conclusions.
   Visualizations required?  While not strictly required, visualizations are often powerful.</p>
  
    <h2>
      18 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/">Code Coverage for ZFS on Linux</a>
      
    </h2>
    <p>I&rsquo;ve been working with Brian Behlendorf on getting code coverage information for the ZFS on Linux. The goal was to get code coverage data for pull requests, as well as branches; this way, we can get a sense of how well tested any given PR is by the automated tests, prior to landing it. There&rsquo;s still some wrinkles that need to be ironed out, but we&rsquo;ve mostly achieved that goal by leveraging codecov.</p>
  
    <h2>
      11 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/">Using &#34;gcov&#34; with ZFS on Linux Kernel Modules</a>
      
    </h2>
    <p>Building a &ldquo;gcov&rdquo; Enabled Linux Kernel In order to extract &ldquo;gcov&rdquo; data from the Linux kernel, and/or Linux kernel modules, a &ldquo;gcov&rdquo; enabled Linux kernel is needed. Since my current development environment is based on Ubuntu 17.04, and the fact that Ubuntu doesn&rsquo;t provide a pre-built kernel with &ldquo;gcov&rdquo; enabled, I had to build the kernel from source. This was actually pretty simple, and most of that process is already documented here.</p>
  
    <h2>
      08 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/">Performance Testing Results for OpenZFS #447</a>
      
    </h2>
    <p>The following are links to the Jupyter notebooks that describe the performance testing that I did for OpenZFS #447, and the results of that testing:
 Max Rate Submit on HDDs Max Rate Submit on SSDs Fixed Rate Submit on HDDs Fixed Rate Submit on SSDs  Additionally, a compressed tarball with all the raw data used to generate those Jupyter notebooks can be found here.</p>
  
    <h2>
      07 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/">Using Python and Jupyter for Performance Testing and Analysis</a>
      
    </h2>
    <p>Introduction I recently worked on some changes to the OpenZFS ZIL (see here), and in the context of working on that project, I discovered some new tools that helped me run my performance tests and analyze their results. What follows is some notes on the tools that I used, and how I used them.
Quick Overview Before I dive into the details of how I used these tools, I wanted to quickly go over what the tools were:</p>
  
    <h2>
      05 Sep 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/">Building and Using &#34;crash&#34; on Ubuntu 16.04</a>
      
    </h2>
    <p>Introduction I&rsquo;ve been working on the ZFS on Linux project recently, and had a need to use crash on the Ubuntu 16.04 based VM I was using. The following is some notes regarding the steps I had to take, in order to build, install, and ultimately run the utility against the &ldquo;live&rdquo; system.
Build and Install &ldquo;crash&rdquo; First, I had to install the build dependencies:
$ sudo apt-get install -y \ git build-essential libncurses5-dev zlib1g-dev bison  Then I could checkout the source code, build, and install:</p>
  
    <h2>
      28 Aug 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/">Using BCC&#39;s &#34;trace&#34; Instead of &#34;printk&#34;</a>
      
    </h2>
    <p>Introduction Recently I&rsquo;ve been working on porting some changes that I made to the OpenZFS ZIL over to the ZFS on Linux codebase; see here for the OpenZFS pull request, and here for the ZFS on Linux pull request.
In my initial port, I was running into a problem where the automated tests would trigger a &ldquo;hang&rdquo; as a result of the readmmap program calling msync:
$ pstree -p 2337 test-runner.</p>
  
    <h2>
      04 Aug 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/">OpenZFS: Isolating ZIL Disk Activity</a>
      
    </h2>
    <p>I recently completed a project to improve the performance of the OpenZFS ZIL (see here for more details); i.e. improving the performance of synchronous activity on OpenZFS, such as writes using the O_SYNC flag. As part of that work, I had to run some performance testing and benchmarking of my code changes (and the system as a whole), to ensure the system was behaving as I expected.
Early on in my benchmarking exercises, I became confused by the data that I was gathering.</p>
  
    <h2>
      16 Mar 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/">Running `sshd` on Windows using Cygwin</a>
      
    </h2>
    <p>Introduction As part of our effort to support Delphix in the Azure cloud environment, we&rsquo;re writing some automation to convert our .iso install media into a VHD image, leveraging Jenkins and Packer in the process.
Essentially, we want to use a Windows server as a Jenkins &ldquo;slave&rdquo;, and run Packer from within a Jenkins job that will run on that Windows system.
In order to do that, the Jenkins &ldquo;master&rdquo; needs to connect with the Windows system, such that it can configure the system to act as a Jenkins slave.</p>
  
    <h2>
      15 Mar 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/">OpenZFS: Notes on ZIL Transactions</a>
      
    </h2>
    <p>Introduction The OpenZFS Intent Log (ZIL) is used to ensure POSIX compliance of certain system calls (that modify the state of a ZFS dataset), and protect against data loss in the face of failure scenarios such as: an operating system crash, power loss, etc. Specifically, it&rsquo;s used as a performance optimization so that applications can be assured that their given system call, and any &ldquo;user data&rdquo; associated with it, will not be &ldquo;lost&rdquo;, without having to wait for an entire transaction group (TXG) to be synced out (which can take on the order of seconds, on a moderately loaded system).</p>
  
    <h2>
      22 Feb 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/">OpenZFS: Refresher on `zpool reguid` Using Examples</a>
      
    </h2>
    <p>Introduction The zpool reguid command can be used to regenerate the GUID for an OpenZFS pool, which is useful when using device level copies to generate multiple pools all with the same contents.
Example using File VDEVs As a contrived example, lets create a zpool backed by a single file vdev:
# mkdir /tmp/tank1 # mkfile -n 256m /tmp/tank1/vdev # zpool create tank1 /tmp/tank1/vdev # zpool list tank1 NAME SIZE ALLOC FREE EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT tank1 240M 78K 240M - 1% 0% 1.</p>
  
    <h2>
      06 Feb 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-02-06-creating-a-custom-amazon-ec2-ami-from-iso/">Creating a Custom Amazon EC2 AMI from ISO (using OI Hipster)</a>
      
    </h2>
    <p>Preface In this post, I&rsquo;ll pick up from where I left off last time, and demonstrate one potential way to convert the installation ISO media generated in that post, into an AMI that can be used to create new VMs in the Amazon EC2 environment. It&rsquo;s important to note a couple things before we start:
 While I&rsquo;ll be generating an AMI based on OI Hipster, this process should be applicable to any Linux or FreeBSD based operating system as well (and quite possibly Windows too, but I don&rsquo;t know much about that platform).</p>
  
    <h2>
      01 Feb 2017
      &raquo;
      <a href="https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/">Creating Custom Installation Media for OI Hipster</a>
      
    </h2>
    <p>Preface This post is a write up of my notes for creating custom installation media for OpenIndiana Hipster, using a custom/patched version of illumos. It assumes that OI Hipster has already been installed on a machine (e.g. installed on a VM using their provided installation media); and this server will be used to build our custom version of illumos, as well as the custom OI installation media. The goal of this exercise is to create a &ldquo;Live DVD&rdquo; that can be used to install our custom version of illumos.</p>
  
    <h2>
      07 Dec 2015
      &raquo;
      <a href="https://www.prakashsurya.com/post/2015-12-07-openzfs-artificial-disk-latency-using-zinject/">OpenZFS: Artificial Disk Latency Using zinject</a>
      
    </h2>
    <p>About a year ago I had the opportunity to work on a small extension to the OpenZFS zinject command with colleagues Matt Ahrens and Frank Salzmann, during one of our Delphix engineering wide hackathon events. Now that it&rsquo;s in the process of landing in the upstream OpenZFS repository, I wanted to take a minute to show it off.
To describe the new functionality, I&rsquo;ll defer to the help message:</p>
  
    <h2>
      23 Mar 2015
      &raquo;
      <a href="https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/">OpenZFS: Reducing ARC Lock Contention</a>
      
    </h2>
    <p>tl;dr; Cached random read performance of 8K blocks was improved by 225% by reducing internal lock contention within the OpenZFS ARC on illumos.
Introduction Locks are a pain. Even worse, is a single lock serializing all of the page cache accesses for a filesystem. While that&rsquo;s not quite the situation the OpenZFS ARC was in earlier this year, it was pretty close.
For those unfamiliar, the OpenZFS file system is built atop its very own page cache based on the paper by Nimrod Megiddo and Dharmendra S.</p>
  
    <h2>
      06 Jan 2015
      &raquo;
      <a href="https://www.prakashsurya.com/post/2015-01-06-openzfs-developer-summit-2014-openzfs-on-illumos/">OpenZFS Developer Summit 2014: OpenZFS on illumos</a>
      
    </h2>
    <p>The OpenZFS project is growing! The second annual OpenZFS developer summit concluded just under two months ago, and overall I thought it went very well. There was roughly 70 attendees, twice as many as the previous year, and the talks given were very engaging and interesting.
I gave a short talk about ZFS on the illumos platform, and also touched briefly on some of my subjective opinions coming from a ZFS on Linux background.</p>
  
</main>
    <footer>
  <hr />

<div class="footer">
  <div id="footer-left">
    <a href="mailto:me@prakashsurya.com">me@prakashsurya.com</a>
  </div>

  <div id="footer-center">
    Las Vegas, NV
  </div>

  <div id="footer-right">
    Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="http://hackcss.com/">hack.css</a>
  </div>
</div>

</footer>
  </body>
</html>
