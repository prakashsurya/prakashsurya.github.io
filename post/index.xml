<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on www.prakashsurya.com</title>
    <link>https://www.prakashsurya.com/post/index.xml</link>
    <description>Recent content in Posts on www.prakashsurya.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Nov 2017 00:00:00 -0800</lastBuildDate>
    <atom:link href="https://www.prakashsurya.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Performance Testing Results for ZFS on Linux #6566</title>
      <link>https://www.prakashsurya.com/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/</guid>
      <description>&lt;p&gt;The following are links to the Jupyter notebooks that describe the
performance testing that I did for &lt;a href=&#34;https://github.com/zfsonlinux/zfs/pull/6566&#34;&gt;ZFS on Linux #6566&lt;/a&gt;, and the
results of that testing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/zfsonlinux-6566-perf-max-rate-submit-hdd.ipynb
&#34;&gt;Max Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/zfsonlinux-6566-perf-max-rate-submit-ssd.ipynb
&#34;&gt;Max Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/zfsonlinux-6566-perf-fixed-rate-submit-hdd.ipynb
&#34;&gt;Fixed Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-11-17-performance-testing-results-for-zfsonlinux-6566/zfsonlinux-6566-perf-fixed-rate-submit-ssd.ipynb
&#34;&gt;Fixed Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, a compressed tarball with all the raw data used to
generate those Jupyter notebooks can be found &lt;a href=&#34;zfsonlinux-6566-perf.tar.xz&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ZIL Performance: How I Doubled Sync Write Speed</title>
      <link>https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-10-24-zil-performance-how-i-doubled-sync-write-speed/</guid>
      <description>

&lt;h1 id=&#34;agenda&#34;&gt;Agenda&lt;/h1&gt;

&lt;h3 id=&#34;1-what-is-the-zil&#34;&gt;1. What is the ZIL?&lt;/h3&gt;

&lt;h3 id=&#34;2-how-is-it-used-how-does-it-work&#34;&gt;2. How is it used? How does it work?&lt;/h3&gt;

&lt;h3 id=&#34;3-the-problem-to-be-fixed-the-solution&#34;&gt;3. The problem to be fixed; the solution.&lt;/h3&gt;

&lt;h3 id=&#34;4-details-on-the-changes-i-made&#34;&gt;4. Details on the changes I made.&lt;/h3&gt;

&lt;h3 id=&#34;5-performance-testing-and-results&#34;&gt;5. Performance testing and results.&lt;/h3&gt;

&lt;p&gt;.footnote[&lt;sup&gt;*&lt;/sup&gt;Press &amp;ldquo;p&amp;rdquo; for notes, and &amp;ldquo;c&amp;rdquo; for split view.]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here&amp;rsquo;s a brief overview of what I plan to discuss.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It&amp;rsquo;s broken up into roughly 3 parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;First I&amp;rsquo;ll give some background, and discuss:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What the ZIL is&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;how it&amp;rsquo;s used&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and how it works.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then I&amp;rsquo;ll get into:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problem I set out to fix&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;how I fixed it&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and provide some details on how I did that.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And then, lastly, I&amp;rsquo;ll show off some graphs&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and the results of my work.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;So, with that out of the way, let&amp;rsquo;s get started.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;1-ndash-what-is-the-zil&#34;&gt;1 &amp;ndash; What is the ZIL?&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First off&amp;hellip; what is the ZIL?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;what-is-the-zil&#34;&gt;What is the ZIL?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ZIL: Acronym for (Z)FS (I)ntent (L)og&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Logs synchronous operations to disk, before &lt;code&gt;spa_sync()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What operations get logged?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_create&lt;/code&gt;, &lt;code&gt;zfs_remove&lt;/code&gt;, &lt;code&gt;zfs_write&lt;/code&gt;, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Doesn&amp;rsquo;t include non-modifying ZPL operations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;zfs_read&lt;/code&gt;, &lt;code&gt;zfs_seek&lt;/code&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What gets logged?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The fact that a logical operation is occurring is logged&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;zfs_remove&lt;/code&gt; &amp;rarr; directory object ID + name only&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Not logging which blocks will change due to logical operation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ZIL stands for ZFS Intent Log.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It&amp;rsquo;s the mechanism that&amp;rsquo;s responsible for logging synchronous
operations to disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The operations that get logged are &amp;ldquo;logical&amp;rdquo; operations, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;zfs_create&lt;/code&gt;, &lt;code&gt;zfs_remove&lt;/code&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This does not include non-modifying operations, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;zfs_read&lt;/code&gt;, &lt;code&gt;zfs_seek&lt;/code&gt;, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The data that get&amp;rsquo;s logged, is simply the fact that the logical
operation occured.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example, for &lt;code&gt;zfs_remove&lt;/code&gt;, it&amp;rsquo;s only&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the name of the object to remove&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and the object ID of the directory from which that name will be
removed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It&amp;rsquo;s &lt;em&gt;not&lt;/em&gt; which blocks on-disk would change due to that removal.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;when-is-the-zil-used&#34;&gt;When is the ZIL used?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Always&lt;sup&gt;*&lt;/sup&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ZPL operations (&lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s) logged via in-memory lists&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lists of in-memory &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s written to disk via &lt;code&gt;zil_commit()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit()&lt;/code&gt; called for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;any&lt;/em&gt; sync write&lt;sup&gt;**&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.footnote[&lt;sup&gt;*&lt;/sup&gt;Except when dataset configured with: &lt;code&gt;sync=disabled&lt;/code&gt;.
          &lt;sup&gt;**&lt;/sup&gt;Except when dataset configured with: &lt;code&gt;sync=always&lt;/code&gt;.]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ZIL is almost always used.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Whenever any of these logged operations occur&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;they&amp;rsquo;re inserted into the ZIL&amp;rsquo;s in-memory list of operations.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;These operations are often called as &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s, or &amp;ldquo;intent log
transactions&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When &lt;code&gt;zil_commit&lt;/code&gt; is called&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s tracked by the in-memory ZIL are then written to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The caveat to all of this, being&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;none of this occurs if the dataset is configured with
&lt;code&gt;sync=disabled&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If &lt;code&gt;sync=disabled&lt;/code&gt;, &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s aren&amp;rsquo;t tracked in-memory&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nor are they written to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;what-is-the-slog&#34;&gt;What is the SLOG?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SLOG: Acronym for (S)eperate (LOG) Device&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Conceptually, SLOG is different than the ZIL&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ZIL is mechanism for writing, SLOG is device written to&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An SLOG is not necessary&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;By default (no SLOG), ZIL will write to main pool VDEVs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An SLOG can be used to improve latency of ZIL writes&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When attached, ZIL writes to SLOG instead of main pool&lt;sup&gt;*&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.footnote[&lt;sup&gt;*&lt;/sup&gt;For some operations; see code for details.]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since I often see the ZIL and SLOG terms used incorrectly&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I wanted to briefly address this.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SLOG stands for Seperate Log Device.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The ZIL and SLOG are different, in that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the ZIL is the mechanism for issuing writes to disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and the SLOG &lt;em&gt;may&lt;/em&gt; be the disk that the writes are issued to.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;With that said, an SLOG is &lt;strong&gt;not&lt;/strong&gt; necessary.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By default, ZIL writes will go to the main pool&amp;rsquo;s disks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;But&lt;/em&gt;, an SLOG can be used to try and improve the latency of ZIL
writes&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if the main pool&amp;rsquo;s VDEVs are deemed &amp;ldquo;too slow&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;why-does-the-zil-exist&#34;&gt;Why does the ZIL exist?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Writes in ZFS are &amp;ldquo;write-back&amp;rdquo;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Data is first written and stored in-memory, in DMU layer&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Later, data for whole pool written to disk via &lt;code&gt;spa_sync()&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Without the ZIL, sync operations could wait for &lt;code&gt;spa_sync()&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spa_sync()&lt;/code&gt; can take tens of seconds (or more) to complete&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, with the ZIL, write amplification can be mitigated&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A single ZPL operation can cause many writes to occur&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ZIL allows operation to &amp;ldquo;complete&amp;rdquo; with minimal data written&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ZIL needed to provide &amp;ldquo;fast&amp;rdquo; synchronous semantics to applications&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Correctness could be acheived without it, but would be &amp;ldquo;too slow&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So, why exactly does the ZIL exist in the first place?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Well, writes in ZFS are &amp;ldquo;write back&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Data is first modified and stored in-memory, in the DMU layer&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and then at some later point, this data is written to disk via
&lt;code&gt;spa_sync()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The problem is, &lt;code&gt;spa_sync()&lt;/code&gt; can take tens of seconds or more&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to write out this data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It&amp;rsquo;s unacceptable for all sync writes to take tens of seconds to
complete.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, writes in ZFS often cause more writes to occur.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example. a singe file write&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;modifying a single block of &amp;ldquo;user data&amp;rdquo;&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;will then cause indirect blocks to also be modified and written.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The ZIL allows this &amp;ldquo;write amplification&amp;rdquo; effect to be mitigated.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Essentially, the ZIL exists as a performance optimization.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To provide synchronous sematics to applications&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;faster than what could be achieved with &lt;code&gt;spa_sync()&lt;/code&gt; alone.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While correctness &lt;em&gt;could&lt;/em&gt; be acheived without the ZIL&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;performance would be unreasonably bad&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which makes it a necessity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;zil-on-disk-format&#34;&gt;ZIL On-Disk Format&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each dataset has it&amp;rsquo;s own unique ZIL on-disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ZIL stored on-disk as a singly linked list of ZIL blocks (&lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr style=&#34;visibility:hidden;&#34; /&gt;

&lt;p&gt;.center[&lt;img src=&#34;zil-on-disk-format.svg&#34; alt=&#34;&#34; /&gt;]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Before I jump into the next section&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I wanted to quickly go over the on-disk format of the ZIL.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each ZFS dataset maintains it own unique ZIL on disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each of these ZIL&amp;rsquo;s is a singly liked list of ZIL blocks&amp;hellip;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;or, as they&amp;rsquo;re also called, &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which stands for &amp;ldquo;log write block&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As one can see here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the uberblock has a pointer to the MOS&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which then has pointers to each dataset in the pool&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and each of these datasets has a pointer it&amp;rsquo;s own ZIL header.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each ZIL header then points to an &lt;code&gt;lwb&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and that block then points to the &amp;ldquo;next&amp;rdquo; block in the list.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;2-ndash-how-is-the-zil-used&#34;&gt;2 &amp;ndash; How is the ZIL used?&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Now let&amp;rsquo;s get into how the ZIL is used&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-is-the-zil-used&#34;&gt;How is the ZIL used?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ZPL will generally interact with the ZIL in two phases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Log the operation(s) &amp;mdash; &lt;code&gt;zil_itx_assign&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tells the ZIL an operation is occurring&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Commit the operation(s) &amp;mdash; &lt;code&gt;zil_commit&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Causes the ZIL to write log record of operation to disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ZIL is used by the ZFS Posix Layer, or ZPL for short.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The ZPL generally interacts with the ZIL in two phases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;First it uses &lt;code&gt;zil_itx_assign&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This cause the ZIL to log the fact that an operation
is occurring.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then it uses &lt;code&gt;zil_commit&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This tells the ZIL to write out these log records to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zfs-write&#34;&gt;Example: &lt;code&gt;zfs_write&lt;/code&gt;&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_write&lt;/code&gt; &amp;rarr; &lt;code&gt;zfs_log_write&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_log_write&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;rarr; &lt;code&gt;zil_itx_create&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;rarr; &lt;code&gt;zil_itx_assign&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_write&lt;/code&gt; &amp;rarr; &lt;code&gt;zil_commit&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s look at &lt;code&gt;zfs_write&lt;/code&gt; as an example of this.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_write&lt;/code&gt; will call &lt;code&gt;zfs_log_write&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;zfs_log_write&lt;/code&gt; will then call:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_itx_create&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;which will create the &lt;code&gt;itx&lt;/code&gt; structure in RAM&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and then it&amp;rsquo;ll call &lt;code&gt;zil_itx_assign&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;which will insert that &lt;code&gt;itx&lt;/code&gt; into the ZIL&amp;rsquo;s in-memory state.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, if this is a sync write, &lt;code&gt;zfs_write&lt;/code&gt; will then call
&lt;code&gt;zil_commit&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;which will cause the &lt;code&gt;itx&lt;/code&gt; to get written out to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zfs-fsync&#34;&gt;Example: &lt;code&gt;zfs_fsync&lt;/code&gt;&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;fsync&lt;/code&gt; &amp;rarr; &lt;code&gt;zil_commit&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;fsync&lt;/code&gt; doesn&amp;rsquo;t create any new modifications&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;only writes previous &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to disk&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;thus, no &lt;code&gt;zfs_log_fsync&lt;/code&gt; function&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now let&amp;rsquo;s look at &lt;code&gt;zfs_fsync&lt;/code&gt; as another example.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;fsync&lt;/code&gt; doesn&amp;rsquo;t create any new modifications.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Instead, it simply ensures any previous operations are written to disk
before it returns.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, &lt;code&gt;zfs_fsync&lt;/code&gt; doesn&amp;rsquo;t call &lt;code&gt;zil_itx_create&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nor does it call &lt;code&gt;zil_itx_assign&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Instead it &lt;em&gt;only&lt;/em&gt; calls &lt;code&gt;zil_commit&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calling &lt;code&gt;zil_commit&lt;/code&gt; will ensure all previous operations are written
to disk, before &lt;code&gt;fsync&lt;/code&gt; returns.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;contract-between-zil-and-zpl&#34;&gt;Contract between ZIL and ZPL.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Parameters to &lt;code&gt;zil_commit&lt;/code&gt;: ZIL pointer, object number&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;These uniquely identify an object whose data is to be committed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When &lt;code&gt;zil_commit&lt;/code&gt; returns:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Operations &lt;em&gt;relevant&lt;/em&gt; to the object specified, will be &lt;em&gt;persistent&lt;/em&gt;
on disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;relevant &amp;ndash; all operations that would modify that object&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;persistent &amp;ndash; Log block(s) written (completed) &amp;rarr; disk flushed&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Interface of &lt;code&gt;zil_commit&lt;/code&gt; doesn&amp;rsquo;t specify &lt;em&gt;which&lt;/em&gt; operation(s) to commit&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The parameters of &lt;code&gt;zil_commit&lt;/code&gt; are such that&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the caller will pass in enough information to uniquely identify an
object whose data is to be committed.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The contract the ZIL maintains with this caller is&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All operations &lt;em&gt;relevant&lt;/em&gt; to the object specified&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;will be &lt;em&gt;persistent&lt;/em&gt; on disk by the time &lt;code&gt;zil_commit&lt;/code&gt; returns.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By &lt;em&gt;relevant&lt;/em&gt;, it means&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;all operations that would modify that object.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And by &lt;em&gt;persistent&lt;/em&gt;, it means&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the operations are written to disk&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and the disks used for those writes are flushed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, we must issue the disk flush &lt;em&gt;after&lt;/em&gt; the writes complete.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, the interface for &lt;code&gt;zil_commit&lt;/code&gt; doesn&amp;rsquo;t allow the caller to
specify &lt;em&gt;which&lt;/em&gt; operations they care about.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, &lt;code&gt;zil_commit&lt;/code&gt; must write &lt;em&gt;all&lt;/em&gt; operations for a given object&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;even if the caller only cares about a subset of those operations.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example, if there&amp;rsquo;s multiple threads writing to the same file,
but at different offsets&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;all offsets must be written to disk before &lt;code&gt;zil_commit&lt;/code&gt; returns&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;even&lt;/em&gt; if the calling thread only cares about one of those offsets.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;2-ndash-how-does-the-zil-work&#34;&gt;2 &amp;ndash; How does the ZIL work?&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;So, how does the ZIL accomplish this?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-does-the-zil-work&#34;&gt;How does the ZIL work?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In memory ZIL contains an &lt;code&gt;itxg_t&lt;/code&gt; structure&lt;sup&gt;*&lt;/sup&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each &lt;code&gt;itxg_t&lt;/code&gt; contains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A single list of sync operations (for all objects)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Object specific lists of async operations&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;.footnote[&lt;sup&gt;*&lt;/sup&gt;Actually multiple &lt;code&gt;itxg_t&lt;/code&gt; structures, one per-txg.]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Well, as I alluded to previously, the ZIL maintains an in-memory list
of &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s that have occurred&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;but haven&amp;rsquo;t yet been written to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This list is maintained via the &lt;code&gt;itxg&lt;/code&gt; structure in each ZIL.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each &lt;code&gt;itxg&lt;/code&gt; structure contains the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a single list of all sync operations that have occurred&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;for all objects in the dataset.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;plus, per-object lists of async operations for each object
modified.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-itx-lists&#34;&gt;Example: itx lists&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;itx-lists.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here&amp;rsquo;s what this might look like&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In this example, the &lt;code&gt;itxg&lt;/code&gt;&amp;rsquo;s sync list has two &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s in it&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;each of which could map to an operation for any object in the dataset.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And then a list of async operations that occurred for &amp;ldquo;object A&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And &lt;em&gt;another&lt;/em&gt; list of async operations that occurred for &amp;ldquo;object B&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When &lt;code&gt;zil_commit&lt;/code&gt; is called&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How do these &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s get written out to disk?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk-1&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;find all relevant &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s, move them to the &amp;ldquo;commit list&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Well&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;First we must determine which of these &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are &lt;em&gt;relevant&lt;/em&gt;&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And then we move these &lt;em&gt;relevant&lt;/em&gt; &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to a new list&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;called the &amp;ldquo;commit list&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-1-01.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here, we have the same &lt;code&gt;itx&lt;/code&gt; lists as before&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Except now, we also show an empty &amp;ldquo;commit list&amp;rdquo; at the bottom.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Also, let&amp;rsquo;s presume &lt;code&gt;zil_commit&lt;/code&gt; is being called for &amp;ldquo;object B&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-1&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-1-02.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first step is to move all of object B&amp;rsquo;s async &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to the sync
list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;So, we select the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-2&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-1-03.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;And move them.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-3&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-1-04.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Next, we move the entire contents of the sync list&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to the commit list.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-4&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-1-05.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Additionally, it&amp;rsquo;s worth pointing out&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The point of the &amp;ldquo;commit list&amp;rdquo; is so we have a list of &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to
write out&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;that will &lt;em&gt;not&lt;/em&gt; be modified by any concurrent ZPL activity.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As new ZPL operations occur, the sync list may change&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for example, operations may be added to it&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;but the commit list will remain the same.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk-2&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;del&gt;Move async itx&amp;rsquo;s for object being commited, to the sync list&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Write all commit list &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to disk&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now that we have a list of &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to be written out, it&amp;rsquo;s time to
actually issue them to disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We do this by iterating over all the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s in the commit list.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, for each &lt;code&gt;itx&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We attempt to copy it into the currently &amp;ldquo;open&amp;rdquo; ZIL block.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If there&amp;rsquo;s insufficient space in the block&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;then we allocate a new block, and issue old one to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, after all &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are copied into &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we issue the last &amp;ldquo;open&amp;rdquo; block to disk&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;allocating the &lt;em&gt;next&lt;/em&gt; &amp;ldquo;open&amp;rdquo; block in the process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Here&amp;rsquo;s what I mean&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-5&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-03.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Given the commit list from before&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and &amp;ldquo;lwb 1&amp;rdquo; as the currently &amp;ldquo;open&amp;rdquo; ZIL block&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We select the first &lt;code&gt;itx&lt;/code&gt; in the list&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-6&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-04.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;and copy this into the block&amp;rsquo;s buffer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This block will remain &amp;ldquo;open&amp;rdquo;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;as denoted by the dotted line&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;since it may also be used for the next &lt;code&gt;itx&lt;/code&gt; in the list.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-7&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-05.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;So, now we move on to the next &lt;code&gt;itx&lt;/code&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-8&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-06.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This one doesn&amp;rsquo;t quite fit in the currently &amp;ldquo;open&amp;rdquo; block&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;So&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-9&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-07.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We must allocate a new block, and issue the current one to disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Here, &amp;ldquo;lwb 1&amp;rdquo; now has a solid line, to indicate it&amp;rsquo;s been issued
to disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And &amp;ldquo;lwb 2&amp;rdquo; has a dashed line, to indicate it&amp;rsquo;s the new &amp;ldquo;open&amp;rdquo;
ZIL block.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, &amp;ldquo;lwb 1&amp;rdquo; maintains a pointer to &amp;ldquo;lwb 2&amp;rdquo; on disk.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now we can go back to processing the commit list&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-10&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-08.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and we copy the &lt;code&gt;itx&lt;/code&gt; into the new &lt;code&gt;lwb&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-11&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-09.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Finally, we reach the last &lt;code&gt;itx&lt;/code&gt; in the list&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-12&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-10.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since this fits into &amp;ldquo;lwb 2&amp;rdquo;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it&amp;rsquo;s copied directly into place.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At this point, the commit list is empty&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-13&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-2-11.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;so we issue &amp;ldquo;lwb 2&amp;rdquo; to disk&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;allocating the next &amp;ldquo;open&amp;rdquo; block in the process.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As one can see, &amp;ldquo;lwb 1 and 2&amp;rdquo; have a solid line&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to indicate they&amp;rsquo;ve both been issued to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;While, &amp;ldquo;lwb 3&amp;rdquo; has a dashed line, to indicate it&amp;rsquo;s now the new
&amp;ldquo;open&amp;rdquo; block&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and it will be used when writing out the next &amp;ldquo;batch&amp;rdquo; of
&lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk-3&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;del&gt;Move async itx&amp;rsquo;s for object being commited, to the sync list&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Write all commit list &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to disk&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wait for all ZIL block writes to complete&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;After we&amp;rsquo;ve issued all ZIL blocks to disk&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We must wait for them to complete.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-14&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-3-01.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The blocks can complete in any order&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-15&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-3-02.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Here, &amp;ldquo;lwb 2&amp;rdquo; completes first&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;example-zil-commit-object-b-16&#34;&gt;Example: &lt;code&gt;zil_commit&lt;/code&gt; Object B&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-commit-3-03.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;and then &amp;ldquo;lwb 1&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At this point, all blocks have completed&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk-4&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;del&gt;Move async itx&amp;rsquo;s for object being commited, to the sync list&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Write all commit list &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to disk&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Wait for all ZIL block writes to complete&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Flush VDEVs&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;So it&amp;rsquo;s time to issue a disk flush to all VDEVs involved&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-are-itx-s-written-to-disk-5&#34;&gt;How are itx&amp;rsquo;s written to disk?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;zil_commit&lt;/code&gt; handles the process of writing &lt;code&gt;itx_t&lt;/code&gt;&amp;rsquo;s to disk:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;del&gt;Move async itx&amp;rsquo;s for object being commited, to the sync list&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Write all commit list &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s to disk&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Wait for all ZIL block writes to complete&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;del&gt;Flush VDEVs&lt;/del&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Notify waiting threads&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Once those flushes complete&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we can notify any waiting threads&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;letting them know their data is safe on disk.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;3-ndash-problem&#34;&gt;3 &amp;ndash; Problem&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Now, let&amp;rsquo;s dive into the problem with all of this&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s grouped and written in &amp;ldquo;batches&amp;rdquo;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The commit list constitutes a batch&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Batch size proportional to sync workload on system&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Waiting threads only notified when &lt;em&gt;all&lt;/em&gt; ZIL blocks in batch complete&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Only a single batch processed at a time&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The main issues can be summarized into the following 3 points:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;First, &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are grouped and written &amp;ldquo;batches&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Where the commit list constitues a batch&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and the batch size is proportional to the sync workload on the
system.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Next, threads waiting for &lt;code&gt;zil_commit&lt;/code&gt; to complete, are only notified
when &lt;em&gt;all&lt;/em&gt; ZIL blocks in a given batch complete.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So, if a given batch is large, that means &lt;code&gt;zil_commit&lt;/code&gt; must wait
for all of those blocks to complete&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;even&lt;/em&gt; if the caller only cares about a small percentage of the
data in that batch.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And lastly, only a single batch can be processed and written out at
any given time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;problem-1&#34;&gt;Problem&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;problem.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Time spent servicing &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s for each disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Color indicates order waiting threads notified&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here&amp;rsquo;s an example of what this ends up looking like&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This is a timeline of the disk activity of an example system.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What can be seen here, is&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;block A through E are all written in the first batch&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;but the disk activity is slightly uneven&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;while disk 2, 3, and 4 only receive a single ZIL block to write&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;disk 1 receives two blocks.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, disks 2, 3, and 4 complete their writes and then remain idle&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;while disk 1 finishes its work.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This idle time is due to the fact that only a single batch can be
processed at a time&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;which leads to inefficient usage of the storage.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;blocks F, G, and H &lt;em&gt;could&lt;/em&gt; have been issued to disks 2, 3, and 4&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;filling this idle time&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;but the batching mechanism prevents this.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Another issue&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is the fact that waiting threads will only be notified once &lt;em&gt;all&lt;/em&gt;
blocks in a batch complete.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if a thread was waiting on data to be written by &amp;ldquo;block A&amp;rdquo;&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;it would have to wait for &amp;ldquo;block E&amp;rdquo; to be written as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This unnecessarily increases the latency of &lt;code&gt;zil_commit&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and, in this case, potentially doubling it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;3-ndash-solution&#34;&gt;3 &amp;ndash; Solution&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The solution is somewhat obvious&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;solution&#34;&gt;Solution&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Remove concept of &amp;ldquo;batches&amp;rdquo;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Allow &lt;code&gt;zil_commit&lt;/code&gt; to issue new ZIL block writes immediately&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In contrast to waiting for the current batch to complete&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Notify threads immediately when &lt;em&gt;dependent&lt;/em&gt; &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s on disk&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In contrast to waiting for &lt;em&gt;all&lt;/em&gt; &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s on disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s just remove the concept of &amp;ldquo;batches&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rather than waiting for the current batch to complete&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we should issue new ZIL blocks to disk immediately&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;as soon as they can be written out.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, rather than waiting for a batch to complete before notifying
threads&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;these threads should be notified immediately when their data is
safe on disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;problem-2&#34;&gt;Problem&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;problem.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Time spent servicing &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s for each disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Color indicates order waiting threads notified&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If we did that&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then we could go from this diagram, which I showed earlier&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;solution-1&#34;&gt;Solution&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;solution.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Time spent servicing &lt;code&gt;lwb&lt;/code&gt;&amp;rsquo;s for each disk&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Color indicates order waiting threads notified&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To this one.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Where all disks in the pool are saturated&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;and&lt;/em&gt; threads are notified as soon as each individual block
completes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;As this diagram illustrates&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we&amp;rsquo;d be able to service the same number of ZIL blocks&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;in nearly half the time&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;potentially doubling our IOPs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And, this is without changing a single thing about the workload&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;nor the underlying storage characteristics.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;4-ndash-details-on-the-changes-i-made&#34;&gt;4 &amp;ndash; Details on the Changes I Made&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;So how was this accomplished?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-01.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The bulk of changes revolve around 3 things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;changing how ZIL blocks are issued to disk&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;changing when the flush commands are sent&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and changing how we notify waiting threads.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Previously, this was a sequential 3 step process&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-02.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before-1&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 1 would consist of creating the ZIL blocks&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;issuing these to disk&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and then waiting for the IO for &lt;em&gt;all&lt;/em&gt; the blocks to complete.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-03.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before-2&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Next, after the blocks completed&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Step 2 would consist of issuing the flush to each VDEV&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and then waiting for those flushes to complete.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-04.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before-3&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Finally, after all flushes completed&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the ZIL&amp;rsquo;s CV would be signalled to notify any waiting threads.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All threads that called &lt;code&gt;zil_commit&lt;/code&gt; would be waiting on this CV&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;so this was the mechanism for letting them know their data was
safe on disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All 3 of these steps would consist of a single batch.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-05.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before-4&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After one batch completes&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;another would start up again.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-before-06.svg)
background-size: 65%&lt;/p&gt;

&lt;h2 id=&#34;before-5&#34;&gt;Before&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;And another.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This 3 step process would repeat as long as the workload would allow.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-01.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now the process is entirely different&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and heavily leaverages the ZIO infrastructure.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Instead of a single root ZIO for an entire batch of blocks&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;each block now has it&amp;rsquo;s own, unique, root ZIO.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each root will eventually have two children:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A &amp;ldquo;write&amp;rdquo; ZIO, containing the &lt;code&gt;itx&lt;/code&gt; data to be written&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And a &amp;ldquo;flush&amp;rdquo; ZIO, that is issued after the &amp;ldquo;write&amp;rdquo; completes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Since these are each child ZIOs, the &amp;ldquo;root&amp;rdquo; cannot complete until
both the &amp;ldquo;write&amp;rdquo; and the &amp;ldquo;flush&amp;rdquo; complete.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This is enforced by the pre-existing ZIO parent-child semantics.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Further, the &amp;ldquo;root&amp;rdquo; ZIO of the &amp;ldquo;previous&amp;rdquo; block&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;will also be a child of the &amp;ldquo;next&amp;rdquo; block.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;for example, here, &amp;ldquo;lwb 1&amp;rdquo; is a child of &amp;ldquo;lwb 2&amp;rdquo;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This ensures &lt;code&gt;lwb&lt;/code&gt; root ZIOs completes in the correct order&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;again, leveraging ZIO dependencies to acheive this.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Finally, each &lt;code&gt;lwb&lt;/code&gt; maintains a list of CVs&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;where each CV on this list maps to a single thread that called
&lt;code&gt;zil_commit&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Then, when the &amp;ldquo;root&amp;rdquo; ZIO for any ZIL block completes&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;each CV in the block&amp;rsquo;s list is signalled&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;notifying the waiting threads that their data is safe on disk.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s walk through an example of what this looks like&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-02.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-1&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, the &lt;code&gt;lwb&lt;/code&gt; structure and &amp;ldquo;root&amp;rdquo; ZIO is created.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Initially, the list of CVs will be empty&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;but as &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are copied into this block&amp;rsquo;s buffer&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;it will begin to accumulate a list of threads waiting on it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-03.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-2&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Next, when the block&amp;rsquo;s buffer is full, its write will be issued
to disk&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-04.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-3&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Then, when that write completes, the flush will be issued&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-05.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-4&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When the &amp;ldquo;flush&amp;rdquo; completes&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;and&lt;/em&gt; since this ZIL block doesn&amp;rsquo;t point to any previous block&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;this will allow the &amp;ldquo;root&amp;rdquo; IO for this specific block to complete&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;at which point, each CV in the block&amp;rsquo;s list will be signalled.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-06.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-5&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The same process will occur for the next block&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-07.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-6&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;And the next.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At this point, though, it&amp;rsquo;s important to note&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the sequence of events doesn&amp;rsquo;t have to occur in this specific order&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-08.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-7&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For example, it&amp;rsquo;s possible for &amp;ldquo;block 2&amp;rdquo; to issue it&amp;rsquo;s write&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;before the write for &amp;ldquo;block 1&amp;rdquo; completes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In this case&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the write for &amp;ldquo;block 1&amp;rdquo; would have been issued&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;then the write for &amp;ldquo;block 2&amp;rdquo;&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;even though block 1&amp;rsquo;s write was still being serviced by the disk.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Previously this would have been prevented due to the &amp;ldquo;batching&amp;rdquo;
mechanism&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the write for &amp;ldquo;block 2&amp;rdquo; could &lt;strong&gt;not&lt;/strong&gt; have been issued, until the
write for &amp;ldquo;block 1&amp;rdquo; was complete.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-09.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-8&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The same goes for the write for &amp;ldquo;block 3&amp;rdquo;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-10.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-9&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It&amp;rsquo;s even possible for the writes of blocks 2 and 3&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to complete before the write of block 1.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If this happens, though&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The root of block 2 and 3 will still be blocked&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;waiting for block 1 to complete.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, even if the flushes were issued and completed for
blocks 2 and 3&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;their CVs would &lt;strong&gt;not&lt;/strong&gt; get signalled yet.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-11.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-10&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As soon as the write for block 1 completes&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The flush would be issued&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;And once block 1&amp;rsquo;s flush completes&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(changes-after-12.svg)
background-size: 85%&lt;/p&gt;

&lt;h2 id=&#34;after-11&#34;&gt;After&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Then, all 3 blocks would &amp;ldquo;complete&amp;rdquo; simultaneously&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and the CVs for all of these blocks would be notified.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;So&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;while before&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;this process was very sequential.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now, it&amp;rsquo;s completely driven by the incoming sync workload&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and disk completion events.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ZIL blocks are created and issued whenever there&amp;rsquo;s data to write&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and waiting threads are notified whenever those writes complete.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;new-tunable-lwb-timeout&#34;&gt;New Tunable: &lt;code&gt;lwb&lt;/code&gt; Timeout&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;.pull-left[&lt;img src=&#34;zil-commit-2-10.svg&#34; alt=&#34;:scale 100%&#34; /&gt;]
.pull-right[&lt;img src=&#34;zil-commit-2-11.svg&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Before jumping into the performance results&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I wanted to quickly talk about how we determine when to issue
a ZIL block to disk.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you remember from earlier in the talk&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;we build up the these blocks by iterating over the commit list&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and copying each &lt;code&gt;itx&lt;/code&gt; into one of the &lt;code&gt;lwb&lt;/code&gt; buffers.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Previously, once we reached end of the commit list&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In essence, the end of a &amp;ldquo;batch&amp;rdquo;&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we would issue the last &lt;code&gt;lwb&lt;/code&gt; to disk.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now that we&amp;rsquo;re &amp;ldquo;batch-less&amp;rdquo; we don&amp;rsquo;t necessarily want to do that.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If we reach the end of the commit list&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;but there&amp;rsquo;s still buffer space available in the current block&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;for example, we could have a 128K ZIL block&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;with only a single 8K write in it&amp;hellip;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;then we &lt;em&gt;actually&lt;/em&gt; want to delay issuing that block&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;in case new &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are generated that would fit into that block.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This way, we can write out more &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s using fewer IOPs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The problem is, we have no way to predict the future&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We don&amp;rsquo;t know, &lt;em&gt;for sure&lt;/em&gt;, if more &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s will be generated.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, if we wait for future &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s but none are generated&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;then we&amp;rsquo;re adding additional latency to the current &lt;code&gt;lwb&lt;/code&gt;
for no benefit.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;But&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;if we don&amp;rsquo;t wait at all, and additional &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s &lt;em&gt;are&lt;/em&gt; generated&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;we could end up using more IOPs than we need to&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and potentially degrade performance by saturating the disk.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The solution we implemented is&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a ZIL block may be delayed up to 5% of the latency of the last
completed ZIL block.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For example, if the last block took 5ms to be serviced by the storage&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;than the next block will wait a maximum of 250us.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If it&amp;rsquo;s filled within that 250us, it&amp;rsquo;ll be issued to disk immediately.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If it&amp;rsquo;s not filled, it&amp;rsquo;ll timeout after 250us&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;and be issued to disk partially filled.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For those wondering, 5% is the default&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;and not that I recommend doing this&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;it &lt;em&gt;can&lt;/em&gt; that can be changed using the new tunable
(&lt;code&gt;zfs_commit_timeout_pct&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;class: middle, center&lt;/p&gt;

&lt;h1 id=&#34;5-ndash-performance-testing-and-results&#34;&gt;5 &amp;ndash; Performance testing and results&lt;/h1&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Finally, let&amp;rsquo;s go over the results of the performance tests that were
used to verify these changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(max-rate-hdd-iops-pctchange.svg)
background-size: 115%&lt;/p&gt;

&lt;h2 id=&#34;83-increase-in-iops-on-average-ndash-max-rate-ndash-8-hdds&#34;&gt;~83% Increase in IOPs on Average &amp;ndash; Max Rate &amp;ndash; 8 HDDs&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I used two different &lt;code&gt;fio&lt;/code&gt; workloads to verify.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For this workload&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;each &lt;code&gt;fio&lt;/code&gt; thread was submitting sync writes as fast as could&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and I measured the total number of IOPs that were achieved&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;while varying the number of &lt;code&gt;fio&lt;/code&gt; threads, from 2 to 1024.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;This graphs shows the percentage difference in IOPs&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;between illumos &lt;em&gt;with&lt;/em&gt; my changes, and illumos &lt;em&gt;without&lt;/em&gt; my changes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The dashed line at the bottom is a visual aid&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to highlight where in the graph a 0% difference is.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Anything above that line is improvement.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On average, I measured an 83% increase in IOPs with my changes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The dotted line is another visual aid&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;showing where exactly 83% improvment is&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;in relation to the actual measurements taken.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Additionally, the zpool used for this graph consisted of 8 traditional
spinning drives.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(max-rate-ssd-iops-pctchange.svg)
background-size: 115%&lt;/p&gt;

&lt;h2 id=&#34;48-increase-in-iops-on-average-ndash-max-rate-ndash-8-ssds&#34;&gt;~48% Increase in IOPs on Average &amp;ndash; Max Rate &amp;ndash; 8 SSDs&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I also ran that same workload on a zpool consisting of 8 SSDs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When running on SSDs, the improvment isn&amp;rsquo;t as dramatic&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;but I was &lt;em&gt;still&lt;/em&gt; able to measure about a 48% improvement,
on average.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The same visual aids are here&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;anything above the dashed line at the bottom is improvement&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;and the dotted line corresponds to the average.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(fixed-rate-hdd-lat-pctchange.svg)
background-size: 115%&lt;/p&gt;

&lt;h2 id=&#34;27-decrease-in-latency-on-average-ndash-fixed-rate-ndash-8-hdds&#34;&gt;~27% Decrease in Latency on Average &amp;ndash; Fixed Rate &amp;ndash; 8 HDDs&lt;/h2&gt;

&lt;p&gt;.footnote[&lt;sup&gt;*&lt;/sup&gt;IOPs increased with new code, and &amp;gt;64 threads; those data points omitted.]&lt;/p&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The second workload tested, was again using &lt;code&gt;fio&lt;/code&gt;&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;but this time&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;each &lt;code&gt;fio&lt;/code&gt; thread would attempt to issue a &lt;em&gt;maximum&lt;/em&gt; of 64 sync
writes per-second.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, the number of IOPs was constant with and without my changes&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;but&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;the latency of each sync write was still improved.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Since we&amp;rsquo;re now measuring latency, rather than IOPs&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;any value &lt;em&gt;below&lt;/em&gt; the dashed line is improvement.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When running this test on my 8 HDD pool&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I measured the latency of each sync write to decrease by an average
of 27%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Also worth noting&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the IOPs began to diverage at thread counts &amp;gt;64&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;where the new code started doing more IOPs than the old code&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;so, I removed those data points to keep the comparison &amp;ldquo;fair&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;background-image: url(fixed-rate-ssd-lat-pctchange.svg)
background-size: 115%&lt;/p&gt;

&lt;h2 id=&#34;16-decrease-in-latency-on-average-ndash-fixed-rate-ndash-8-ssds&#34;&gt;~16% Decrease in Latency on Average &amp;ndash; Fixed Rate &amp;ndash; 8 SSDs&lt;/h2&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;And lastly, I also ran this workload on my 8 SSD system.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The IOPs were the same for all thread counts on this pool&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;so I didn&amp;rsquo;t have to remove any data points.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Here, I measured the latency to decrease by an average of 16% on this
system.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;more-details&#34;&gt;More Details&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Two &lt;code&gt;fio&lt;/code&gt; workloads were used:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;each thread submitting sync writes as fast as it could&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;each thread submitting 64 sync writes per second&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1, 2, 4, and 8 disk zpools; both SSD and HDD&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;fio&lt;/code&gt; threads ranging from 1 to 1024; increasing in powers of 2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Full details can be found &lt;a href=&#34;https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;???&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here&amp;rsquo;s some more details on the performance tests that I ran&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I just wanted to include this in the slides for posterity&amp;rsquo;s sake.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Additionally, there&amp;rsquo;s a link to even more information about the
testing I did, and my results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Generating Code Coverage Reports for ZFS on Linux</title>
      <link>https://www.prakashsurya.com/post/2017-09-28-generating-code-coverage-reports-for-zfs-on-linux/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-28-generating-code-coverage-reports-for-zfs-on-linux/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is another post about collecting code coverage data for the ZFS on
Linux project. We&amp;rsquo;ve recently added a new make target to the project, so
I wanted to highlight how easy it is to use this to generate static HTML
based code coverage reports, and/or to generate a report that can be
used with other tools and services (e.g. &lt;a href=&#34;https://codecov.io/gh/zfsonlinux/zfs&#34;&gt;codecov.io&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Before I get into the specifics for how to run the tests and generate
the coverage report, I want to show off the results. After the tests
have been run, one can use &lt;code&gt;make code-coverage-capture&lt;/code&gt; to generate the
code coverage information.&lt;/p&gt;

&lt;p&gt;This generates a directory full of static HTML pages, and a &lt;code&gt;.info&lt;/code&gt;
file; both of which describe the lines executed for the set of tests
that were run. The HTML pages are to be consumed by humans, and the
&lt;code&gt;.info&lt;/code&gt; file consumed by tools.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;zfs-0.7.0-coverage/index.html&#34;&gt;here&lt;/a&gt; to view an example of the
static HTML pages.&lt;/li&gt;
&lt;li&gt;See &lt;a href=&#34;zfs-0.7.0-coverage.info&#34;&gt;here&lt;/a&gt; to view/download an example of
the &lt;code&gt;.info&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ll go into the details below, but that coverage was generated after
running &lt;code&gt;zloop&lt;/code&gt; and ZTS, and only contains data for user execution.
Generating this data for kernel execution isn&amp;rsquo;t difficult, it just
requires a little more work, and is out of scope for this post (I have
some notes on this at the end of this post).&lt;/p&gt;

&lt;h2 id=&#34;details&#34;&gt;Details&lt;/h2&gt;

&lt;p&gt;With the end result in mind, lets now dive into the details of how I
generated those reports.&lt;/p&gt;

&lt;p&gt;As one might expect, we first need to compile the ZFS project, and
ensure it&amp;rsquo;s configured to correctly generate the needed code coverage
data. We do this via the new &lt;code&gt;--enable-code-coverage&lt;/code&gt; flag to
&lt;code&gt;configure&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./autogen.sh &amp;amp;&amp;amp; ./configure --enable-code-coverage &amp;amp;&amp;amp; make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, before we jump into running the tests, we need to run the following
commands to setup our environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./scripts/zfs-tests.sh -c         # create and populate constrained PATH
$ sudo ./scripts/zfs-helpers.sh -vi # create various symlinks in &amp;quot;/&amp;quot;
$ sudo ./scripts/zfs.sh -u          # unload kernel modules
$ sudo ./scripts/zfs.sh             # load kernel modules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We need to run the tests completely in-tree, so that the &lt;code&gt;.gcda&lt;/code&gt; files
will be created within the ZFS repository tree. This enables the make
target to find these files, which it needs to generate the coverage
reports. Thus, the above commands are needed to configure the system to
enable running the tests in-tree vs. installing the various libraries,
commands, and kernel modules into the root filesystem.&lt;/p&gt;

&lt;p&gt;With that done, we can go ahead and run our first test. This test will
run &lt;code&gt;ztest&lt;/code&gt; in a loop for an hour:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo ./scripts/zloop.sh -t $((60*60))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that completes, we can run ZTS (ZFS Test Suite):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./scripts/zfs-tests.sh -vx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, now we can generate the code coverage data for the tests
that we ran above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export CODE_COVERAGE_BRANCH_COVERAGE=1
$ make code-coverage-capture
  LCOV   --capture zfs-0.7.0-coverage.info
  LCOV   --remove /tmp/*
lcov: WARNING: negative counts found in tracefile zfs-0.7.0-coverage.info.tmp
  GEN    zfs-0.7.0-coverage
file:///export/home/delphix/zfs/zfs-0.7.0-coverage/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it!&lt;/p&gt;

&lt;p&gt;After running that &lt;code&gt;make&lt;/code&gt; command it should emit a &lt;code&gt;.info&lt;/code&gt; file, and a
directory full of static HTML pages; much like the ones that I linked to
in the &amp;ldquo;Examples&amp;rdquo; section earlier in this post.&lt;/p&gt;

&lt;h2 id=&#34;more-details&#34;&gt;More Details&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;ve read this far, you might be interested in the following
details as well.&lt;/p&gt;

&lt;h3 id=&#34;branch-coverage&#34;&gt;Branch Coverage&lt;/h3&gt;

&lt;p&gt;We set the &lt;code&gt;CODE_COVERAGE_BRANCH_COVERAGE&lt;/code&gt; environment variable prior to
running &lt;code&gt;make code-coverage-capture&lt;/code&gt;, so that we&amp;rsquo;ll get coverage data
for which branches of a conditional are executed, and which branches are
not. This allows us to see when a conditional is executed, but only a
subset of all possibilities of the branch are covered.&lt;/p&gt;

&lt;p&gt;By default, &lt;code&gt;lcov&lt;/code&gt; (which is used by the make target) has this feature
disabled, so branch coverage will not be collected without setting this
variable.&lt;/p&gt;

&lt;h3 id=&#34;kernel-coverage&#34;&gt;Kernel Coverage&lt;/h3&gt;

&lt;p&gt;I mentioned this before, but it&amp;rsquo;s worth pointing out again. The
instructions in this post will only produce code coverage data for user
execution. Generating data for kernel execution isn&amp;rsquo;t much more
difficult than what&amp;rsquo;s described above, but it requires a couple of
additional things.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The kernel must be compiled with &lt;code&gt;gcov&lt;/code&gt; support. If your kernel
isn&amp;rsquo;t already compiled with this support, than you&amp;rsquo;ll have to build
a new kernel that&amp;rsquo;s configured to expose the required code coverage
data. I wrote some notes on this &lt;a href=&#34;http://localhost:1313/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/&#34;&gt;here&lt;/a&gt;. A more
authoritative source can be found &lt;a href=&#34;https://kernelnewbies.org/KernelBuild&#34;&gt;here&lt;/a&gt; and
&lt;a href=&#34;https://www.kernel.org/doc/html/latest/dev-tools/gcov.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Once running a kernel that supports exporting gcov data, the
coverage data for the kernel modules need to be copied into the zfs
tree prior to generating the reports (i.e. prior to running &lt;code&gt;make
code-coverage-capture&lt;/code&gt;). The following small script is one way this
can be done:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Allow access to gcov files as a non-root user
$ sudo chmod -R a+rx /sys/kernel/debug/gcov
$ sudo chmod a+rx /sys/kernel/debug

$ GCOV_KERNEL=&amp;quot;/sys/kernel/debug/gcov&amp;quot;
$ ZFS_BUILD=&amp;quot;$(readlink -f ~/zfs)&amp;quot;
$ pushd &amp;quot;$GCOV_KERNEL$ZFS_BUILD&amp;quot; &amp;gt;/dev/null
$ find . -name &amp;quot;*.gcda&amp;quot; -exec sh -c &#39;cp -v $0 &#39;$ZFS_BUILD&#39;/$0&#39; {} \;
$ find . -name &amp;quot;*.gcno&amp;quot; -exec sh -c &#39;cp -vdn $0 &#39;$ZFS_BUILD&#39;/$0&#39; {} \;
$ popd &amp;gt;/dev/null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea here, is to copy the kernel module&amp;rsquo;s &lt;code&gt;.gcda&lt;/code&gt; and &lt;code&gt;.gcno&lt;/code&gt;
files out of &lt;code&gt;/sys/kernel/debug/gcov&lt;/code&gt; and place them along side the
&lt;code&gt;.c&lt;/code&gt; file they correspond to. This way, these files can be found by
&lt;code&gt;make code-coverage-capture&lt;/code&gt; just like the user execution files. See
&lt;a href=&#34;https://github.com/prakashsurya/zfs-buildbot/blob/master/scripts/bb-test-cleanup.sh&#34;&gt;here&lt;/a&gt; for an example of how we&amp;rsquo;re doing this in
the ZFS on Linux project&amp;rsquo;s automation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;commit-adding-new-make-target&#34;&gt;Commit Adding New Make Target&lt;/h3&gt;

&lt;p&gt;The new &lt;code&gt;code-coverage-capture&lt;/code&gt; make target was added in this commit to
the ZFS on Linux project:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;commit acf044420b134b022da5c866b19df69934ad3778
Author: Prakash Surya &amp;lt;prakash.surya@delphix.com&amp;gt;
Date:   Fri Sep 22 18:49:57 2017 -0700

    Add support for &amp;quot;--enable-code-coverage&amp;quot; option

    This change adds support for a new option that can be passed to the
    configure script: &amp;quot;--enable-code-coverage&amp;quot;. Further, the &amp;quot;--enable-gcov&amp;quot;
    option has been removed, as this new option provides the same
    functionality (plus more).

    When using this new option the following make targets are available:

     * check-code-coverage
     * code-coverage-capture
     * code-coverage-clean

    Note: these make targets can only be run from the root of the project.

    Reviewed-by: Brian Behlendorf &amp;lt;behlendorf1@llnl.gov&amp;gt;
    Signed-off-by: Prakash Surya &amp;lt;prakash.surya@delphix.com&amp;gt;
    Closes #6670
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, in order to use this new make target, one will need to use a
version of the project that contains this commit.&lt;/p&gt;

&lt;h3 id=&#34;zfs-commit-tested&#34;&gt;ZFS Commit Tested&lt;/h3&gt;

&lt;p&gt;For the testing and coverage data I showed in the &amp;ldquo;Examples&amp;rdquo; section, I
used the following commit from the ZFS on Linux&amp;rsquo;s &amp;ldquo;master&amp;rdquo; branch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;commit 269db7a4b3ef2bc14f3c2cf95f050479cbd69e72
Author: Simon Guest &amp;lt;simon.guest@tesujimath.org&amp;gt;
Date:   Thu Sep 28 06:39:47 2017 +1300

    vdev_id: extension for new scsi topology

    On systems with SCSI rather than SAS disk topology, this change enables
    the vdev_id script to match against the block device path, and therefore
    create a vdev alias in /dev/disk/by-vdev.

    Reviewed-by: Tony Hutter &amp;lt;hutter2@llnl.gov&amp;gt;
    Reviewed-by: Brian Behlendorf &amp;lt;behlendorf1@llnl.gov&amp;gt;
    Signed-off-by: Simon Guest &amp;lt;simon.guest@tesujimath.org&amp;gt;
    Closes #6592
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ZFS on Linux Code Coverage</title>
      <link>https://www.prakashsurya.com/post/2017-09-26-zfs-on-linux-code-coverage/</link>
      <pubDate>Tue, 26 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-26-zfs-on-linux-code-coverage/</guid>
      <description>

&lt;h1 id=&#34;branches-pull-requests&#34;&gt;Branches + Pull Requests&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Code coverage data is collected for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;All commits merged to a branch (e.g. master)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;All pull requests for the &amp;ldquo;zfs&amp;rdquo; project&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Code coverage collected after running &lt;strong&gt;all&lt;/strong&gt; tests&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ztest, zfstest, zfsstress, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Data generated using &lt;code&gt;make code-coverage-capture&lt;/code&gt; &amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emits &lt;code&gt;.info&lt;/code&gt; file and static HTML pages&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;.info&lt;/code&gt; file uploaded to &lt;code&gt;codecov.io&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/zfsonlinux/zfs&#34;&gt;ZFS on Linux + Codecov&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;user-kernel&#34;&gt;User + Kernel&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Code coverage data is collected for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;User mode execution (e.g. libzpool, zdb, etc.)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kernel mode execution (e.g. &amp;ldquo;zfs&amp;rdquo; kernel module)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Same file &lt;em&gt;may&lt;/em&gt; be executed in user and kernel mode&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;e.g. libzpool references files in &amp;ldquo;modules&amp;rdquo; directories&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;User coverage enabled via &lt;code&gt;--enable-code-coverage&lt;/code&gt; option&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kernel coverage enabled via custom kernel&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;More details &lt;a href=&#34;https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-header&#34;&gt;Codecov: Header&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;header.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-diff&#34;&gt;Codecov: Diff&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;diff.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-files&#34;&gt;Codecov: Files&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;files.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-files-continued&#34;&gt;Codecov: Files (continued)&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;files-module-zfs.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-flags&#34;&gt;Codecov: Flags&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;flags.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-build&#34;&gt;Codecov: Build&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;build.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-graphs&#34;&gt;Codecov: Graphs&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;graphs.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;codecov-pr-comment&#34;&gt;Codecov: PR Comment&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;pr-comment.png&#34; alt=&#34;:scale 75%&#34; /&gt;]&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;coverage-integrated-w-commits-page&#34;&gt;Coverage Integrated w/ Commits Page&lt;/h1&gt;

&lt;p&gt;.center[&lt;img src=&#34;commits-page.png&#34; alt=&#34;:scale 100%&#34; /&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python &#43; Jupyter for Performance Testing</title>
      <link>https://www.prakashsurya.com/post/2017-09-19-python-plus-jupyter-for-performance-testing/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-19-python-plus-jupyter-for-performance-testing/</guid>
      <description>

&lt;h1 id=&#34;setting-the-stage&#34;&gt;Setting the stage.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Working on performance improvement to ZFS (sync writes)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To verify my changes, I needed to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Measure the performance of the system &lt;strong&gt;without&lt;/strong&gt; my changes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Measure the performance of the system &lt;strong&gt;with&lt;/strong&gt; my changes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Analyze the difference(s) in performance with and without my
changes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Collect tangential information from the system, to support
(or refute) my conclusions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;visualizations-required&#34;&gt;Visualizations required?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While not strictly required, visualizations are often powerful.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Examples:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Flamegraphs for on-CPU Analysis.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Heatmaps and/or Histograms for multi-modal latency data.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Simple 2D line graphs for high level application metrics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Thus, visualizations are &lt;em&gt;kind of&lt;/em&gt; required&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Analysis is prohibitively difficult without them.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;performance-testing-overview&#34;&gt;Performance testing overview.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Generally, performance testing takes the following approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Run some (usually known) workload.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Collect application and/or system metrics in some &amp;ldquo;random&amp;rdquo; format.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The format depends on the metric being collect.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Different metrics output data in different formats.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Consume metric data with a tool to generate visualizations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Analyze raw data and/or visualizations to form conclusions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Analysis must be easy to share&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;So it can be scrutinized by others.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learn, Refine, Repeat.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;always-use-the-right-tool-for-the-job&#34;&gt;Always use the right tool for the job.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Without proper tooling, any of the prior steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;can become tedious.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;can be done incorrectly (and lead to incorrect conclusions).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;can be insufficiently documented.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Without proper documentation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;mistakes can go unrecognized.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;methods cannot be shared.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;analysis cannot be scrutinized.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;conclusions can be forgotten.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;results cannot be reproduced.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &amp;ldquo;right tool&amp;rdquo; must enable solutions to these complications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;this-must-be-a-solved-problem-right&#34;&gt;This must be a solved problem&amp;hellip; right?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Rather than re-invent the wheel, lets learn from my co-workers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What tools were used for past performance related work?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Excel&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Google Spreadsheets&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;How was data transferred into the spreadsheet?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CSV file&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Copy/Paste&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Everything done in an ad-hoc basis, specific to each project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Workload chosen by developer, using tools familiar to them.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Usually, all steps in the process undocumented (often forgotten).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;ok-google-spreadsheets-it-is-take-one&#34;&gt;OK, Google Spreadsheets it is&amp;hellip; take one.&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;fio&amp;rdquo; was used; it output metrics about the IO it performs:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;  write: io=4171.3MB, bw=70549KB/s, iops=8818, runt= 60544msec
    clat (usec): min=680, max=2260.4K, avg=115400.25, stdev=214661.67
     lat (usec): min=681, max=2260.4K, avg=115401.19, stdev=214661.63
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;40 unique test configurations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;fio run with 10 different thread counts; 1 to 1024 threads.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;zpool used with 4 different disk counts; 1 to 8 disks.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;80 tests in total; 40 &lt;strong&gt;with&lt;/strong&gt; my changes, 40 &lt;strong&gt;without&lt;/strong&gt; my changes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each test, I would manually copy/paste fio data into spreadsheet.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Graphs generated from the data was nice&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Inputting data into the spreadsheet was terrible.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;end-result&#34;&gt;End result.&lt;/h1&gt;

&lt;p&gt;.center[```
iops    |    1   |    2   |     4   |     8   |     16   |     32   | &amp;hellip;
&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;ndash;+&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;+&amp;mdash;&amp;mdash;&amp;mdash;-+&amp;mdash;&amp;mdash;&amp;mdash;-+&amp;mdash;-
1 disk  | 164.79 | 328.89 |  672.61 | 1361.03 | 18414.90 | 19130.33 | &amp;hellip;
2 disks | 201.11 | 390.75 | 1171.71 | 1342.84 |  2630.02 |  5389.89 | &amp;hellip;
4 disks | 200.31 | 364.79 | 1184.56 | 2228.02 |  2677.17 |  5223.90 | &amp;hellip;
8 disks | 180.57 | 395.95 | 1158.49 | 1940.64 |  3602.46 |  5340.03 | &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-]&#34;&gt;
.center[![:scale 85%](spreadsheet-iops.png)]

 - Would use a meeting to discuss results, share analysis, etc.

---

# Google Spreadsheets; take two.

 - &amp;quot;Take one&amp;quot; was lame... So, I started looking for ways to improve it.

 - Discovered that fio can output JSON data using &amp;quot;--output-format&amp;quot;.

 - Maybe that, combined with &amp;quot;jq&amp;quot; and some Bash would help?

 - Wrote a Bash script to:

    - iterate over all fio JSON output files

    - use jq to parse IOPs data for each test iteration

    - output CSV (to stdout) for all 40 test configurations

 - I would then manually copy/paste CSV data into spreadsheet.

 - Using a CSV file rather than copy/paste isn&#39;t much different.

---

# Same result; easier to generate.

.center[```
iops    |    1   |    2   |     4   |     8   |     16   |     32   | ...
--------+--------+--------+---------+---------+----------+----------+----
1 disk  | 164.79 | 328.89 |  672.61 | 1361.03 | 18414.90 | 19130.33 | ...
2 disks | 201.11 | 390.75 | 1171.71 | 1342.84 |  2630.02 |  5389.89 | ...
4 disks | 200.31 | 364.79 | 1184.56 | 2228.02 |  2677.17 |  5223.90 | ...
8 disks | 180.57 | 395.95 | 1158.49 | 1940.64 |  3602.46 |  5340.03 | ...
```]

.center[![:scale 85%](spreadsheet-iops.png)]

 - Still no supporting documentation to explain process or results.

---

background-image: url(spreadsheet-iops.png)

# Huh.. The blue line looks different.. Why?

---

# Application metrics is not sufficient.

 - Started looking at data from &amp;quot;iostat&amp;quot;

 - Used a script to:

    - log &amp;quot;iostat&amp;quot; output to a file for each test configuration...

    - then parse the output files for each configuration...

    - then output CSV to standard output.

 - CSV data would be manually copied into the spreadsheet

 - Now, there were copy/pasted tables (and graphs) for:

    - fio IOPs vs. fio threads

    - iostat &amp;quot;%b&amp;quot;, &amp;quot;actv&amp;quot;, &amp;quot;asvc_t&amp;quot;, and &amp;quot;w/s&amp;quot; (each vs. fio threads)

 - Starting to encroach on original problem; too much copy/paste.

---

# iostat&#39;s %b vs. fio threads

.center[```
   %b   |   1   |   2   |   4   |   8   |   16  |   32  | ...
--------+-------+-------+-------+-------+-------+-------+----
1 disk  | 81.68 | 80.37 | 97.40 | 99.42 | 99.42 | 99.42 | ...
2 disks | 38.93 | 38.36 | 48.25 | 91.25 | 88.07 | 77.83 | ...
4 disks | 19.52 | 19.47 | 24.08 | 42.15 | 78.91 | 82.66 | ...
8 disks |  9.96 |  9.54 | 12.00 | 21.02 | 36.17 | 71.26 | ...
```]

.center[![:scale 100%](spreadsheet-pct-b.png)]

---

# iostat&#39;s actv vs. fio threads

.center[```
  actv  |   1   |   2   |   4   |   8   |   16  |   32  | ...
--------+-------+-------+-------+-------+-------+-------+----
1 disk  |  0.81 |  0.81 |  1.00 |  2.00 |  3.57 |  7.39 | ...
2 disks |  0.40 |  0.39 |  0.49 |  0.93 |  1.66 |  2.77 | ...
4 disks |  0.19 |  0.20 |  0.24 |  0.42 |  0.80 |  1.43 | ...
8 disks |  0.10 |  0.10 |  0.12 |  0.21 |  0.36 |  0.72 | ...
```]

.center[![:scale 100%](spreadsheet-actv.png)]

---

# iostat&#39;s asvc_t vs. fio threads

.center[```
 asvc_t |   1   |   2   |   4   |   8   |   16  |   32  | ...
--------+-------+-------+-------+-------+-------+-------+----
1 disk  |  5.04 |  4.97 |  5.84 |  5.80 |  0.98 |  1.65 | ...
2 disks |  4.27 |  4.26 |  3.56 |  5.49 |  5.07 |  4.08 | ...
4 disks |  4.03 |  4.38 |  3.42 |  3.15 |  4.75 |  4.40 | ...
8 disks |  5.65 |  4.02 |  3.43 |  3.55 |  3.23 |  4.34 | ...
```]

.center[![:scale 100%](spreadsheet-asvc-t.png)]

---

# Spreadsheets: decent, but far from ideal.

 - Pros:

    - No setup required.

    - Visualizations helped understand data.

 - Cons:

    - Data input is manual and error prone.

    - Available visualizations can be limiting; e.g. flamegraphs?

    - Code is seperate from data/visualizations.

    - No way to review code/data for correctness.&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;

    - No way to annotate data/visualizations with explanations.

.footnote[&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;http://www.nytimes.com/2013/04/19/opinion/krugman-the-excel-depression.html]

---

# Is there a better way?

 - Spent some time googling around for different ideas/approaches.

 - Discovered Jupyter and Jupyter Notebooks.

 - With Jupyter, I am able to:

    - Generate complex visualizations using Python libraries

    - Perform data analysis using Python libraries

    - Embed text/explanations inline with visualizations

    - Embed python data analysis code directly in the notebook

---

# What is Jupyter?

- Excerpt taken from &amp;quot;What is Jupyter?&amp;quot; article&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;But without attracting the hype, Jupyter Notebooks are revolutionizing
the way engineers and data scientists work together. If all important
work is collaborative, the most important tools we have are tools for
collaboration, tools that make working together more productive.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s what Jupyter is, in a nutshell: it&amp;rsquo;s a tool for collaborating.
Its built for writing and sharing code and text, within the context of
a web page.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;Code is never just code. It&amp;rsquo;s part of a thought process, an argument,
even an experiment. This is particularly true for data analysis, but
it&amp;rsquo;s true for almost any application. Jupyter lets you build a &amp;ldquo;lab
notebook&amp;rdquo; that shows your work: the code, the data, the results, along
with your explanation and reasoning.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
.footnote[&amp;lt;sup&amp;gt;1&amp;lt;/sup&amp;gt;https://www.oreilly.com/ideas/what-is-jupyter]

---

# Basic example 1: Visualizaing fio IOPs

 - Link to [notebook][basic-example-1-notebook]

 - Link to [nbviewer][basic-example-1-nbviewer]

---

# Basic example 2: Visualizaing iostat

 - Link to [notebook][basic-example-2-notebook]

 - Link to [nbviewer][basic-example-2-nbviewer]

---

# Real example: Results for OpenZFS #447

 - Link to [Max Rate Submit on HDDs][max-rate-submit-hdds]

 - Link to [Max Rate Submit on SSDs][max-rate-submit-ssds]

 - Link to [Fixed Rate Submit on HDDs][fixed-rate-submit-hdds]

 - Link to [Fixed Rate Submit on HDDs][fixed-rate-submit-hdds]

.footnote[&amp;lt;sup&amp;gt;\*&amp;lt;/sup&amp;gt;https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/]

---

# My Jupyter Tips

 - Use relavant Python libraries; e.g. Pandas, Matplotlib, etc.

 - Format output data to allow easier ingestion.

    - e.g. use `pandas.read_csv` rather than custom parsing.

 - Use `pandas.DataFrame`; makes data analysis and graphing easy:

    - `df.plot()` to plot data.

    - `df3 = df1 - df2` to determine the difference

 - `seaborn` library can help make graphs subjectively &amp;quot;prettier&amp;quot;.

    - As simple as `import seaborn` to change defaults

---

# How can YOU use Jupyter?

 - Official documentation: [Jupyter Notebook Quickstart](http://jupyter.readthedocs.io/en/latest/content-quickstart.html)

 - My notes: [Using Python and Jupyter for Performance Testing and Analysis](https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/)

 - The &amp;quot;`jupyter`&amp;quot; DCenter image is Ubuntu 17.04 with Jupyter pre-installed.

    - No LDAP; log in using `delphix` user and run `jupyter`

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$ jupyter notebook &amp;ndash;ip=0.0.0.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;Copy/paste this URL into your browser when you connect for the first time,
to login with a token:
    http://0.0.0.0:8888/?token=431434aa3c192dd33613c4bff990e4207a3af5e402f48012
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-can-we-use-jupyter&#34;&gt;How can WE use Jupyter?&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For notebooks that we don&amp;rsquo;t want publically accessible:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create an internal Jupyter service; e.g. &lt;code&gt;jupyter.delphix.com&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create an internal nbviewer service; e.g. &lt;code&gt;nbviewer.delphix.com&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Taking this a step further: &lt;a href=&#34;https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091&#34;&gt;Scaling Knowledge at Airbnb&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Teaching debugging techniques&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sharing novel, intesting, and/or complicated RCA of bugs&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ldquo;Marketing&amp;rdquo; what one is working on to peers/organization&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Disseminating random, but useful, TIL stories&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Would require a cultural shift, to adopt Jupyter effectively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Code Coverage for ZFS on Linux</title>
      <link>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</link>
      <pubDate>Mon, 18 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working with Brian Behlendorf on getting code coverage
information for the ZFS on Linux. The goal was to get code coverage data
for pull requests, as well as branches; this way, we can get a sense of
how well tested any given PR is by the automated tests, prior to landing
it. There&amp;rsquo;s still some wrinkles that need to be ironed out, but we&amp;rsquo;ve
mostly achieved that goal by leveraging &lt;a href=&#34;https://codecov.io/&#34;&gt;codecov.io&lt;/a&gt;, along
with a small &lt;a href=&#34;https://github.com/zfsonlinux/zfs-buildbot/blob/master/scripts/bb-test-cleanup.sh&#34;&gt;bash script&lt;/a&gt;. Here&amp;rsquo;s an &lt;a href=&#34;https://codecov.io/gh/zfsonlinux/zfs/pull/6566/diff&#34;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While what we&amp;rsquo;ve currently implemented is better than nothing, there&amp;rsquo;s
some potential improvements that&amp;rsquo;d I&amp;rsquo;d like to investigate when I have
time (and this post serves as a way to help me remember what they are):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Currently in the automated tests that are run for the ZFS on Linux
 project, the kernel&amp;rsquo;s code coverage data isn&amp;rsquo;t reset prior to
 running any tests. This is because in my testing, a hang would
 occur whenever I would write to the kernel&amp;rsquo;s &amp;ldquo;reset&amp;rdquo; file. For
 example, I would run this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; # echo &#39;&#39; &amp;gt; /sys/kernel/debug/gcov/reset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and it would never return; it would have the following backtrace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat /proc/1624/stack
[&amp;lt;ffffffffb736f501&amp;gt;] __synchronize_srcu+0xe1/0x100
[&amp;lt;ffffffffb736f5eb&amp;gt;] synchronize_srcu+0xcb/0x1c0
[&amp;lt;ffffffffb7749390&amp;gt;] debugfs_remove+0xa0/0xe0
[&amp;lt;ffffffffb73e5f82&amp;gt;] release_node+0x62/0x140
[&amp;lt;ffffffffb73e613b&amp;gt;] reset_write+0xdb/0x120
[&amp;lt;ffffffffb774a3e4&amp;gt;] full_proxy_write+0x84/0xd0
[&amp;lt;ffffffffb758a90f&amp;gt;] __vfs_write+0x3f/0x230
[&amp;lt;ffffffffb758e38e&amp;gt;] vfs_write+0xfe/0x270
[&amp;lt;ffffffffb758e889&amp;gt;] SyS_write+0x79/0x130
[&amp;lt;ffffffffb7f8083b&amp;gt;] entry_SYSCALL_64_fastpath+0x1e/0xa9
[&amp;lt;ffffffffffffffff&amp;gt;] 0xffffffffffffffff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, I never got around to doing any root cause analysis
of the issue. Instead, we simply aren&amp;rsquo;t resetting the kernel&amp;rsquo;s
coverage data, which is fine because we only upload a single unified
report to Codecov (which contains the data for all tests in the
automated test suite).&lt;/p&gt;

&lt;p&gt;Also, it&amp;rsquo;s worth noting that while writing this post, I attempted
that same command to clear the kernel&amp;rsquo;s coverage data, and I was
unable to reproduce the hang. I&amp;rsquo;m running a different kernel now, so
perhaps the previous behavior was a bug in the kernel that&amp;rsquo;s been
fixed in a later release? Without an RCA, anything is possible.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We&amp;rsquo;re not collecting any data for the userspace libraries, such as
 &lt;code&gt;libzpool&lt;/code&gt;; which unfortunately means we don&amp;rsquo;t get much coverage
 data from runs of &lt;code&gt;ztest&lt;/code&gt; and &lt;code&gt;zloop&lt;/code&gt;. The reason behind this is
 simply due to the additional implementation complexity required to
 support the libraries:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Some of the libraries are composed of the same source files as
 the kernel modules. Thus, we&amp;rsquo;d need to be careful when
 generating the gcov reports if we included these libaries, such
 that the kernel report for a given source file isn&amp;rsquo;t replaced
 by the userspace report (or vice versa). If we happened to be
 careless, it&amp;rsquo;d be easy to generate the kernel report for a
 source file, and then accidentally overwrite that report with
 the userspace data when generating the userspace report.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;code&gt;.gcda&lt;/code&gt; files for the libraries often are found in a
 &lt;code&gt;.libs&lt;/code&gt; sub-directory, rather than directly in the same
 directory as the source code (like non-libary sources). For
 example, the &lt;code&gt;.gcda&lt;/code&gt; files for &lt;code&gt;libzpool&lt;/code&gt; are here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ find lib/libzpool -name &#39;*.gcda&#39; | head -n 5
 lib/libzpool/.libs/zfs_rlock.gcda
 lib/libzpool/.libs/zpool_prop.gcda
 lib/libzpool/.libs/vdev_raidz.gcda
 lib/libzpool/.libs/dsl_pool.gcda
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is in contrast to where these files are stored for the
 commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ find cmd -name &#39;*.gcda&#39; | head -n 5
 cmd/zfs/zfs_iter.gcda
 cmd/zfs/zfs_main.gcda
 cmd/ztest/ztest.gcda
 cmd/mount_zfs/mount_zfs.gcda
 cmd/zpool/zpool_iter.gcda
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, we have to use the &lt;code&gt;-o&lt;/code&gt; option when running &lt;code&gt;gcov&lt;/code&gt;
 for the libraries, but not for the commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libzpool
 $ gcov -o .libs arc.gcno
 File &#39;../../module/zfs/arc.c&#39;
 Lines executed:60.72% of 3055
 Creating &#39;arc.c.gcov&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The gcov data files reference their corresponding source files
 using relative paths to parts of the project&amp;rsquo;s repository. This
 happens when a library references one of the source files from
 the &lt;code&gt;module&lt;/code&gt; directory (i.e. when file is built both for kernel
 mode and user mode), and is a result of how the build system
 works for these libraries.&lt;/p&gt;

&lt;p&gt;Thus, if &lt;code&gt;gcov&lt;/code&gt; is run from the &amp;ldquo;root&amp;rdquo; of the project
 directory, it will be unable to find the source files for that
 library. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ gcov -b lib/libzpool/arc.gcno
 ...
 Cannot open source file ../../module/zfs/arc.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be relatively easily worked around by first &lt;code&gt;cd&lt;/code&gt;-ing
 into the directory that contains the &lt;code&gt;.gcno&lt;/code&gt; file, and then
 running &lt;code&gt;gcov&lt;/code&gt;, but that idea falls apart for the &lt;code&gt;libicp&lt;/code&gt;
 library.&lt;/p&gt;

&lt;p&gt;For this libary, one must execute &lt;code&gt;gcov&lt;/code&gt; from the &lt;code&gt;lib/libicp&lt;/code&gt;
 directory, but the &lt;code&gt;.gcno&lt;/code&gt; files are stored in subdirectories
 from there. For example, this fails:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libicp/algs/sha2
 $ gcov -o .libs sha2.gcno
 File &#39;../../module/icp/algs/sha2/sha2.c&#39;
 Lines executed:67.41% of 135
 Creating &#39;sha2.c.gcov&#39;
 Cannot open source file ../../module/icp/algs/sha2/sha2.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this works:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libicp
 $ gcov -o algs/sha2/.libs algs/sha2/sha2.gcno
 File &#39;../../module/icp/algs/sha2/sha2.c&#39;
 Lines executed:67.41% of 135
 Creating &#39;sha2.c.gcov&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The last complication to consider is that, since we can have
 two different modes of execution for any given source file
 (since we compile certain files in both kernel and user mode),
 it&amp;rsquo;d be ideal to have seperate coverage reports for kernel and
 user mode execution. Codecov appears to support this via its
 concept of &amp;ldquo;flags&amp;rdquo; (i.e. using the &lt;code&gt;-F&lt;/code&gt; option of their &lt;a href=&#34;https://docs.codecov.io/v4.3.6/docs/about-the-codecov-bash-uploader&#34;&gt;Bash
 uploader&lt;/a&gt;), but that functionality still needs to be
 evaluated to make sure it&amp;rsquo;ll work for our needs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All in all, coming up with solutions for any (or all) of these
complications is possible, it just requires a little more work to be
done. For the first attempt at collecting coverage data, I opted to keep
the implementation simple yet functional and useful. I hope to revisit
these issues, and extend the coverage data to include the libraries
soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using &#34;gcov&#34; with ZFS on Linux Kernel Modules</title>
      <link>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</guid>
      <description>

&lt;h2 id=&#34;building-a-gcov-enabled-linux-kernel&#34;&gt;Building a &amp;ldquo;gcov&amp;rdquo; Enabled Linux Kernel&lt;/h2&gt;

&lt;p&gt;In order to extract &amp;ldquo;gcov&amp;rdquo; data from the Linux kernel, and/or Linux
kernel modules, a &amp;ldquo;gcov&amp;rdquo; enabled Linux kernel is needed. Since my
current development environment is based on Ubuntu 17.04, and the fact
that Ubuntu doesn&amp;rsquo;t provide a pre-built kernel with &amp;ldquo;gcov&amp;rdquo; enabled, I
had to build the kernel from source. This was actually pretty simple,
and most of that process is already documented &lt;a href=&#34;https://kernelnewbies.org/KernelBuild&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;install-the-kernel-build-dependencies&#34;&gt;Install the Kernel Build Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install libncurses5-dev \
      gcc make git exuberant-ctags bc libssl-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;obtaining-the-mainline-kernel-sources&#34;&gt;Obtaining the Mainline Kernel Sources&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/torvalds/linux.git ~/linux
$ cd ~/linux
$ git checkout v4.13
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;default-kernel-build-configuration&#34;&gt;Default Kernel Build Configuration&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cp /boot/config-$(uname -r) .config
$ yes &#39;&#39; | make oldconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adding-gcov-specific-configurations&#34;&gt;Adding &amp;ldquo;gcov&amp;rdquo; Specific Configurations&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ echo &#39;CONFIG_DEBUG_FS=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_KERNEL=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_FORMAT_AUTODETECT=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_PROFILE_ALL=y&#39; &amp;gt;&amp;gt; .config
$ yes &#39;&#39; | make oldconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;building-and-installing-the-custom-kernel&#34;&gt;Building and Installing the Custom Kernel&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ make -j$(nproc)
$ sudo make modules_install install
$ sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;verifying-gcov-support-is-enabled&#34;&gt;Verifying &amp;ldquo;gcov&amp;rdquo; Support is Enabled&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mount -t debugfs none /sys/kernel/debug
$ sudo ls -1d /sys/kernel/debug/gcov
/sys/kernel/debug/gcov
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;building-zfs-on-linux-modules&#34;&gt;Building ZFS on Linux Modules&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s nothing special that we need to do, in order to enable &amp;ldquo;gcov&amp;rdquo;
support for the ZFS on Linux modules. Since the kernel was built with
&amp;ldquo;gcov&amp;rdquo; enabled, the modules will be automatically built with it enabled
as well.&lt;/p&gt;

&lt;h3 id=&#34;building-the-spl-repository&#34;&gt;Building the SPL Repository&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/zfsonlinux/spl.git ~/spl
$ cd ~/spl
$ ./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;building-the-zfs-repository&#34;&gt;Building the ZFS Repository&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/zfsonlinux/zfs.git ~/zfs
$ cd ~/zfs
$ ./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;running-a-test-and-collecting-data-files&#34;&gt;Running a Test and Collecting Data Files&lt;/h2&gt;

&lt;p&gt;At this point, we&amp;rsquo;re running on a &amp;ldquo;gcov&amp;rdquo; enabled Linux kernel, and the
ZFS on Linux kernel modules should be installed such that we can load
them easily with &lt;code&gt;modprobe&lt;/code&gt;. Thus, we can finally run a trivial test,
loading and unloading the &amp;ldquo;zfs&amp;rdquo; module, and then use &lt;code&gt;gcov&lt;/code&gt; to
extract the code coverage information from that test.&lt;/p&gt;

&lt;h3 id=&#34;loading-and-unloading-the-zfs-module&#34;&gt;Loading and Unloading the &amp;ldquo;zfs&amp;rdquo; module&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo modprobe zfs
$ sudo modprobe -r zfs
$ dmesg | grep ZFS
[ 1214.307936] ZFS: Loaded module v0.7.0-1, ZFS pool version 5000, ZFS filesystem version 5
[ 1216.475316] ZFS: Unloaded module v0.7.0-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;collecting-gcov-data-files&#34;&gt;Collecting &amp;ldquo;gcov&amp;rdquo; Data Files&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo chmod a+rx /sys/kernel/debug
$ TEMPDIR=$(mktemp -d)
$ cd /sys/kernel/debug/gcov/$HOME
$ find . -type d -exec mkdir -p $TEMPDIR/{} \;
$ find . -name &#39;*.gcda&#39; -exec sh -c &#39;sudo cat $0 &amp;gt; &#39;$TEMPDIR&#39;/$0&#39; {} \;
$ find . -name &#39;*.gcno&#39; -exec sh -c &#39;cp -d $0 &#39;$TEMPDIR&#39;/$0&#39; {} \;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-data-files-to-extract-code-coverage-information&#34;&gt;Use Data Files to Extract Code Coverage Information&lt;/h2&gt;

&lt;p&gt;Now that we have the &amp;ldquo;gcov&amp;rdquo; data files stored in &lt;code&gt;$TEMPDIR&lt;/code&gt;, we can
finally use these to extract the code converage data for any particular
source code file that we&amp;rsquo;re interested in. For this example, we&amp;rsquo;re going
to look at the &lt;code&gt;zfs_ioctl.c&lt;/code&gt; file, and more specifically, at the &lt;code&gt;_fini&lt;/code&gt;
function which is run a single time when the &amp;ldquo;zfs&amp;rdquo; module is unloaded:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /tmp
$ gcov -o $TEMPDIR/zfs/module/zfs zfs_ioctl.c
$ cat zfs_ioctl.c.gcov
...
        -: 6751:static void __exit
        1: 6752:_fini(void)
        -: 6753:{
        1: 6754:        zfs_detach();
        1: 6755:        zfs_fini();
        1: 6756:        spa_fini();
        1: 6757:        zvol_fini();
        -: 6758:
        1: 6759:        tsd_destroy(&amp;amp;zfs_fsyncer_key);
        1: 6760:        tsd_destroy(&amp;amp;rrw_tsd_key);
        1: 6761:        tsd_destroy(&amp;amp;zfs_allow_log_key);
        -: 6762:
        1: 6763:        printk(KERN_NOTICE &amp;quot;ZFS: Unloaded module v%s-%s%s\n&amp;quot;,
        -: 6764:            ZFS_META_VERSION, ZFS_META_RELEASE, ZFS_DEBUG_STR);
        1: 6765:}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see that each line of the &lt;code&gt;_fini&lt;/code&gt; function was executed
exactly a single time, which is expected based on our trival load/unload
test case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Performance Testing Results for OpenZFS #447</title>
      <link>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</guid>
      <description>&lt;p&gt;The following are links to the Jupyter notebooks that describe the
performance testing that I did for &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;OpenZFS #447&lt;/a&gt;, and the results
of that testing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-max-rate-submit-hdd.ipynb
&#34;&gt;Max Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-max-rate-submit-ssd.ipynb
&#34;&gt;Max Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-fixed-rate-submit-hdd.ipynb
&#34;&gt;Fixed Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-fixed-rate-submit-ssd.ipynb
&#34;&gt;Fixed Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, a compressed tarball with all the raw data used to
generate those Jupyter notebooks can be found &lt;a href=&#34;openzfs-447-perf.tar.xz&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Python and Jupyter for Performance Testing and Analysis</title>
      <link>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I recently worked on some changes to the OpenZFS ZIL (see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt;),
and in the context of working on that project, I discovered some new
tools that helped me run my performance tests and analyze their
results. What follows is some notes on the tools that I used, and how I
used them.&lt;/p&gt;

&lt;h2 id=&#34;quick-overview&#34;&gt;Quick Overview&lt;/h2&gt;

&lt;p&gt;Before I dive into the details of how I used these tools, I wanted to
quickly go over what the tools were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/axboe/fio&#34;&gt;fio&lt;/a&gt; was used to generate the workload, and provide statistics
about the performance from the application&amp;rsquo;s perspective.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://jupyter.org/&#34;&gt;Jupyter&lt;/a&gt; was used for analysis of the test results; e.g.
performing data manipulations, generating visualizations, and
presenting the data/visualizations along with text explanations in
a unified document.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; was used for everything; e.g. running the tests,
capturing the results, analysis of the data, and generating
visualizations were all driven by python scripts. In addition to the
core language, the following modules were used:
&lt;a href=&#34;https://amoffat.github.io/sh/&#34;&gt;sh&lt;/a&gt;,
&lt;a href=&#34;http://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;,
&lt;a href=&#34;http://www.numpy.org/&#34;&gt;numpy&lt;/a&gt;,
&lt;a href=&#34;https://seaborn.pydata.org/&#34;&gt;seaborn&lt;/a&gt;,
and &lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pyenv/pyenv&#34;&gt;pyenv&lt;/a&gt; and &lt;a href=&#34;https://github.com/pyenv/pyenv-virtualenv&#34;&gt;pyenv-virtualenv&lt;/a&gt; were used to
provide an isolated Python environment.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://nbviewer.jupyter.org/&#34;&gt;nbviewer&lt;/a&gt; were used to share the
Jupyter notebooks, which contained the results of the tests.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;install-prerequisites-on-ubuntu-16-04&#34;&gt;Install Prerequisites on Ubuntu 16.04&lt;/h2&gt;

&lt;p&gt;Before we can use the above referenced tools, we first must download,
build, and/or install them such that they are available for us to use.
Below is some instructions for how to do that on an Ubuntu 16.04 VM that
I used; if using another operating system, the specific commands may not
work, but hopefully what&amp;rsquo;s included here can be easily adapted.&lt;/p&gt;

&lt;h3 id=&#34;install-build-dependencies&#34;&gt;Install Build Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt install -y git build-essential zlib1g-dev \
    libbz2-dev libssl-dev libreadline-dev libsqlite3-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-pyenv-and-pyenv-virtualenv&#34;&gt;Install &amp;ldquo;pyenv&amp;rdquo; and &amp;ldquo;pyenv virtualenv&amp;rdquo;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/pyenv/pyenv ~/.pyenv
$ git clone https://github.com/pyenv/pyenv-virtualenv.git \
    ~/.pyenv/plugins/pyenv-virtualenv

$ echo &#39;export PYENV_ROOT=&amp;quot;$HOME/.pyenv&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;eval &amp;quot;$(pyenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;eval &amp;quot;$(pyenv virtualenv-init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-python-3-and-create-virtual-environment&#34;&gt;Install Python 3 and Create Virtual Environment&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ pyenv install 3.6.2
$ pyenv virtualenv
$ pyenv virtualenv 3.6.2 jupyter-example
$ mkdir ~/jupyter-example
$ cd ~/jupyter-example
$ echo jupyter-example &amp;gt; .python-version
$ pip install jupyter pandas numpy seaborn matplotlib sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-and-install-fio&#34;&gt;Build and Install &amp;ldquo;fio&amp;rdquo;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/axboe/fio ~/fio
$ cd ~/fio
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-additional-utilities&#34;&gt;Install Additional Utilities&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt install -y zfsutils-linux jq sysstat
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generate-results-data-required-for-analysis&#34;&gt;Generate Results Data Required for Analysis&lt;/h2&gt;

&lt;p&gt;Now that all of the necessary tools have been installed, we can write a
Python script that uses these tools to run the tests and collect any
data that will be needed for proper analysis.&lt;/p&gt;

&lt;h3 id=&#34;generate-fio-test-configuration&#34;&gt;Generate &amp;ldquo;fio&amp;rdquo; Test Configuration&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ll be using &amp;ldquo;fio&amp;rdquo; to generate the workload. Let&amp;rsquo;s first create the
configuration file that will be passed to fio, which tells it how to
behave:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; ~/jupyter-example/workload.fio &amp;lt;&amp;lt;EOF
[global]
group_reporting
clocksource=cpu
ioengine=psync
fallocate=none
rw=write
blocksize=8k
time_based
iodepth=1
thread=0
direct=0
sync=1

[workload]
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;generate-python-script-to-run-fio-tests&#34;&gt;Generate Python Script to Run &amp;ldquo;fio&amp;rdquo; Tests&lt;/h3&gt;

&lt;p&gt;Now we&amp;rsquo;ll create the python script that will be used to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;create a ZFS pool and dataset from a known set of disks&lt;/li&gt;
&lt;li&gt;run &amp;ldquo;fio&amp;rdquo;, such that it uses the ZFS dataset previously created&lt;/li&gt;
&lt;li&gt;run &amp;ldquo;iostat&amp;rdquo;, collecting disk metrics concurrently while &amp;ldquo;fio&amp;rdquo; runs&lt;/li&gt;
&lt;li&gt;copy the data generated from &amp;ldquo;fio&amp;rdquo; and &amp;ldquo;iostat&amp;rdquo; to a &amp;ldquo;results&amp;rdquo;
 directory so they can be analyzed at a later time&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here&amp;rsquo;s what the script may look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; ~/jupyter-example/workload.py &amp;lt;&amp;lt;EOF
#!/usr/bin/env python3

import sh
import tempfile

def fio(directory, numjobs, disks, runtime=60):
  with tempfile.TemporaryDirectory(dir=&#39;/var/tmp&#39;) as tempdir:
    procs = []

    for d in disks:
      iostat = sh.iostat(&#39;-dxy&#39;, d, &#39;1&#39;,
                         _piped=True, _bg=True, _bg_exc=False)
      procs.append(iostat)

      grep = sh.grep(iostat, d, _bg=True, _bg_exc=False,
                     _out=&#39;{:s}/iostat-{:s}.txt&#39;.format(tempdir, d))
      procs.append(grep)

    sh.sudo.fio(&#39;--directory={:s}&#39;.format(directory),
                &#39;--size={:.0f}M&#39;.format(2**20 / numjobs),
                &#39;--numjobs={:d}&#39;.format(numjobs),
                &#39;--runtime={:d}&#39;.format(runtime),
                &#39;--output={:s}/{:s}&#39;.format(tempdir, &#39;fio.json&#39;),
                &#39;--output-format=json+&#39;,
                &#39;./workload.fio&#39;)

    for p in procs:
      try:
        sh.sudo.kill(p.pid)
        p.wait
      except (sh.ErrorReturnCode_1, sh.SignalException_SIGTERM):
        pass

    directory = &#39;results/{:d}-disks/{:d}-jobs&#39;.format(len(disks), numjobs)
    sh.mkdir(&#39;-p&#39;, directory)
    sh.rm(&#39;-rf&#39;, directory)
    sh.cp(&#39;-r&#39;, tempdir, directory)
    sh.chmod(&#39;755&#39;, directory)

def test(disks):
  for numjobs in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:
    try:
      sh.sudo.zpool(&#39;create&#39;, &#39;-f&#39;, &#39;tank&#39;, disks)
      sh.sudo.zfs(&#39;create&#39;, &#39;-o&#39;, &#39;recsize=8k&#39;, &#39;tank/dozer&#39;)
      fio(&#39;/tank/dozer&#39;, numjobs, disks)
    finally:
      sh.sudo.zpool(&#39;destroy&#39;, &#39;tank&#39;)

def main():
  sh.rm(&#39;-rf&#39;, &#39;results&#39;)
  sh.mkdir(&#39;results&#39;)

  test([&#39;sdb&#39;])
  test([&#39;sdb&#39;, &#39;sdc&#39;])
  test([&#39;sdb&#39;, &#39;sdc&#39;, &#39;sdd&#39;])

if __name__ == &#39;__main__&#39;:
  main()
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;run-python-script-to-generate-results&#34;&gt;Run Python Script to Generate Results&lt;/h3&gt;

&lt;p&gt;The python script created in the previous section can then be run, which
will generate a &amp;ldquo;results&amp;rdquo; directory with all of the raw data from &amp;ldquo;fio&amp;rdquo;
and &amp;ldquo;iostat&amp;rdquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ time python3 workload.py

real    34m55.771s
user    2m19.040s
sys     12m20.656s

(jupyter-example) $ ls results/*
results/1-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

results/2-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

results/3-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

(jupyter-example) $ ls results/*-disks/1024-jobs
results/1-disks/1024-jobs:
fio.json  iostat-sdb.txt

results/2-disks/1024-jobs:
fio.json  iostat-sdb.txt  iostat-sdc.txt

results/3-disks/1024-jobs:
fio.json  iostat-sdb.txt  iostat-sdc.txt  iostat-sdd.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see, each unique test configuration (i.e. number of disks in
the zpool, and number of &amp;ldquo;fio&amp;rdquo; threads) has its own directory containing
the results for that specific test configuration. The &amp;ldquo;fio.json&amp;rdquo;
contains the JSON formatted output from &amp;ldquo;fio&amp;rdquo;, and the &amp;ldquo;iostat-*.txt&amp;rdquo;
files contains the output from &amp;ldquo;iostat&amp;rdquo; for each specific disk.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a quick inspection of one of the &amp;ldquo;fio&amp;rdquo; files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ head results/2-disks/32-jobs/fio.json
{
  &amp;quot;fio version&amp;quot; : &amp;quot;fio-3.0-48-g83a9&amp;quot;,
  &amp;quot;timestamp&amp;quot; : 1504768846,
  &amp;quot;timestamp_ms&amp;quot; : 1504768846239,
  &amp;quot;time&amp;quot; : &amp;quot;Thu Sep  7 07:20:46 2017&amp;quot;,
  &amp;quot;global options&amp;quot; : {
    &amp;quot;directory&amp;quot; : &amp;quot;/tank/dozer&amp;quot;,
    &amp;quot;size&amp;quot; : &amp;quot;32768M&amp;quot;,
    &amp;quot;runtime&amp;quot; : &amp;quot;60&amp;quot;,
    &amp;quot;clocksource&amp;quot; : &amp;quot;cpu&amp;quot;,

(jupyter-example) $ jq -Mr .jobs[0].write.lat_ns results/2-disks/32-jobs/fio.json
{
  &amp;quot;min&amp;quot;: 712505,
  &amp;quot;max&amp;quot;: 312646560,
  &amp;quot;mean&amp;quot;: 5412690.837107,
  &amp;quot;stddev&amp;quot;: 7953187.998254
}

(jupyter-example) $ jq -Mr .jobs[0].write.iops results/2-disks/32-jobs/fio.json
5906.296339
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this, we can see that on average, each write made by fio took
roughly 5ms to complete. Additionally, fio averaged about 5.9K IOPs
during that specific test&amp;rsquo;s runtime.&lt;/p&gt;

&lt;p&gt;Similarly, we can briefly look at the iostat data collected for that
same test configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ head results/2-disks/32-jobs/iostat-sdb.txt
sdb               0.00     0.00    0.00  304.00     0.00 18116.50   119.19     0.54    1.78    0.00    1.78   1.45  44.00
sdb               0.00     0.00    0.00  667.00     0.00 60725.50   182.09     1.23    1.84    0.00    1.84   1.27  84.40
sdb               0.00     0.00    0.00  500.00     0.00 49707.50   198.83     0.92    1.86    0.00    1.86   1.58  78.80
sdb               0.00     0.00    0.00  502.00     0.00 45371.00   180.76     0.91    1.80    0.00    1.80   1.50  75.20
sdb               0.00     0.00    0.00  523.00     0.00 47920.50   183.25     1.07    2.06    0.00    2.06   1.48  77.20
sdb               0.00     0.00    0.00  598.00     0.00 58390.50   195.29     1.38    2.29    0.00    2.29   1.55  92.80
sdb               0.00     0.00    0.00  393.00     0.00 37456.00   190.62     0.98    2.49    0.00    2.49   2.06  80.80
sdb               0.00     0.00    0.00  527.00     0.00 48050.00   182.35     1.08    2.05    0.00    2.05   1.53  80.40
sdb               0.00     0.00    0.00  590.00     0.00 55418.00   187.86     1.24    2.10    0.00    2.10   1.48  87.60
sdb               0.00     0.00    0.00  663.00     0.00 64226.50   193.75     1.18    1.78    0.00    1.78   1.32  87.20

(jupyter-example) $ head results/2-disks/32-jobs/iostat-sdc.txt
sdc               0.00     0.00    0.00  295.00     0.00 25714.00   174.33     0.50    1.69    0.00    1.69   1.40  41.20
sdc               0.00     0.00    0.00  640.00     0.00 59921.00   187.25     1.22    1.90    0.00    1.90   1.33  85.20
sdc               0.00     0.00    0.00  509.00     0.00 53581.50   210.54     1.00    1.96    0.00    1.96   1.70  86.40
sdc               0.00     0.00    0.00  512.00     0.00 41799.00   163.28     0.95    1.85    0.00    1.85   1.48  75.60
sdc               0.00     0.00    0.00  538.00     0.00 48975.50   182.07     1.09    2.03    0.00    2.03   1.50  80.80
sdc               0.00     0.00    0.00  601.00     0.00 50568.00   168.28     1.36    2.25    0.00    2.25   1.50  90.40
sdc               0.00     0.00    0.00  395.00     0.00 38467.00   194.77     1.03    2.61    0.00    2.61   2.14  84.40
sdc               0.00     0.00    0.00  540.00     0.00 49031.00   181.60     1.03    1.91    0.00    1.91   1.45  78.40
sdc               0.00     0.00    0.00  580.00     0.00 53762.50   185.39     1.28    2.21    0.00    2.21   1.49  86.40
sdc               0.00     0.00    0.00  648.00     0.00 57146.50   176.38     1.19    1.83    0.00    1.83   1.31  85.20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These files contain the output of &amp;ldquo;iostat -dxy&amp;rdquo; for each device in the
ZFS pool used for the specific test configuration. In this case, the
pool consisted of 2 disks, &amp;ldquo;sdb&amp;rdquo; and &amp;ldquo;sdc&amp;rdquo;. Each line in the file
represents a 1 second interval.&lt;/p&gt;

&lt;p&gt;For completeness, since the iostat column headers are not included in
these files, here&amp;rsquo;s what they are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, we can see that usually an IO request sent to these disks was
serviced in less than 2ms, judging by the &lt;code&gt;svctm&lt;/code&gt; column.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-results-data-with-pandas-and-jupyter&#34;&gt;Analyzing Results Data with Pandas and Jupyter&lt;/h2&gt;

&lt;p&gt;Now that the data had been gathered from &amp;ldquo;fio&amp;rdquo; and &amp;ldquo;iostat&amp;rdquo; for all of
the test configurations, it was time to parse and analyze the data.&lt;/p&gt;

&lt;h3 id=&#34;parsing-fio-results-with-pandas&#34;&gt;Parsing &amp;ldquo;fio&amp;rdquo; Results with Pandas&lt;/h3&gt;

&lt;p&gt;To parse the &amp;ldquo;fio&amp;rdquo; results files, I used a combination of the previously
used &amp;ldquo;sh&amp;rdquo; Python module, and the &amp;ldquo;jq&amp;rdquo; command. Here&amp;rsquo;s some code that
would parse the the IOPs reported by fio for each test configuration,
and print the results as a table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ python3
Python 3.6.2 (default, Sep  6 2017, 21:23:54)
[GCC 5.4.0 20160609] on linux
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import pandas
&amp;gt;&amp;gt;&amp;gt; import sh
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; numjobs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
&amp;gt;&amp;gt;&amp;gt; numdisks = [1, 2, 3]
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; jq = sh.jq.bake(&#39;-M&#39;, &#39;-r&#39;)
&amp;gt;&amp;gt;&amp;gt; iops = pandas.DataFrame()
&amp;gt;&amp;gt;&amp;gt; for i in numdisks:
...   tmp = []
...   for j in numjobs:
...     data = jq(&#39;.jobs[0].write.iops&#39;,
...               &#39;results/{:d}-disks/{:d}-jobs/fio.json&#39;.format(i, j))
...     tmp.append(float(data.strip()))
...   iops[&#39;{:d} disks&#39;.format(i)] = pandas.Series(tmp, numjobs)
...
&amp;gt;&amp;gt;&amp;gt; print(iops)
          1 disks       2 disks       3 disks
1      693.355111    539.923004    399.353355
2      946.218459    817.014473    465.543389
4     1487.733742   1367.766810    817.556196
8     2597.526832   2714.935671    902.661356
16    3958.774225   4132.298054   3854.193054
32    6192.515329   5906.296339   5730.420583
64    7470.722522   8216.463923   8146.322770
128   8649.000333   9370.233395   9525.552187
256   8601.175804  10942.662150  10868.330780
512   7327.968610  11595.399661  11321.241803
1024  7792.668597  11137.351190  11391.167610
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, here&amp;rsquo;s code that would report the average write latency (in
nanoseconds) reported by fio for each test configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ python3
Python 3.6.2 (default, Sep  6 2017, 21:23:54)
[GCC 5.4.0 20160609] on linux
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import pandas
&amp;gt;&amp;gt;&amp;gt; import sh
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; numjobs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
&amp;gt;&amp;gt;&amp;gt; numdisks = [1, 2, 3]
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; jq = sh.jq.bake(&#39;-M&#39;, &#39;-r&#39;)
&amp;gt;&amp;gt;&amp;gt; lat = pandas.DataFrame()
&amp;gt;&amp;gt;&amp;gt; for i in numdisks:
...   tmp = []
...   for j in numjobs:
...     data = jq(&#39;.jobs[0].write.lat_ns.mean&#39;,
...               &#39;results/{:d}-disks/{:d}-jobs/fio.json&#39;.format(i, j))
...     tmp.append(float(data.strip()))
...   lat[&#39;{:d} disks&#39;.format(i)] = pandas.Series(tmp, numjobs)
...
&amp;gt;&amp;gt;&amp;gt; print(lat)
           1 disks       2 disks       3 disks
1     1.438905e+06  1.848592e+06  2.500436e+06
2     2.110580e+06  2.444135e+06  4.292531e+06
4     2.685592e+06  2.919950e+06  4.889253e+06
8     3.076803e+06  2.943452e+06  8.858595e+06
16    4.038235e+06  3.868267e+06  4.148222e+06
32    5.163324e+06  5.412691e+06  5.580323e+06
64    8.561707e+06  7.783084e+06  7.850346e+06
128   1.478921e+07  1.364914e+07  1.342665e+07
256   2.973660e+07  2.333695e+07  2.352923e+07
512   6.972930e+07  4.406545e+07  4.511673e+07
1024  1.305812e+08  9.137294e+07  8.931305e+07
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;visualizing-fio-results-with-jupyter&#34;&gt;Visualizing &amp;ldquo;fio&amp;rdquo; Results with Jupyter&lt;/h3&gt;

&lt;p&gt;While the text-based tables shown in the previous section are better
than nothing, Jupyter can be used to execute this parsing code, and
visualize the data using Python&amp;rsquo;s &amp;ldquo;matplotlib&amp;rdquo; graphing module.&lt;/p&gt;

&lt;p&gt;The Jupyter notebook software is easy to start up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ jupyter notebook --ip=0.0.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then one can navigate to the server running the notebook using a web
browser; e.g. I would enter &lt;code&gt;http://ps-jupyter.dcenter.delphix.com:8888&lt;/code&gt;
into my browser.&lt;/p&gt;

&lt;p&gt;If the &lt;code&gt;jupyter&lt;/code&gt; command is run from a local shell (e.g. on one&amp;rsquo;s
workstation), the &lt;code&gt;--ip&lt;/code&gt; option can be ommitted, and the command will
automatically attempt to open a new browser window with the notebook&amp;rsquo;s
URL already populated.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an &lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/visualizing-fio-results-with-jupyter.ipynb
&#34;&gt;example&lt;/a&gt; Jupyter notebook, migrating the parsing
code from the prior section into the notebook, and adding some more logic
to generate graphs rather than text-based tables.&lt;/p&gt;

&lt;h3 id=&#34;visualizing-iostat-results-with-jupyter&#34;&gt;Visualizing &amp;ldquo;iostat&amp;rdquo; Results with Jupyter&lt;/h3&gt;

&lt;p&gt;Similarly, the data from &amp;ldquo;iostat&amp;rdquo; can also be parsed and visualized just
like the &amp;ldquo;fio&amp;rdquo; data. Rather than repeat the explanations from the prior
sections, I&amp;rsquo;ll simply link directly to the &lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/visualizing-iostat-results-with-jupyter.ipynb
&#34;&gt;example&lt;/a&gt;
Jupyter notebook; which contains the code for both parsing the &amp;ldquo;iostat&amp;rdquo;
data files, as well as generating graphs from that data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building and Using &#34;crash&#34; on Ubuntu 16.04</title>
      <link>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve been working on the ZFS on Linux project recently, and had a need
to use &lt;code&gt;crash&lt;/code&gt; on the Ubuntu 16.04 based VM I was using. The following
is some notes regarding the steps I had to take, in order to build,
install, and ultimately run the utility against the &amp;ldquo;live&amp;rdquo; system.&lt;/p&gt;

&lt;h2 id=&#34;build-and-install-crash&#34;&gt;Build and Install &amp;ldquo;crash&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;First, I had to install the build dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install -y \
    git build-essential libncurses5-dev zlib1g-dev bison
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I could checkout the source code, build, and install:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/crash-utility/crash
$ cd crash
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;install-kernel-debug-symbols&#34;&gt;Install Kernel Debug Symbols&lt;/h2&gt;

&lt;p&gt;In order to run the &lt;code&gt;crash&lt;/code&gt; utility against the live system, we need
access to the uncompressed kernel image. We can obtain this by
installing the corresponding &lt;code&gt;-dbgsym&lt;/code&gt; package for the running kernel.&lt;/p&gt;

&lt;p&gt;First, we have to enable the APT repositories that contain these
packages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo tee /etc/apt/sources.list.d/ddebs.list &amp;lt;&amp;lt;EOF
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)          main restricted universe multiverse
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)-updates  main restricted universe multiverse
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)-proposed main restricted universe multiverse
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then add the correct key for these repositories:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-key adv \
    --keyserver keyserver.ubuntu.com \
    --recv-keys 428D7C01 C8CAB6595FDFF622
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now we can refresh the APT cache and install the kernel&amp;rsquo;s &lt;code&gt;-dbgsym&lt;/code&gt;
package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get update -y
$ sudo apt-get install -y linux-image-$(uname -r)-dbgsym
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;run-crash-against-live-system&#34;&gt;Run &amp;ldquo;crash&amp;rdquo; Against Live System&lt;/h2&gt;

&lt;p&gt;Finally, with all of the above done, I could run the &lt;code&gt;crash&lt;/code&gt; utility
against the running system like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo crash /usr/lib/debug/boot/vmlinux-$(uname -r)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a quick example for dumping the kernel backtrace for a specific
process (the process I needed to debug):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crash&amp;gt; bt 28365
PID: 28365  TASK: ffff887b7b978e00  CPU: 5   COMMAND: &amp;quot;fio&amp;quot;
 #0 [ffff887cd2167870] __schedule at ffffffff8183e9ee
 #1 [ffff887cd21678c0] schedule at ffffffff8183f0d5
 #2 [ffff887cd21678d8] spl_panic at ffffffffc02d70ca [spl]
 #3 [ffff887cd2167a60] zil_commit at ffffffffc1271daa [zfs]
 #4 [ffff887cd2167b50] zfs_write at ffffffffc1260d6d [zfs]
 #5 [ffff887cd2167d48] zpl_write_common_iovec at ffffffffc128b391 [zfs]
 #6 [ffff887cd2167dc0] zpl_iter_write at ffffffffc128b4e1 [zfs]
 #7 [ffff887cd2167e30] new_sync_write at ffffffff8120ec0b
 #8 [ffff887cd2167eb8] __vfs_write at ffffffff8120ec76
 #9 [ffff887cd2167ec8] vfs_write at ffffffff8120f5f9
#10 [ffff887cd2167f08] sys_pwrite64 at ffffffff81210465
#11 [ffff887cd2167f50] entry_SYSCALL_64_fastpath at ffffffff818431f2
    RIP: 00007f08274fbda3  RSP: 00007ffc1eae8a70  RFLAGS: 00000293
    RAX: ffffffffffffffda  RBX: 00007f080f6d8240  RCX: 00007f08274fbda3
    RDX: 0000000000002000  RSI: 0000000000f9b660  RDI: 0000000000000004
    RBP: 00007f080f6d8240   R8: 0000000000000000   R9: 0000000000000001
    R10: 0000000000374000  R11: 0000000000000293  R12: 000000000009bea3
    R13: 0000000000000001  R14: 0000000000002000  R15: 0000000000372000
    ORIG_RAX: 0000000000000012  CS: 0033  SS: 002b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And another example showing information about the running system:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crash&amp;gt; sys
      KERNEL: /usr/lib/debug/boot/vmlinux-4.4.0-93-generic
    DUMPFILE: /proc/kcore
        CPUS: 32
        DATE: Tue Sep  5 22:55:59 2017
      UPTIME: 06:58:42
LOAD AVERAGE: 512.32, 512.25, 512.26
       TASKS: 1035
    NODENAME: ubuntu-16-04
     RELEASE: 4.4.0-93-generic
     VERSION: #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017
     MACHINE: x86_64  (2199 Mhz)
      MEMORY: 512 GB
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Using BCC&#39;s &#34;trace&#34; Instead of &#34;printk&#34;</title>
      <link>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Recently I&amp;rsquo;ve been working on porting some changes that I made to the
OpenZFS ZIL over to the ZFS on Linux codebase; see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt;
for the OpenZFS pull request, and &lt;a href=&#34;https://github.com/zfsonlinux/zfs/pull/6566&#34;&gt;here&lt;/a&gt; for the ZFS on
Linux pull request.&lt;/p&gt;

&lt;p&gt;In my initial port, I was running into a problem where the automated
tests would trigger a &amp;ldquo;hang&amp;rdquo; as a result of the &lt;code&gt;readmmap&lt;/code&gt; program
calling &lt;code&gt;msync&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pstree -p 2337
test-runner.py(2337)-+-sudo(3183)---mmap_read_001_p(3185)---readmmap(3198)
                     `-{test-runner.py}(3184)

$ sudo cat /proc/3198/stack
[&amp;lt;ffffffff9bdafb68&amp;gt;] wait_on_page_bit_common+0x118/0x1d0
[&amp;lt;ffffffff9bdafd34&amp;gt;] __filemap_fdatawait_range+0x114/0x190
[&amp;lt;ffffffff9bdafdc4&amp;gt;] filemap_fdatawait_range+0x14/0x30
[&amp;lt;ffffffff9bdb2477&amp;gt;] filemap_write_and_wait_range+0x57/0x90
[&amp;lt;ffffffffc08f049d&amp;gt;] zpl_fsync+0x3d/0x110 [zfs]
[&amp;lt;ffffffff9be7b93b&amp;gt;] vfs_fsync_range+0x4b/0xb0
[&amp;lt;ffffffff9bdf6af2&amp;gt;] SyS_msync+0x182/0x200
[&amp;lt;ffffffff9c4d453b&amp;gt;] entry_SYSCALL_64_fastpath+0x1e/0xad
[&amp;lt;ffffffffffffffff&amp;gt;] 0xffffffffffffffff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this state was reached, the &lt;code&gt;msync&lt;/code&gt; call would never return.&lt;/p&gt;

&lt;p&gt;Without diving too far into the technical details, my hunch was that
the &lt;code&gt;zfs_putpage_commit_cb&lt;/code&gt; function was not being called properly. At
this point, I wanted to verify this, so I could then revisit the code
with concrete data to support my suspicision.&lt;/p&gt;

&lt;p&gt;If I hit this issue on illumos, I would have quickly jumped to using
either &lt;code&gt;dtrace&lt;/code&gt; or &lt;code&gt;mdb&lt;/code&gt; to help verify and debug the situation. Since
I was on Linux, I had neither of these tools at my disposal. Thankfully
though, I did have a test case that would reliably reproduce
the issue in a matter of minutes.&lt;/p&gt;

&lt;p&gt;I thought about adding some &lt;code&gt;printk&lt;/code&gt; or &lt;code&gt;zfs_dbgmsg&lt;/code&gt; statments to the
code and re-compiling to gather some data, but after having used
&lt;code&gt;dtrace&lt;/code&gt; on illumos, I resented that idea. I had previously read about
&lt;a href=&#34;http://www.brendangregg.com/blog/2016-03-05/linux-bpf-superpowers.html&#34;&gt;Linux BPF&lt;/a&gt; and &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;Linux BCC&lt;/a&gt;, so this felt like a
good opportunity try and experiment with those to see if I could use it
to gain better insight into my problem, without making any code changes.&lt;/p&gt;

&lt;h2 id=&#34;building-and-installing-bcc-from-source&#34;&gt;Building and Installing BCC from Source&lt;/h2&gt;

&lt;p&gt;First off, I wanted to build and install BCC from source, rather than
use a pre-built package. This decision was strictly for educational
purposes; I wanted to learn how easy or difficult this would be, in case
I ever wanted to make modifications to it in the future.&lt;/p&gt;

&lt;p&gt;The project contains easy to follow instructions for doing this in its
&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/INSTALL.md&#34;&gt;Install.md&lt;/a&gt; document. Since I was running on Ubuntu 17.04,
the process was documented and works as described:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Trusty and older
VER=trusty
$ echo &amp;quot;deb http://llvm.org/apt/$VER/ llvm-toolchain-$VER-3.7 main
$ deb-src http://llvm.org/apt/$VER/ llvm-toolchain-$VER-3.7 main&amp;quot; | \
  sudo tee /etc/apt/sources.list.d/llvm.list
$ wget -O - http://llvm.org/apt/llvm-snapshot.gpg.key | sudo apt-key add -
$ sudo apt-get update

# All versions
$ sudo apt-get -y install bison build-essential cmake flex git libedit-dev \
  libllvm3.7 llvm-3.7-dev libclang-3.7-dev python zlib1g-dev libelf-dev

# For Lua support
$ sudo apt-get -y install luajit luajit-5.1-dev

$ git clone https://github.com/iovisor/bcc.git
$ mkdir bcc/build; cd bcc/build
$ cmake .. -DCMAKE_INSTALL_PREFIX=/usr
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running those commands, I found the various BCC tools, examples,
and manpages installed in &lt;code&gt;/usr/share/bcc&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;using-bcc-s-trace&#34;&gt;Using BCC&amp;rsquo;s &amp;ldquo;trace&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Now that I had the BCC tools installed, I could use them to gather some
data about my issue. While there&amp;rsquo;s quite a few tools in the BCC
repository to help trace the various Linux subsystems, what I needed was
targeted specifically at the &lt;code&gt;zfs_putpage_commit_cb&lt;/code&gt; function. I wanted
to see if that function was getting called at all; and for that, the &lt;code&gt;trace&lt;/code&gt;
command was just what I needed.&lt;/p&gt;

&lt;p&gt;First I used this tool while running the ZFS modules without my changes,
to provide a baseline:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/share/bcc/tools/trace -K zfs_putpage_commit_cb 2&amp;gt;/dev/null
PID    TID    COMM         FUNC
27605  27605  readmmap     zfs_putpage_commit_cb
        zfs_putpage_commit_cb+0x1 [kernel]
        zil_commit+0x17 [kernel]
        zpl_writepages+0xd6 [kernel]
        do_writepages+0x1e [kernel]
        __filemap_fdatawrite_range+0xc6 [kernel]
        filemap_write_and_wait_range+0x41 [kernel]
        zpl_fsync+0x3d [kernel]
        vfs_fsync_range+0x4b [kernel]
        sys_msync+0x182 [kernel]
        entry_SYSCALL_64_fastpath+0x1e [kernel]
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the reproducer running in another shell, this told me that this
function definitely was being called when my changes were not applied,
and even provided a stack trace that lead to the function call.&lt;/p&gt;

&lt;p&gt;Now that I verified the behavior without my changes, it was time to run
the same test, but with my modified version of ZFS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/share/bcc/tools/trace -K zfs_putpage_commit_cb 2&amp;gt;/dev/null
PID    TID    COMM         FUNC
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just like before, I had the reproducer running in another shell, but
this time the &lt;code&gt;trace&lt;/code&gt; command didn&amp;rsquo;t produce any output, which means the
function wasn&amp;rsquo;t called.&lt;/p&gt;

&lt;p&gt;With these observations in mind, I was able to re-visit the code and
ultimately track down my problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Isolating ZIL Disk Activity</title>
      <link>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</guid>
      <description>&lt;p&gt;I recently completed a project to improve the performance of the OpenZFS
ZIL (see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt; for more
details); i.e. improving the performance of synchronous activity on
OpenZFS, such as writes using the &lt;code&gt;O_SYNC&lt;/code&gt; flag.  As part of that work,
I had to run some performance testing and benchmarking of my code
changes (and the system as a whole), to ensure the system was behaving
as I expected.&lt;/p&gt;

&lt;p&gt;Early on in my benchmarking exercises, I became confused by the data
that I was gathering. I was expecting only a certain number of writes to
be active on the underlying storage devices at any given time, based on
the known workload that I was applying to the zpool, and based on my
understanding of the ZIL&amp;rsquo;s mechanics. When running these known workloads
and inspecting the &lt;code&gt;actv&lt;/code&gt; column from &lt;code&gt;iostat&lt;/code&gt; though, I was
consistently seeing more write activity on the devices than I expected.&lt;/p&gt;

&lt;p&gt;At this point, I was starting to question my understanding of the code
that I had written, and my understanding of the ZIL&amp;rsquo;s mechanics as a
whole. Since I knew exactly the IO workload that was being applied to
the system, why wasn&amp;rsquo;t it behaving as I had predicted?&lt;/p&gt;

&lt;p&gt;After scratching my head and consulting the code numerous times, I asked
Matt Ahrens if he had any clues as to what might be going on. Matt was
quick to remind me that I was failing to incorporate the IO that would
occur as part of &lt;code&gt;spa_sync()&lt;/code&gt; into my mental model. Additionally, he
suggested that since it would be difficult to know exactly how many IOs
to expect from &lt;code&gt;spa_sync()&lt;/code&gt;, which then makes it difficult to verify the
&lt;code&gt;actv&lt;/code&gt; column from &lt;code&gt;iostat&lt;/code&gt; w.r.t. my code changes, I should configure
the system to effectively disable &lt;code&gt;spa_sync()&lt;/code&gt; altogether. This way, all
of the IOs that would be active on the disk would be a result of a ZIL
write, which is exactly what I was previously expecting.&lt;/p&gt;

&lt;p&gt;To acheive this configuration, Matt pointed me at the following kernel
perameters: &lt;code&gt;zfs_dirty_data_max&lt;/code&gt;, &lt;code&gt;zfs_dirty_data_sync&lt;/code&gt;, and
&lt;code&gt;zfs_txg_timeout&lt;/code&gt;. Basically, I had to set all of these values such that
the dirty limit would never be reached, and thus, a TXG sync would never
trigger as a result of the amount of dirty data my workload generated.
Since my test system had 128 GB of RAM, I used the following
commands/values to achieve this configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -kwe &#39;zfs_dirty_data_max/z 0t68719476736&#39;
$ sudo mdb -kwe &#39;zfs_dirty_data_sync/z 0t34359738368&#39;
$ sudo mdb -kwe &#39;zfs_txg_timeout/z 0t3600&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also important to note that these values are dependent on the rate
at which my workload would dirty data (i.e. the workload&amp;rsquo;s write
throughput), and the duration of the test. I ensured that the workload
would not be able to dirty enough data to cause TXG sync prior to the
test completing. With all of this configured correctly, the only way
writes would get issued to disk would be via the ZIL, which is exactly
what I wanted.&lt;/p&gt;

&lt;p&gt;Additionally, I further tuned the system to disable the IO aggregation
that ZFS may do when there&amp;rsquo;s sufficient write activity to warrant it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -kwe &#39;zfs_vdev_aggregation_limit/z 0t0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While this setting didn&amp;rsquo;t help my workload&amp;rsquo;s throughput, it did help me
validate the correctness of my code changes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running `sshd` on Windows using Cygwin</title>
      <link>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As part of our effort to support &lt;a href=&#34;https://www.delphix.com/&#34;&gt;Delphix&lt;/a&gt; in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Microsoft_Azure&#34;&gt;Azure&lt;/a&gt; cloud
environment, we&amp;rsquo;re writing some automation to convert our &lt;code&gt;.iso&lt;/code&gt; install
media into a &lt;a href=&#34;https://en.wikipedia.org/wiki/VHD_(file_format)&#34;&gt;VHD&lt;/a&gt; image, leveraging &lt;a href=&#34;https://en.wikipedia.org/wiki/Jenkins_(software)&#34;&gt;Jenkins&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Packer_(software)&#34;&gt;Packer&lt;/a&gt; in
the process.&lt;/p&gt;

&lt;p&gt;Essentially, we want to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Microsoft_Windows&#34;&gt;Windows&lt;/a&gt; server as a Jenkins &amp;ldquo;slave&amp;rdquo;,
and run Packer from within a Jenkins job that will run on that Windows
system.&lt;/p&gt;

&lt;p&gt;In order to do that, the Jenkins &amp;ldquo;master&amp;rdquo; needs to connect with the
Windows system, such that it can configure the system to act as a
Jenkins slave. Additionally, due to some of our existing Jenkins
infrastructure, this requires the Jenkins &amp;ldquo;master&amp;rdquo; using &lt;code&gt;ssh&lt;/code&gt; to
connect to the Windows box. Thus, &lt;code&gt;sshd&lt;/code&gt; is needed on the Windows
system.&lt;/p&gt;

&lt;h2 id=&#34;running-sshd-on-windows-using-cygwin&#34;&gt;Running &lt;code&gt;sshd&lt;/code&gt; on Windows using Cygwin&lt;/h2&gt;

&lt;p&gt;The following is a quick run-down of the steps I needed to perform, to
install and configure &lt;code&gt;sshd&lt;/code&gt; on our Windows server, enabling &lt;code&gt;ssh&lt;/code&gt;
clients (and our Jenkins master) to connect to it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install Cygwin on the Windows server:

&lt;ul&gt;
&lt;li&gt;I used RDP to connect to the server as the &lt;code&gt;Administrator&lt;/code&gt; user&lt;/li&gt;
&lt;li&gt;Used a web browser to access the &lt;a href=&#34;https://cygwin.com/install.html&#34;&gt;installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Downloaded the &lt;code&gt;setup-x86_64.exe&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;Executed &lt;code&gt;setup-x86_64.exe&lt;/code&gt;, and followed the setup instructions&lt;/li&gt;
&lt;li&gt;When reaching the package selection screen of the setup wizard, I
explicitly selected the following packages:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cygrunsrv&lt;/code&gt; from the &lt;code&gt;Admin&lt;/code&gt; group&lt;/li&gt;
&lt;li&gt;&lt;code&gt;openssh&lt;/code&gt; from the &lt;code&gt;Net&lt;/code&gt; group&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;After the Cygwin setup finished, I used the instructions found
 &lt;a href=&#34;http://www.noah.org/ssh/cygwin-sshd.html&#34;&gt;here&lt;/a&gt; to configure and start the &lt;code&gt;sshd&lt;/code&gt; service. This involved
 running the following commands in a Cygwin shell:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ssh-host-config -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cygrunsrv -S sshd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;At this point, the &lt;code&gt;sshd&lt;/code&gt; service was running, but the Windows
 server&amp;rsquo;s firewall was blocking incoming connections to port 22. So,
 I had to edit the server&amp;rsquo;s firewall rules to allow incomming
 connections on port 22 (the port &lt;code&gt;sshd&lt;/code&gt; was running on). For this,
 I followed the instructions that I found &lt;a href=&#34;https://techtorials.me/cygwin/configure-windows-firewall/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;installing-additional-software&#34;&gt;Installing Additional Software&lt;/h2&gt;

&lt;p&gt;In addition to installing Cygwin and running &lt;code&gt;sshd&lt;/code&gt;, we also needed to
perform a little more configuration to the Windows system, as our
Jenkins job had these additional dependencies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To allow ssh access to other systems in our environment from the
Jenkins job, the necessary &lt;code&gt;ssh&lt;/code&gt; keys had to be installed in the
&lt;code&gt;Administrator&lt;/code&gt; user&amp;rsquo;s home directory.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git&lt;/code&gt; was installed using the Cygwin installer.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;packer&lt;/code&gt; was downloaded from the project&amp;rsquo;s &lt;a href=&#34;https://www.packer.io/downloads.html&#34;&gt;download page&lt;/a&gt; and
installed simply by placing the binary at the correct location.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pv&lt;/code&gt; was manually compiled and installed following the directions on
the &lt;a href=&#34;https://sourceforge.net/projects/pvforcygwin/&#34;&gt;project&amp;rsquo;s website&lt;/a&gt;; i.e. using these commands:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wget http://www.ivarch.com/programs/sources/pv-1.3.4.tar.bz2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tar -xf pv-1.3.4.tar.bz2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd pv-1.3.4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./configure --prefix=&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Notes on ZIL Transactions</title>
      <link>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;http://open-zfs.org&#34;&gt;OpenZFS&lt;/a&gt; &lt;a href=&#34;http://nex7.blogspot.com/2013/04/zfs-intent-log.html&#34;&gt;Intent Log&lt;/a&gt; (ZIL) is used to ensure &lt;a href=&#34;https://en.wikipedia.org/wiki/POSIX&#34;&gt;POSIX&lt;/a&gt;
compliance of certain system calls (that modify the state of a ZFS
dataset), and protect against data loss in the face of failure scenarios
such as: an operating system crash, power loss, etc. Specifically, it&amp;rsquo;s
used as a performance optimization so that applications can be assured
that their given system call, and any &amp;ldquo;user data&amp;rdquo; associated with it,
will not be &amp;ldquo;lost&amp;rdquo;, without having to wait for an entire &lt;a href=&#34;https://www.delphix.com/blog/delphix-engineering/zfs-fundamentals-transaction-groups&#34;&gt;transaction
group&lt;/a&gt; (TXG) to be synced out (which can take on the order of
seconds, on a moderately loaded system).&lt;/p&gt;

&lt;p&gt;So how does this process work? How are these system calls tracked? This
post will attempt to explain how the system calls enter the ZIL, and how
they&amp;rsquo;re tracked by the in-memory portion of the ZIL. What isn&amp;rsquo;t covered
in this post is how the in-memory representation of the system calls get
written to disk, nor how the on-disk ZIL is used to &amp;ldquo;replay&amp;rdquo; the system
calls after a failure event.&lt;/p&gt;

&lt;h2 id=&#34;zfs-intent-log-transactions&#34;&gt;ZFS Intent Log Transactions&lt;/h2&gt;

&lt;p&gt;Much like how ZFS&amp;rsquo;s DMU layer operates on the notion of &amp;ldquo;transactions&amp;rdquo;,
so does the ZIL. For each system call that modifies a dataset&amp;rsquo;s state, a
ZIL transaction is created; referred to as an &lt;code&gt;itx&lt;/code&gt; in the code and this
document. It&amp;rsquo;s important to note, these &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are created for
&amp;ldquo;synchronous&amp;rdquo; system calls, as well as &amp;ldquo;asynchronous&amp;rdquo; ones. For example,
an &lt;code&gt;itx&lt;/code&gt; will be generated by an application calling &lt;code&gt;write&lt;/code&gt; on a ZFS
dataset, whether the &lt;code&gt;O_SYNC&lt;/code&gt; flag is used or not. Additionally, &lt;em&gt;all&lt;/em&gt;
system calls that modify a dataset will cause an &lt;code&gt;itx&lt;/code&gt; to be generated;
e.g. &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;rename&lt;/code&gt;, &lt;code&gt;setattr&lt;/code&gt;, etc. all will generate an &lt;code&gt;itx&lt;/code&gt;
unique to that system call.&lt;/p&gt;

&lt;h3 id=&#34;in-memory-representation-of-zil-transactions&#34;&gt;In-Memory Representation of ZIL Transactions&lt;/h3&gt;

&lt;p&gt;Each &lt;code&gt;itx&lt;/code&gt; is composed of an &lt;code&gt;itx_t&lt;/code&gt;, as well as a system call specific
component. The &lt;code&gt;itx_t&lt;/code&gt; portion looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct itx {
	list_node_t	itx_node;	/* linkage on zl_itx_list */
	void		*itx_private;	/* type-specific opaque data */
	itx_wr_state_t	itx_wr_state;	/* write state */
	uint8_t		itx_sync;	/* synchronous transaction */
	uint64_t	itx_sod;	/* record size on disk */
	uint64_t	itx_oid;	/* object id */
	lr_t		itx_lr;		/* common part of log record */
	/* followed by type-specific part of lr_xx_t and its immediate data */
} itx_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for a &lt;code&gt;write&lt;/code&gt; system call, the &lt;code&gt;lr_write_t&lt;/code&gt; structure would be
tacked onto the end of the &lt;code&gt;itx_t&lt;/code&gt;; this structure looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct {
	lr_t		lr_common;	/* common portion of log record */
	uint64_t	lr_foid;	/* file object to write */
	uint64_t	lr_offset;	/* offset to write to */
	uint64_t	lr_length;	/* user data length to write */
	uint64_t	lr_blkoff;	/* no longer used */
	blkptr_t	lr_blkptr;	/* spa block pointer for replay */
	/* write data will follow for small writes */
} lr_write_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, the in-memory representation for a &lt;code&gt;write&lt;/code&gt; &lt;code&gt;itx&lt;/code&gt; actually looks
like the following (assuming a small amount of data is being written):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct {
	itx_t common;
	lr_write_t uncommon;
	void *data;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The portion of the structure that&amp;rsquo;s common to all &lt;code&gt;itx&lt;/code&gt; types starts at
offset 0, the write specific portion immediately follows the &lt;code&gt;itx_t&lt;/code&gt;,
and then finally the user-data that is being written.&lt;/p&gt;

&lt;h3 id=&#34;determing-the-type-of-a-zil-transaction&#34;&gt;Determing the Type of a ZIL Transaction&lt;/h3&gt;

&lt;p&gt;Since the last structure depicted in the previous section doesn&amp;rsquo;t
actually exist, it&amp;rsquo;s important to understand how the code is able to
determine which structure follows the &lt;code&gt;itx_t&lt;/code&gt; portion of an &lt;code&gt;itx&lt;/code&gt;; since
it could be any one of the many options (e.g. &lt;code&gt;lr_write_t&lt;/code&gt;,
&lt;code&gt;lr_rename_t&lt;/code&gt;, &lt;code&gt;lr_setattr_t&lt;/code&gt;, etc).&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;lr_t&lt;/code&gt; contained within the &lt;code&gt;itx_t&lt;/code&gt; is used for this purpose. It
contains an &lt;code&gt;lrc_txtype&lt;/code&gt; field, which can be used to determine the
&lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s type. For example, if the &lt;code&gt;itx&lt;/code&gt; was for a &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;lrc_tx_type&lt;/code&gt;
would equal &lt;code&gt;TX_WRITE&lt;/code&gt;, and the &lt;code&gt;lr_write_t&lt;/code&gt; could be obtained like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ASSERT(itx-&amp;gt;itx_lr.lrc_txtype == TX_WRITE);
lr_write_t *lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;zil-transactions-by-example&#34;&gt;ZIL Transactions by Example&lt;/h2&gt;

&lt;p&gt;Now that we have a brief understanding of what an &lt;code&gt;itx&lt;/code&gt; is, lets look at
how these are generated and added to the ZIL; we&amp;rsquo;ll be using
&lt;code&gt;zfs_write&lt;/code&gt; as the example code path.&lt;/p&gt;

&lt;h3 id=&#34;accessing-the-in-memory-zil-structure&#34;&gt;Accessing the In-Memory ZIL Structure&lt;/h3&gt;

&lt;p&gt;The first line of code within &lt;code&gt;zfs_write&lt;/code&gt; that pertains to the ZIL&amp;rsquo;s
machinery is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zilog = zfsvfs-&amp;gt;z_log;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to manipulate the ZIL, a pointer to the ZIL structure (i.e. the
&lt;code&gt;zilog_t&lt;/code&gt;) is needed. This is obtained using the &lt;code&gt;vnode_t&lt;/code&gt; (passed in as
a parameter to &lt;code&gt;zfs_write&lt;/code&gt;) to extract a pointer to the corresponding
&lt;code&gt;znode_t&lt;/code&gt;; the &lt;code&gt;v_data&lt;/code&gt; field of the &lt;code&gt;vnode_t&lt;/code&gt; holds a pointer to the
&lt;code&gt;znode_t&lt;/code&gt;. Once we have the &lt;code&gt;znode_t&lt;/code&gt; for this specific file, it&amp;rsquo;s
trivial to use its &lt;code&gt;z_zfsvfs&lt;/code&gt; field to access the &lt;code&gt;zfsvfs_t&lt;/code&gt;, and then
the &lt;code&gt;z_log&lt;/code&gt; field to access the &lt;code&gt;zilog_t&lt;/code&gt;. It&amp;rsquo;s worth noting that the
&lt;code&gt;zilog_t&lt;/code&gt; is shared across all files in the same dataset (i.e. there&amp;rsquo;s a
single ZIL per ZFS dataset).&lt;/p&gt;

&lt;h3 id=&#34;zil-transaction-creation-and-assignment&#34;&gt;ZIL Transaction Creation and Assignment&lt;/h3&gt;

&lt;p&gt;Once the &lt;code&gt;zilog_t&lt;/code&gt; structure is obtained, the next line within
&lt;code&gt;zfs_write&lt;/code&gt; pertaining to the ZIL is this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zfs_log_write(zilog, tx, TX_WRITE, zp, woff, tx_bytes, ioflag);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;zfs_log_write&lt;/code&gt; function is used to create new &lt;code&gt;itx&lt;/code&gt;s, as well as
insert them into one of the ZIL&amp;rsquo;s list of transactions.&lt;/p&gt;

&lt;h4 id=&#34;zil-itx-create&#34;&gt;&lt;code&gt;zil_itx_create&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Inside of &lt;code&gt;zil_log_write&lt;/code&gt;, the function &lt;code&gt;zil_itx_create&lt;/code&gt; is used to
allocate one or more &lt;code&gt;itx_t&lt;/code&gt; structures to represent the &lt;code&gt;write&lt;/code&gt; system
within the in-memory portion of the ZIL. That can be seen here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (resid) {
	itx_t *itx;
	lr_write_t *lr;
	ssize_t len;

	/*
	 * If the write would overflow the largest block then split it.
	 */
	if (write_state != WR_INDIRECT &amp;amp;&amp;amp; resid &amp;gt; ZIL_MAX_LOG_DATA)
		len = SPA_OLD_MAXBLOCKSIZE &amp;gt;&amp;gt; 1;
	else
		len = resid;

	// ... &amp;lt;snip&amp;gt; ...

	itx = zil_itx_create(txtype, sizeof (*lr) +
	    (write_state == WR_COPIED ? len : 0));

	// ... &amp;lt;snip&amp;gt; ...

	resid -= len;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;zil_itx_create&lt;/code&gt; simply allocates and initializes the in-memory
&lt;code&gt;itx_t&lt;/code&gt; structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;itx_t *
zil_itx_create(uint64_t txtype, size_t lrsize)
{
	itx_t *itx;

	lrsize = P2ROUNDUP_TYPED(lrsize, sizeof (uint64_t), size_t);

	itx = kmem_alloc(offsetof(itx_t, itx_lr) + lrsize, KM_SLEEP);
	itx-&amp;gt;itx_lr.lrc_txtype = txtype;
	itx-&amp;gt;itx_lr.lrc_reclen = lrsize;
	itx-&amp;gt;itx_sod = lrsize; /* if write &amp;amp; WR_NEED_COPY will be increased */
	itx-&amp;gt;itx_lr.lrc_seq = 0;	/* defensive */
	itx-&amp;gt;itx_sync = B_TRUE;		/* default is synchronous */

	return (itx);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, for a single &lt;code&gt;write&lt;/code&gt; (and single call to &lt;code&gt;zfs_log_write&lt;/code&gt;), 1 or
more &lt;code&gt;itx&lt;/code&gt;s will be created to represent the system call.&lt;/p&gt;

&lt;h4 id=&#34;zil-itx-assign&#34;&gt;&lt;code&gt;zil_itx_assign&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;At this point, the &lt;code&gt;itx&lt;/code&gt;s representing the &lt;code&gt;write&lt;/code&gt; will have been
allocated and initialized, but they&amp;rsquo;re not yet part of the ZIL. In order
to add the &lt;code&gt;itx&lt;/code&gt;s to the ZIL, the &lt;code&gt;zil_itx_assign&lt;/code&gt; function is used.
Expanding the previous code snippet from &lt;code&gt;zfs_write&lt;/code&gt;, we can see that
&lt;code&gt;zil_itx_assign&lt;/code&gt; is called immediately after the individual &lt;code&gt;itx&lt;/code&gt;s are
created and initialized:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (resid) {
	itx_t *itx;
	lr_write_t *lr;
	ssize_t len;

	/*
	 * If the write would overflow the largest block then split it.
	 */
	if (write_state != WR_INDIRECT &amp;amp;&amp;amp; resid &amp;gt; ZIL_MAX_LOG_DATA)
		len = SPA_OLD_MAXBLOCKSIZE &amp;gt;&amp;gt; 1;
	else
		len = resid;

	itx = zil_itx_create(txtype, sizeof (*lr) +
	    (write_state == WR_COPIED ? len : 0));
	lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
	if (write_state == WR_COPIED &amp;amp;&amp;amp; dmu_read(zp-&amp;gt;z_zfsvfs-&amp;gt;z_os,
	    zp-&amp;gt;z_id, off, len, lr + 1, DMU_READ_NO_PREFETCH) != 0) {
		zil_itx_destroy(itx);
		itx = zil_itx_create(txtype, sizeof (*lr));
		lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
		write_state = WR_NEED_COPY;
	}

	itx-&amp;gt;itx_wr_state = write_state;
	if (write_state == WR_NEED_COPY)
		itx-&amp;gt;itx_sod += len;
	lr-&amp;gt;lr_foid = zp-&amp;gt;z_id;
	lr-&amp;gt;lr_offset = off;
	lr-&amp;gt;lr_length = len;
	lr-&amp;gt;lr_blkoff = 0;
	BP_ZERO(&amp;amp;lr-&amp;gt;lr_blkptr);

	itx-&amp;gt;itx_private = zp-&amp;gt;z_zfsvfs;

	if (!(ioflag &amp;amp; (FSYNC | FDSYNC)) &amp;amp;&amp;amp; (zp-&amp;gt;z_sync_cnt == 0) &amp;amp;&amp;amp;
	    (fsync_cnt == 0))
		itx-&amp;gt;itx_sync = B_FALSE;

	zil_itx_assign(zilog, itx, tx);

	off += len;
	resid -= len;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The purpose of &lt;code&gt;zil_itx_assign&lt;/code&gt; is to insert the new &lt;code&gt;itx&lt;/code&gt; into one of
the ZIL&amp;rsquo;s list of transactions; the details of that process is covered
next (it&amp;rsquo;s complicated enough to warrant a new section).&lt;/p&gt;

&lt;h3 id=&#34;zl-itxg-4-i-sync-list-i-async-tree-and-ia-list&#34;&gt;&lt;code&gt;zl_itxg[4]&lt;/code&gt;, &lt;code&gt;i_sync_list&lt;/code&gt;, &lt;code&gt;i_async_tree&lt;/code&gt;, and &lt;code&gt;ia_list&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;In order to fully understand how a given &lt;code&gt;itx&lt;/code&gt; gets inserted into the
in memory representation of the ZIL (via &lt;code&gt;zil_itx_assign&lt;/code&gt;), one has to
understand a few things about the &lt;code&gt;zilog_t&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;zl_itxg&lt;/code&gt; field of the &lt;code&gt;zilog_t&lt;/code&gt; contains 4 unique &lt;code&gt;itxg_t&lt;/code&gt;s,
 and each &lt;code&gt;itxg_t&lt;/code&gt; maps to a particular DMU transaction group. Each
 &lt;code&gt;itx&lt;/code&gt; created is applicable to a specific DMU transaction (&lt;code&gt;tx&lt;/code&gt;),
 which is then applicable to a specific DMU transaction group
 (&lt;code&gt;txg&lt;/code&gt;). Thus, the &lt;code&gt;tx&lt;/code&gt; is used to determine the &lt;code&gt;txg&lt;/code&gt; for this
 specific &lt;code&gt;itx&lt;/code&gt;, and the &lt;code&gt;txg&lt;/code&gt; is used to determine which of the 4
 different &lt;code&gt;itxg_t&lt;/code&gt;s (from the &lt;code&gt;zl_itxg&lt;/code&gt; array) will be used when
 inserting the &lt;code&gt;itx&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each of the 4 &lt;code&gt;itxg_t&lt;/code&gt;&amp;rsquo;s maintain a:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i_sync_list&lt;/code&gt; which is a simple linked list of &lt;code&gt;itx_t&lt;/code&gt; structures.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i_async_tree&lt;/code&gt; which is an AVL tree of &lt;code&gt;itx_async_node_t&lt;/code&gt;
structures (indexed by DMU object ID), where each node in the tree
(each &lt;code&gt;itx_async_node_t&lt;/code&gt;) maintains its own &lt;code&gt;ia_list&lt;/code&gt;, which is a
linked list of &lt;code&gt;itx_t&lt;/code&gt; structures.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Below is a diagram that attempts to illustrate this (it&amp;rsquo;s not as
complicated as the above explanation might sound):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-itx-lists.png&#34; alt=&#34;ZIL Transaction Lists&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;insertion-of-a-zil-transaction&#34;&gt;Insertion of a ZIL Transaction&lt;/h3&gt;

&lt;p&gt;So, when &lt;code&gt;zil_itx_assign&lt;/code&gt; is called in the context of &lt;code&gt;zfs_log_write&lt;/code&gt;,
the &lt;code&gt;itx&lt;/code&gt; will be inserted into one of &lt;code&gt;itxg_t&lt;/code&gt;&amp;rsquo;s lists described in the
previous section. The following logic/criteria is used to decide which
list to use, when inserting the new &lt;code&gt;itx&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First, which of the 4 &lt;code&gt;itxg_t&lt;/code&gt;s needs to be chosen. The &lt;code&gt;txg&lt;/code&gt; that
 the given &lt;code&gt;itx&lt;/code&gt; is associated with is used to make this decision.
 &lt;code&gt;dmu_tx_get_txg&lt;/code&gt; is used to obtain the &lt;code&gt;txg&lt;/code&gt; from the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s &lt;code&gt;tx&lt;/code&gt;,
 and then the &lt;code&gt;txg&lt;/code&gt; is bit-wise AND-ed with &lt;code&gt;TXG_MASK&lt;/code&gt; to determine
 which &lt;code&gt;itxg_t&lt;/code&gt; to use.&lt;/li&gt;
&lt;li&gt;Now that the &lt;code&gt;itxg_t&lt;/code&gt; is chosen, the linked list that will be used
 when inserting the &lt;code&gt;itx&lt;/code&gt; needs to be decided. This depends on the
 value of the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s &lt;code&gt;itx_sync&lt;/code&gt; field (i.e. based on if the write
 is synchronous or not):

&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;itx_sync&lt;/code&gt; is &lt;code&gt;TRUE&lt;/code&gt;, the &lt;code&gt;itx&lt;/code&gt; is appended to the
&lt;code&gt;i_sync_list&lt;/code&gt; of the &lt;code&gt;itxg_t&lt;/code&gt; chosen in (1).&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;itx_sync&lt;/code&gt; is &lt;code&gt;FALSE&lt;/code&gt;, &lt;code&gt;avl_find&lt;/code&gt; is used to find the
&lt;code&gt;itx_asnyc_node_t&lt;/code&gt; for the specific file being written, from
the &lt;code&gt;i_async_tree&lt;/code&gt; of the &lt;code&gt;itxg_t&lt;/code&gt; chosen in (1). Once the
&lt;code&gt;itx_async_node_t&lt;/code&gt; is found, the &lt;code&gt;itx&lt;/code&gt; is appended to that
structure&amp;rsquo;s &lt;code&gt;ia_list&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the &lt;code&gt;itx&lt;/code&gt; has been assigned to either the &lt;code&gt;i_sync_list&lt;/code&gt; or one of
the &lt;code&gt;ia_list&lt;/code&gt; lists (contained in the &lt;code&gt;itx_async_node_t&lt;/code&gt;), the work of
&lt;code&gt;zfs_log_write&lt;/code&gt; is finished. This specific &lt;code&gt;write&lt;/code&gt; system call has been
recorded and inserted into the in-memory representation of the ZIL, but
this record will still be lost if a power loss were to happen at this
point.&lt;/p&gt;

&lt;p&gt;In order for the record of the &lt;code&gt;write&lt;/code&gt; to persist in the event of a
power loss (or an equivalent operating system crash), the &lt;code&gt;itx&lt;/code&gt;s need
to be written to the on-disk representation of the ZIL. Notes on that
process will be left for another post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Refresher on `zpool reguid` Using Examples</title>
      <link>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;zpool reguid&lt;/code&gt; command can be used to regenerate the &lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSGuids&#34;&gt;GUID&lt;/a&gt;
for an &lt;a href=&#34;http://www.open-zfs.org&#34;&gt;OpenZFS&lt;/a&gt; pool, which is useful when using device level
copies to generate multiple pools all with the same contents.&lt;/p&gt;

&lt;h2 id=&#34;example-using-file-vdevs&#34;&gt;Example using File VDEVs&lt;/h2&gt;

&lt;p&gt;As a contrived example, lets create a zpool backed by a single file
vdev:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mkdir /tmp/tank1
# mkfile -n 256m /tmp/tank1/vdev
# zpool create tank1 /tmp/tank1/vdev
# zpool list tank1
NAME    SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank1   240M    78K   240M         -     1%     0%  1.00x  ONLINE  -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can export this zpool, copy the file, and attempt to import two
identical zpools; one backed by the original file we created above, and
another zpool backed by the copy of the original file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank1
# mkdir /tmp/tank2
# cp /tmp/tank1/vdev /tmp/tank2/vdev
# zpool import -d /tmp/tank1 tank1
# zpool import -d /tmp/tank2 tank1 tank2
cannot import &#39;tank1&#39;: a pool with that name is already created/imported,
and no additional pools with that name were found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While the intention was to have two pools imported, &lt;code&gt;tank1&lt;/code&gt; and &lt;code&gt;tank2&lt;/code&gt;,
as one can see, the second &lt;code&gt;zpool import&lt;/code&gt; failed. The failure was due
to the two pools sharing the same GUID; we can verify the two pools
share the same GUID using &lt;code&gt;zdb&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank1
# zdb -e -p /tmp/tank1 -C tank1 | grep pool_guid
        pool_guid: 750003400893681264
# zdb -e -p /tmp/tank2 -C tank1 | grep pool_guid
        pool_guid: 750003400893681264
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is where &lt;code&gt;zpool reguid&lt;/code&gt; becomes useful; after importing &lt;code&gt;tank1&lt;/code&gt; we
can use &lt;code&gt;zpool reguid&lt;/code&gt; to change that zpool&amp;rsquo;s GUID such that we can then
import &lt;code&gt;tank2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool import -d /tmp/tank1 tank1
# zpool reguid tank1
# zpool import -d /tmp/tank2 tank1 tank2
# zpool list tank1 tank2
NAME    SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank1   240M   110K   240M         -     3%     0%  1.00x  ONLINE  -
tank2   240M   114K   240M         -     2%     0%  1.00x  ONLINE  -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, we can use &lt;code&gt;zdb&lt;/code&gt; again to verify the GUID for the two
pools is different:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zdb -C tank1 | grep pool_guid
        pool_guid: 12195257967456241841
# zdb -C tank2 | grep pool_guid
        pool_guid: 750003400893681264
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: we don&amp;rsquo;t need the &lt;code&gt;-e&lt;/code&gt; and &lt;code&gt;-p&lt;/code&gt; options like before, because these
two pools are imported.&lt;/p&gt;

&lt;h2 id=&#34;examples-using-damaged-zpools&#34;&gt;Examples Using Damaged ZPOOLs&lt;/h2&gt;

&lt;p&gt;The manpage for the &lt;code&gt;zpool reguid&lt;/code&gt; command states this can only be used
on zpools that are &amp;ldquo;online and healthy&amp;rdquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     zpool reguid pool
             Generates a new unique identifier for the pool. You must ensure
             that all devices in this pool are online and healthy before
             performing this action.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, what happens when a device is damaged and this command is attempted
to be used? Let&amp;rsquo;s try some examples and find out.&lt;/p&gt;

&lt;h3 id=&#34;example-using-a-damaged-vdev-in-an-empty-zpool&#34;&gt;Example Using a Damaged VDEV in an Empty ZPOOL&lt;/h3&gt;

&lt;p&gt;For this example, we&amp;rsquo;ll create another zpool using file based VDEVs, but
this time the zpool will be striped across 3 files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mkdir /tmp/tank
# mkfile -n 256m /tmp/tank/vdev1
# mkfile -n 256m /tmp/tank/vdev2
# mkfile -n 256m /tmp/tank/vdev3
# zpool create tank /tmp/tank/vdev{1,2,3}
# zpool list -v tank
NAME                SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank                720M    78K   720M         -     0%     0%  1.00x  ONLINE  -
  /tmp/tank/vdev1   240M    30K   240M         -     0%     0%
  /tmp/tank/vdev2   240M    18K   240M         -     0%     0%
  /tmp/tank/vdev3   240M    30K   240M         -     0%     0%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets damage the pool by writing random data over the most of one of
the VDEVs. We explicitly avoid writing to the first and last 1MB of the
file to leave the VDEV&amp;rsquo;s labels intact.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@oi-hipster:~# dd if=/dev/urandom of=/tmp/tank/vdev3 bs=1M seek=1 count=254
254+0 records in
254+0 records out
266338304 bytes transferred in 4.534832 secs (58731677 bytes/sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To verify the damage, we use &lt;code&gt;zpool scrub&lt;/code&gt; and &lt;code&gt;zpool status&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool scrub
# zpool status -v tank
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
        using &#39;zpool clear&#39; or replace the device with &#39;zpool replace&#39;.
   see: http://illumos.org/msg/ZFS-8000-9P
  scan: scrub repaired 28.5K in 0h0m with 0 errors on Wed Feb 22 22:34:18 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               DEGRADED     0     0     0
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  DEGRADED     0     0    24  too many errors

errors: No known data errors
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, when we attempt to run &lt;code&gt;zpool reguid&lt;/code&gt;, the command fails due to the
zpool being in an unhealthy state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool reguid tank
cannot reguid &#39;tank&#39;: one or more devices is currently unavailable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but if we use &lt;code&gt;zpool clear&lt;/code&gt; to bring the pool back into a healthy state,
we can issue the &lt;code&gt;zpool reguid&lt;/code&gt; command without issues:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool clear tank
# zpool reguid tank
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: remember that &lt;code&gt;zpool scrub&lt;/code&gt; will have corrected the damage
previously done by &lt;code&gt;dd&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example-using-a-damaged-vdev-in-a-non-empty-zpool&#34;&gt;Example Using a Damaged VDEV in a Non-Empty ZPOOL&lt;/h3&gt;

&lt;p&gt;Exanding on the previous example, we&amp;rsquo;ll perform an almost identical
test, except we&amp;rsquo;ll now fill the zpool with a 256MB file instead of using
an empty zpool. So, again, we start by creating the zpool and our 256MB
file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank
# rm /tmp/tank/*
# mkfile -n 256m /tmp/tank/vdev1
# mkfile -n 256m /tmp/tank/vdev2
# mkfile -n 256m /tmp/tank/vdev3
# zpool create tank /tmp/tank/vdev{1,2,3}
# dd if=/dev/urandom of=/tank/file bs=1M count=256
256+0 records in
256+0 records out
268435456 bytes transferred in 4.641438 secs (57834542 bytes/sec)
# zpool list -v tank
NAME                SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank                720M   256M   464M         -    10%    35%  1.00x  ONLINE  -
  /tmp/tank/vdev1   240M  85.3M   155M         -    11%    35%
  /tmp/tank/vdev2   240M  85.3M   155M         -    10%    35%
  /tmp/tank/vdev3   240M  85.8M   154M         -    10%    35%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now damaging the VDEV, and running &lt;code&gt;zpool scrub&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dd if=/dev/urandom of=/tmp/tank/vdev3 bs=1M seek=1 count=254
254+0 records in
254+0 records out
266338304 bytes transferred in 4.225401 secs (63032676 bytes/sec)
# zpool scrub tank
# zpool status -v tank
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://illumos.org/msg/ZFS-8000-8A
  scan: scrub repaired 87.5K in 0h0m with 686 errors on Wed Feb 22 22:58:40 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               DEGRADED     0     0   686
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  DEGRADED     0     0 1.37K  too many errors

errors: Permanent errors have been detected in the following files:

        /tank/file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, there&amp;rsquo;s &amp;ldquo;Permanent errors&amp;rdquo; detected to the file that we
created, but the pool itself should still be intact and reparied by the
&lt;code&gt;zpool scrub&lt;/code&gt;. Thus, we&amp;rsquo;ll use &lt;code&gt;zpool clear&lt;/code&gt; to clear the errors like we
did in the previous example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool clear tank
# zpool status -v tank
  pool: tank
 state: ONLINE
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://illumos.org/msg/ZFS-8000-8A
  scan: scrub repaired 87.5K in 0h0m with 686 errors on Wed Feb 22 22:58:40 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               ONLINE       0     0     0
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  ONLINE       0     0     0

errors: Permanent errors have been detected in the following files:

        /tank/file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, I would expect that we could now use &lt;code&gt;zpool reguid&lt;/code&gt;
on this zpool; even though there&amp;rsquo;s &amp;ldquo;Permanent errors&amp;rdquo; in the user data
file, the pool is online an healthy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zdb -C tank | grep pool_guid
        pool_guid: 6847294342266961459
# zpool reguid tank
# zdb -C tank | grep pool_guid
        pool_guid: 13248255241205296188
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and this is exactly the behavior seen.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>