<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on www.prakashsurya.com</title>
    <link>https://www.prakashsurya.com/post/index.xml</link>
    <description>Recent content in Posts on www.prakashsurya.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Sep 2017 00:00:00 -0800</lastBuildDate>
    <atom:link href="https://www.prakashsurya.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Code Coverage for ZFS on Linux</title>
      <link>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</link>
      <pubDate>Mon, 18 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-18-code-coverage-for-zfs-on-linux/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working with Brian Behlendorf on getting code coverage
information for the ZFS on Linux. The goal was to get code coverage data
for pull requests, as well as branches; this way, we can get a sense of
how well tested any given PR is by the automated tests, prior to landing
it. There&amp;rsquo;s still some wrinkles that need to be ironed out, but we&amp;rsquo;ve
mostly achieved that goal by leveraging &lt;a href=&#34;https://codecov.io/&#34;&gt;codecov.io&lt;/a&gt;, along
with a small &lt;a href=&#34;https://github.com/zfsonlinux/zfs-buildbot/blob/master/scripts/bb-test-cleanup.sh&#34;&gt;bash script&lt;/a&gt;. Here&amp;rsquo;s an &lt;a href=&#34;https://codecov.io/gh/zfsonlinux/zfs/pull/6566/diff&#34;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While what we&amp;rsquo;ve currently implemented is better than nothing, there&amp;rsquo;s
some potential improvements that&amp;rsquo;d I&amp;rsquo;d like to investigate when I have
time (and this post serves as a way to help me remember what they are):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Currently in the automated tests that are run for the ZFS on Linux
 project, the kernel&amp;rsquo;s code coverage data isn&amp;rsquo;t reset prior to
 running any tests. This is because in my testing, a hang would
 occur whenever I would write to the kernel&amp;rsquo;s &amp;ldquo;reset&amp;rdquo; file. For
 example, I would run this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; # echo &#39;&#39; &amp;gt; /sys/kernel/debug/gcov/reset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and it would never return; it would have the following backtrace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat /proc/1624/stack
[&amp;lt;ffffffffb736f501&amp;gt;] __synchronize_srcu+0xe1/0x100
[&amp;lt;ffffffffb736f5eb&amp;gt;] synchronize_srcu+0xcb/0x1c0
[&amp;lt;ffffffffb7749390&amp;gt;] debugfs_remove+0xa0/0xe0
[&amp;lt;ffffffffb73e5f82&amp;gt;] release_node+0x62/0x140
[&amp;lt;ffffffffb73e613b&amp;gt;] reset_write+0xdb/0x120
[&amp;lt;ffffffffb774a3e4&amp;gt;] full_proxy_write+0x84/0xd0
[&amp;lt;ffffffffb758a90f&amp;gt;] __vfs_write+0x3f/0x230
[&amp;lt;ffffffffb758e38e&amp;gt;] vfs_write+0xfe/0x270
[&amp;lt;ffffffffb758e889&amp;gt;] SyS_write+0x79/0x130
[&amp;lt;ffffffffb7f8083b&amp;gt;] entry_SYSCALL_64_fastpath+0x1e/0xa9
[&amp;lt;ffffffffffffffff&amp;gt;] 0xffffffffffffffff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately, I never got around to doing any root cause analysis
of the issue. Instead, we simply aren&amp;rsquo;t resetting the kernel&amp;rsquo;s
coverage data, which is fine because we only upload a single unified
report to Codecov (which contains the data for all tests in the
automated test suite).&lt;/p&gt;

&lt;p&gt;Also, it&amp;rsquo;s worth noting that while writing this post, I attempted
that same command to clear the kernel&amp;rsquo;s coverage data, and I was
unable to reproduce the hang. I&amp;rsquo;m running a different kernel now, so
perhaps the previous behavior was a bug in the kernel that&amp;rsquo;s been
fixed in a later release? Without an RCA, anything is possible.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We&amp;rsquo;re not collecting any data for the userspace libraries, such as
 &lt;code&gt;libzpool&lt;/code&gt;; which unfortunately means we don&amp;rsquo;t get much coverage
 data from runs of &lt;code&gt;ztest&lt;/code&gt; and &lt;code&gt;zloop&lt;/code&gt;. The reason behind this is
 simply due to the additional implementation complexity required to
 support the libraries:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Some of the libraries are composed of the same source files as
 the kernel modules. Thus, we&amp;rsquo;d need to be careful when
 generating the gcov reports if we included these libaries, such
 that the kernel report for a given source file isn&amp;rsquo;t replaced
 by the userspace report (or vice versa). If we happened to be
 careless, it&amp;rsquo;d be easy to generate the kernel report for a
 source file, and then accidentally overwrite that report with
 the userspace data when generating the userspace report.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The &lt;code&gt;.gcda&lt;/code&gt; files for the libraries often are found in a
 &lt;code&gt;.libs&lt;/code&gt; sub-directory, rather than directly in the same
 directory as the source code (like non-libary sources). For
 example, the &lt;code&gt;.gcda&lt;/code&gt; files for &lt;code&gt;libzpool&lt;/code&gt; are here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ find lib/libzpool -name &#39;*.gcda&#39; | head -n 5
 lib/libzpool/.libs/zfs_rlock.gcda
 lib/libzpool/.libs/zpool_prop.gcda
 lib/libzpool/.libs/vdev_raidz.gcda
 lib/libzpool/.libs/dsl_pool.gcda
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is in contrast to where these files are stored for the
 commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ find cmd -name &#39;*.gcda&#39; | head -n 5
 cmd/zfs/zfs_iter.gcda
 cmd/zfs/zfs_main.gcda
 cmd/ztest/ztest.gcda
 cmd/mount_zfs/mount_zfs.gcda
 cmd/zpool/zpool_iter.gcda
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result, we have to use the &lt;code&gt;-o&lt;/code&gt; option when running &lt;code&gt;gcov&lt;/code&gt;
 for the libraries, but not for the commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libzpool
 $ gcov -o .libs arc.gcno
 File &#39;../../module/zfs/arc.c&#39;
 Lines executed:60.72% of 3055
 Creating &#39;arc.c.gcov&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The gcov data files reference their corresponding source files
 using relative paths to parts of the project&amp;rsquo;s repository. This
 happens when a library references one of the source files from
 the &lt;code&gt;module&lt;/code&gt; directory (i.e. when file is built both for kernel
 mode and user mode), and is a result of how the build system
 works for these libraries.&lt;/p&gt;

&lt;p&gt;Thus, if &lt;code&gt;gcov&lt;/code&gt; is run from the &amp;ldquo;root&amp;rdquo; of the project
 directory, it will be unable to find the source files for that
 library. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ gcov -b lib/libzpool/arc.gcno
 ...
 Cannot open source file ../../module/zfs/arc.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be relatively easily worked around by first &lt;code&gt;cd&lt;/code&gt;-ing
 into the directory that contains the &lt;code&gt;.gcno&lt;/code&gt; file, and then
 running &lt;code&gt;gcov&lt;/code&gt;, but that idea falls apart for the &lt;code&gt;libicp&lt;/code&gt;
 library.&lt;/p&gt;

&lt;p&gt;For this libary, one must execute &lt;code&gt;gcov&lt;/code&gt; from the &lt;code&gt;lib/libicp&lt;/code&gt;
 directory, but the &lt;code&gt;.gcno&lt;/code&gt; files are stored in subdirectories
 from there. For example, this fails:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libicp/algs/sha2
 $ gcov -o .libs sha2.gcno
 File &#39;../../module/icp/algs/sha2/sha2.c&#39;
 Lines executed:67.41% of 135
 Creating &#39;sha2.c.gcov&#39;
 Cannot open source file ../../module/icp/algs/sha2/sha2.c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this works:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $ cd ~/zfs/lib/libicp
 $ gcov -o algs/sha2/.libs algs/sha2/sha2.gcno
 File &#39;../../module/icp/algs/sha2/sha2.c&#39;
 Lines executed:67.41% of 135
 Creating &#39;sha2.c.gcov&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The last complication to consider is that, since we can have
 two different modes of execution for any given source file
 (since we compile certain files in both kernel and user mode),
 it&amp;rsquo;d be ideal to have seperate coverage reports for kernel and
 user mode execution. Codecov appears to support this via its
 concept of &amp;ldquo;flags&amp;rdquo; (i.e. using the &lt;code&gt;-F&lt;/code&gt; option of their &lt;a href=&#34;https://docs.codecov.io/v4.3.6/docs/about-the-codecov-bash-uploader&#34;&gt;Bash
 uploader&lt;/a&gt;), but that functionality still needs to be
 evaluated to make sure it&amp;rsquo;ll work for our needs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All in all, coming up with solutions for any (or all) of these
complications is possible, it just requires a little more work to be
done. For the first attempt at collecting coverage data, I opted to keep
the implementation simple yet functional and useful. I hope to revisit
these issues, and extend the coverage data to include the libraries
soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using &#34;gcov&#34; with ZFS on Linux Kernel Modules</title>
      <link>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</link>
      <pubDate>Mon, 11 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-11-using-gcov-with-zfs-on-linux-kernel-modules/</guid>
      <description>

&lt;h2 id=&#34;building-a-gcov-enabled-linux-kernel&#34;&gt;Building a &amp;ldquo;gcov&amp;rdquo; Enabled Linux Kernel&lt;/h2&gt;

&lt;p&gt;In order to extract &amp;ldquo;gcov&amp;rdquo; data from the Linux kernel, and/or Linux
kernel modules, a &amp;ldquo;gcov&amp;rdquo; enabled Linux kernel is needed. Since my
current development environment is based on Ubuntu 17.04, and the fact
that Ubuntu doesn&amp;rsquo;t provide a pre-built kernel with &amp;ldquo;gcov&amp;rdquo; enabled, I
had to build the kernel from source. This was actually pretty simple,
and most of that process is already documented &lt;a href=&#34;https://kernelnewbies.org/KernelBuild&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;install-the-kernel-build-dependencies&#34;&gt;Install the Kernel Build Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install libncurses5-dev \
      gcc make git exuberant-ctags bc libssl-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;obtaining-the-mainline-kernel-sources&#34;&gt;Obtaining the Mainline Kernel Sources&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/torvalds/linux.git ~/linux
$ cd ~/linux
$ git checkout v4.13
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;default-kernel-build-configuration&#34;&gt;Default Kernel Build Configuration&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ cp /boot/config-$(uname -r) .config
$ yes &#39;&#39; | make oldconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adding-gcov-specific-configurations&#34;&gt;Adding &amp;ldquo;gcov&amp;rdquo; Specific Configurations&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ echo &#39;CONFIG_DEBUG_FS=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_KERNEL=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_FORMAT_AUTODETECT=y&#39; &amp;gt;&amp;gt; .config
$ echo &#39;CONFIG_GCOV_PROFILE_ALL=y&#39; &amp;gt;&amp;gt; .config
$ yes &#39;&#39; | make oldconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;building-and-installing-the-custom-kernel&#34;&gt;Building and Installing the Custom Kernel&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ make -j$(nproc)
$ sudo make modules_install install
$ sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;verifying-gcov-support-is-enabled&#34;&gt;Verifying &amp;ldquo;gcov&amp;rdquo; Support is Enabled&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mount -t debugfs none /sys/kernel/debug
$ sudo ls -1d /sys/kernel/debug/gcov
/sys/kernel/debug/gcov
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;building-zfs-on-linux-modules&#34;&gt;Building ZFS on Linux Modules&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s nothing special that we need to do, in order to enable &amp;ldquo;gcov&amp;rdquo;
support for the ZFS on Linux modules. Since the kernel was built with
&amp;ldquo;gcov&amp;rdquo; enabled, the modules will be automatically built with it enabled
as well.&lt;/p&gt;

&lt;h3 id=&#34;building-the-spl-repository&#34;&gt;Building the SPL Repository&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/zfsonlinux/spl.git ~/spl
$ cd ~/spl
$ ./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;building-the-zfs-repository&#34;&gt;Building the ZFS Repository&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/zfsonlinux/zfs.git ~/zfs
$ cd ~/zfs
$ ./autogen.sh &amp;amp;&amp;amp; ./configure &amp;amp;&amp;amp; make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;running-a-test-and-collecting-data-files&#34;&gt;Running a Test and Collecting Data Files&lt;/h2&gt;

&lt;p&gt;At this point, we&amp;rsquo;re running on a &amp;ldquo;gcov&amp;rdquo; enabled Linux kernel, and the
ZFS on Linux kernel modules should be installed such that we can load
them easily with &lt;code&gt;modprobe&lt;/code&gt;. Thus, we can finally run a trivial test,
loading and unloading the &amp;ldquo;zfs&amp;rdquo; module, and then use &lt;code&gt;gcov&lt;/code&gt; to
extract the code coverage information from that test.&lt;/p&gt;

&lt;h3 id=&#34;loading-and-unloading-the-zfs-module&#34;&gt;Loading and Unloading the &amp;ldquo;zfs&amp;rdquo; module&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo modprobe zfs
$ sudo modprobe -r zfs
$ dmesg | grep ZFS
[ 1214.307936] ZFS: Loaded module v0.7.0-1, ZFS pool version 5000, ZFS filesystem version 5
[ 1216.475316] ZFS: Unloaded module v0.7.0-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;collecting-gcov-data-files&#34;&gt;Collecting &amp;ldquo;gcov&amp;rdquo; Data Files&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo chmod a+rx /sys/kernel/debug
$ TEMPDIR=$(mktemp -d)
$ cd /sys/kernel/debug/gcov/$HOME
$ find . -type d -exec mkdir -p $TEMPDIR/{} \;
$ find . -name &#39;*.gcda&#39; -exec sh -c &#39;sudo cat $0 &amp;gt; &#39;$TEMPDIR&#39;/$0&#39; {} \;
$ find . -name &#39;*.gcno&#39; -exec sh -c &#39;cp -d $0 &#39;$TEMPDIR&#39;/$0&#39; {} \;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;use-data-files-to-extract-code-coverage-information&#34;&gt;Use Data Files to Extract Code Coverage Information&lt;/h2&gt;

&lt;p&gt;Now that we have the &amp;ldquo;gcov&amp;rdquo; data files stored in &lt;code&gt;$TEMPDIR&lt;/code&gt;, we can
finally use these to extract the code converage data for any particular
source code file that we&amp;rsquo;re interested in. For this example, we&amp;rsquo;re going
to look at the &lt;code&gt;zfs_ioctl.c&lt;/code&gt; file, and more specifically, at the &lt;code&gt;_fini&lt;/code&gt;
function which is run a single time when the &amp;ldquo;zfs&amp;rdquo; module is unloaded:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /tmp
$ gcov -o $TEMPDIR/zfs/module/zfs zfs_ioctl.c
$ cat zfs_ioctl.c.gcov
...
        -: 6751:static void __exit
        1: 6752:_fini(void)
        -: 6753:{
        1: 6754:        zfs_detach();
        1: 6755:        zfs_fini();
        1: 6756:        spa_fini();
        1: 6757:        zvol_fini();
        -: 6758:
        1: 6759:        tsd_destroy(&amp;amp;zfs_fsyncer_key);
        1: 6760:        tsd_destroy(&amp;amp;rrw_tsd_key);
        1: 6761:        tsd_destroy(&amp;amp;zfs_allow_log_key);
        -: 6762:
        1: 6763:        printk(KERN_NOTICE &amp;quot;ZFS: Unloaded module v%s-%s%s\n&amp;quot;,
        -: 6764:            ZFS_META_VERSION, ZFS_META_RELEASE, ZFS_DEBUG_STR);
        1: 6765:}
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see that each line of the &lt;code&gt;_fini&lt;/code&gt; function was executed
exactly a single time, which is expected based on our trival load/unload
test case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Performance Testing Results for OpenZFS #447</title>
      <link>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-08-performance-testing-results-for-openzfs-447/</guid>
      <description>&lt;p&gt;The following are links to the Jupyter notebooks that describe the
performance testing that I did for &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;OpenZFS #447&lt;/a&gt;, and the results
of that testing:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-max-rate-submit-hdd.ipynb
&#34;&gt;Max Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-max-rate-submit-ssd.ipynb
&#34;&gt;Max Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-fixed-rate-submit-hdd.ipynb
&#34;&gt;Fixed Rate Submit on HDDs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-08-performance-testing-results-for-openzfs-447/openzfs-447-perf-fixed-rate-submit-ssd.ipynb
&#34;&gt;Fixed Rate Submit on SSDs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, a compressed tarball with all the raw data used to
generate those Jupyter notebooks can be found &lt;a href=&#34;openzfs-447-perf.tar.xz&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Python and Jupyter for Performance Testing and Analysis</title>
      <link>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I recently worked on some changes to the OpenZFS ZIL (see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt;),
and in the context of working on that project, I discovered some new
tools that helped me run my performance tests and analyze their
results. What follows is some notes on the tools that I used, and how I
used them.&lt;/p&gt;

&lt;h2 id=&#34;quick-overview&#34;&gt;Quick Overview&lt;/h2&gt;

&lt;p&gt;Before I dive into the details of how I used these tools, I wanted to
quickly go over what the tools were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/axboe/fio&#34;&gt;fio&lt;/a&gt; was used to generate the workload, and provide statistics
about the performance from the application&amp;rsquo;s perspective.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://jupyter.org/&#34;&gt;Jupyter&lt;/a&gt; was used for analysis of the test results; e.g.
performing data manipulations, generating visualizations, and
presenting the data/visualizations along with text explanations in
a unified document.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; was used for everything; e.g. running the tests,
capturing the results, analysis of the data, and generating
visualizations were all driven by python scripts. In addition to the
core language, the following modules were used:
&lt;a href=&#34;https://amoffat.github.io/sh/&#34;&gt;sh&lt;/a&gt;,
&lt;a href=&#34;http://pandas.pydata.org/&#34;&gt;pandas&lt;/a&gt;,
&lt;a href=&#34;http://www.numpy.org/&#34;&gt;numpy&lt;/a&gt;,
&lt;a href=&#34;https://seaborn.pydata.org/&#34;&gt;seaborn&lt;/a&gt;,
and &lt;a href=&#34;https://matplotlib.org/&#34;&gt;matplotlib&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pyenv/pyenv&#34;&gt;pyenv&lt;/a&gt; and &lt;a href=&#34;https://github.com/pyenv/pyenv-virtualenv&#34;&gt;pyenv-virtualenv&lt;/a&gt; were used to
provide an isolated Python environment.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://nbviewer.jupyter.org/&#34;&gt;nbviewer&lt;/a&gt; were used to share the
Jupyter notebooks, which contained the results of the tests.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;install-prerequisites-on-ubuntu-16-04&#34;&gt;Install Prerequisites on Ubuntu 16.04&lt;/h2&gt;

&lt;p&gt;Before we can use the above referenced tools, we first must download,
build, and/or install them such that they are available for us to use.
Below is some instructions for how to do that on an Ubuntu 16.04 VM that
I used; if using another operating system, the specific commands may not
work, but hopefully what&amp;rsquo;s included here can be easily adapted.&lt;/p&gt;

&lt;h3 id=&#34;install-build-dependencies&#34;&gt;Install Build Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt install -y git build-essential zlib1g-dev \
    libbz2-dev libssl-dev libreadline-dev libsqlite3-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-pyenv-and-pyenv-virtualenv&#34;&gt;Install &amp;ldquo;pyenv&amp;rdquo; and &amp;ldquo;pyenv virtualenv&amp;rdquo;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/pyenv/pyenv ~/.pyenv
$ git clone https://github.com/pyenv/pyenv-virtualenv.git \
    ~/.pyenv/plugins/pyenv-virtualenv

$ echo &#39;export PYENV_ROOT=&amp;quot;$HOME/.pyenv&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;export PATH=&amp;quot;$PYENV_ROOT/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;eval &amp;quot;$(pyenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
$ echo &#39;eval &amp;quot;$(pyenv virtualenv-init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-python-3-and-create-virtual-environment&#34;&gt;Install Python 3 and Create Virtual Environment&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ pyenv install 3.6.2
$ pyenv virtualenv
$ pyenv virtualenv 3.6.2 jupyter-example
$ mkdir ~/jupyter-example
$ cd ~/jupyter-example
$ echo jupyter-example &amp;gt; .python-version
$ pip install jupyter pandas numpy seaborn matplotlib sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-and-install-fio&#34;&gt;Build and Install &amp;ldquo;fio&amp;rdquo;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/axboe/fio ~/fio
$ cd ~/fio
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-additional-utilities&#34;&gt;Install Additional Utilities&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt install -y zfsutils-linux jq sysstat
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generate-results-data-required-for-analysis&#34;&gt;Generate Results Data Required for Analysis&lt;/h2&gt;

&lt;p&gt;Now that all of the necessary tools have been installed, we can write a
Python script that uses these tools to run the tests and collect any
data that will be needed for proper analysis.&lt;/p&gt;

&lt;h3 id=&#34;generate-fio-test-configuration&#34;&gt;Generate &amp;ldquo;fio&amp;rdquo; Test Configuration&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ll be using &amp;ldquo;fio&amp;rdquo; to generate the workload. Let&amp;rsquo;s first create the
configuration file that will be passed to fio, which tells it how to
behave:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; ~/jupyter-example/workload.fio &amp;lt;&amp;lt;EOF
[global]
group_reporting
clocksource=cpu
ioengine=psync
fallocate=none
rw=write
blocksize=8k
time_based
iodepth=1
thread=0
direct=0
sync=1

[workload]
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;generate-python-script-to-run-fio-tests&#34;&gt;Generate Python Script to Run &amp;ldquo;fio&amp;rdquo; Tests&lt;/h3&gt;

&lt;p&gt;Now we&amp;rsquo;ll create the python script that will be used to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;create a ZFS pool and dataset from a known set of disks&lt;/li&gt;
&lt;li&gt;run &amp;ldquo;fio&amp;rdquo;, such that it uses the ZFS dataset previously created&lt;/li&gt;
&lt;li&gt;run &amp;ldquo;iostat&amp;rdquo;, collecting disk metrics concurrently while &amp;ldquo;fio&amp;rdquo; runs&lt;/li&gt;
&lt;li&gt;copy the data generated from &amp;ldquo;fio&amp;rdquo; and &amp;ldquo;iostat&amp;rdquo; to a &amp;ldquo;results&amp;rdquo;
 directory so they can be analyzed at a later time&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here&amp;rsquo;s what the script may look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; ~/jupyter-example/workload.py &amp;lt;&amp;lt;EOF
#!/usr/bin/env python3

import sh
import tempfile

def fio(directory, numjobs, disks, runtime=60):
  with tempfile.TemporaryDirectory(dir=&#39;/var/tmp&#39;) as tempdir:
    procs = []

    for d in disks:
      iostat = sh.iostat(&#39;-dxy&#39;, d, &#39;1&#39;,
                         _piped=True, _bg=True, _bg_exc=False)
      procs.append(iostat)

      grep = sh.grep(iostat, d, _bg=True, _bg_exc=False,
                     _out=&#39;{:s}/iostat-{:s}.txt&#39;.format(tempdir, d))
      procs.append(grep)

    sh.sudo.fio(&#39;--directory={:s}&#39;.format(directory),
                &#39;--size={:.0f}M&#39;.format(2**20 / numjobs),
                &#39;--numjobs={:d}&#39;.format(numjobs),
                &#39;--runtime={:d}&#39;.format(runtime),
                &#39;--output={:s}/{:s}&#39;.format(tempdir, &#39;fio.json&#39;),
                &#39;--output-format=json+&#39;,
                &#39;./workload.fio&#39;)

    for p in procs:
      try:
        sh.sudo.kill(p.pid)
        p.wait
      except (sh.ErrorReturnCode_1, sh.SignalException_SIGTERM):
        pass

    directory = &#39;results/{:d}-disks/{:d}-jobs&#39;.format(len(disks), numjobs)
    sh.mkdir(&#39;-p&#39;, directory)
    sh.rm(&#39;-rf&#39;, directory)
    sh.cp(&#39;-r&#39;, tempdir, directory)
    sh.chmod(&#39;755&#39;, directory)

def test(disks):
  for numjobs in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:
    try:
      sh.sudo.zpool(&#39;create&#39;, &#39;-f&#39;, &#39;tank&#39;, disks)
      sh.sudo.zfs(&#39;create&#39;, &#39;-o&#39;, &#39;recsize=8k&#39;, &#39;tank/dozer&#39;)
      fio(&#39;/tank/dozer&#39;, numjobs, disks)
    finally:
      sh.sudo.zpool(&#39;destroy&#39;, &#39;tank&#39;)

def main():
  sh.rm(&#39;-rf&#39;, &#39;results&#39;)
  sh.mkdir(&#39;results&#39;)

  test([&#39;sdb&#39;])
  test([&#39;sdb&#39;, &#39;sdc&#39;])
  test([&#39;sdb&#39;, &#39;sdc&#39;, &#39;sdd&#39;])

if __name__ == &#39;__main__&#39;:
  main()
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;run-python-script-to-generate-results&#34;&gt;Run Python Script to Generate Results&lt;/h3&gt;

&lt;p&gt;The python script created in the previous section can then be run, which
will generate a &amp;ldquo;results&amp;rdquo; directory with all of the raw data from &amp;ldquo;fio&amp;rdquo;
and &amp;ldquo;iostat&amp;rdquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ time python3 workload.py

real    34m55.771s
user    2m19.040s
sys     12m20.656s

(jupyter-example) $ ls results/*
results/1-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

results/2-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

results/3-disks:
1024-jobs  128-jobs  16-jobs  1-jobs  256-jobs  2-jobs  32-jobs  4-jobs  512-jobs  64-jobs  8-jobs

(jupyter-example) $ ls results/*-disks/1024-jobs
results/1-disks/1024-jobs:
fio.json  iostat-sdb.txt

results/2-disks/1024-jobs:
fio.json  iostat-sdb.txt  iostat-sdc.txt

results/3-disks/1024-jobs:
fio.json  iostat-sdb.txt  iostat-sdc.txt  iostat-sdd.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see, each unique test configuration (i.e. number of disks in
the zpool, and number of &amp;ldquo;fio&amp;rdquo; threads) has its own directory containing
the results for that specific test configuration. The &amp;ldquo;fio.json&amp;rdquo;
contains the JSON formatted output from &amp;ldquo;fio&amp;rdquo;, and the &amp;ldquo;iostat-*.txt&amp;rdquo;
files contains the output from &amp;ldquo;iostat&amp;rdquo; for each specific disk.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a quick inspection of one of the &amp;ldquo;fio&amp;rdquo; files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ head results/2-disks/32-jobs/fio.json
{
  &amp;quot;fio version&amp;quot; : &amp;quot;fio-3.0-48-g83a9&amp;quot;,
  &amp;quot;timestamp&amp;quot; : 1504768846,
  &amp;quot;timestamp_ms&amp;quot; : 1504768846239,
  &amp;quot;time&amp;quot; : &amp;quot;Thu Sep  7 07:20:46 2017&amp;quot;,
  &amp;quot;global options&amp;quot; : {
    &amp;quot;directory&amp;quot; : &amp;quot;/tank/dozer&amp;quot;,
    &amp;quot;size&amp;quot; : &amp;quot;32768M&amp;quot;,
    &amp;quot;runtime&amp;quot; : &amp;quot;60&amp;quot;,
    &amp;quot;clocksource&amp;quot; : &amp;quot;cpu&amp;quot;,

(jupyter-example) $ jq -Mr .jobs[0].write.lat_ns results/2-disks/32-jobs/fio.json
{
  &amp;quot;min&amp;quot;: 712505,
  &amp;quot;max&amp;quot;: 312646560,
  &amp;quot;mean&amp;quot;: 5412690.837107,
  &amp;quot;stddev&amp;quot;: 7953187.998254
}

(jupyter-example) $ jq -Mr .jobs[0].write.iops results/2-disks/32-jobs/fio.json
5906.296339
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this, we can see that on average, each write made by fio took
roughly 5ms to complete. Additionally, fio averaged about 5.9K IOPs
during that specific test&amp;rsquo;s runtime.&lt;/p&gt;

&lt;p&gt;Similarly, we can briefly look at the iostat data collected for that
same test configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ head results/2-disks/32-jobs/iostat-sdb.txt
sdb               0.00     0.00    0.00  304.00     0.00 18116.50   119.19     0.54    1.78    0.00    1.78   1.45  44.00
sdb               0.00     0.00    0.00  667.00     0.00 60725.50   182.09     1.23    1.84    0.00    1.84   1.27  84.40
sdb               0.00     0.00    0.00  500.00     0.00 49707.50   198.83     0.92    1.86    0.00    1.86   1.58  78.80
sdb               0.00     0.00    0.00  502.00     0.00 45371.00   180.76     0.91    1.80    0.00    1.80   1.50  75.20
sdb               0.00     0.00    0.00  523.00     0.00 47920.50   183.25     1.07    2.06    0.00    2.06   1.48  77.20
sdb               0.00     0.00    0.00  598.00     0.00 58390.50   195.29     1.38    2.29    0.00    2.29   1.55  92.80
sdb               0.00     0.00    0.00  393.00     0.00 37456.00   190.62     0.98    2.49    0.00    2.49   2.06  80.80
sdb               0.00     0.00    0.00  527.00     0.00 48050.00   182.35     1.08    2.05    0.00    2.05   1.53  80.40
sdb               0.00     0.00    0.00  590.00     0.00 55418.00   187.86     1.24    2.10    0.00    2.10   1.48  87.60
sdb               0.00     0.00    0.00  663.00     0.00 64226.50   193.75     1.18    1.78    0.00    1.78   1.32  87.20

(jupyter-example) $ head results/2-disks/32-jobs/iostat-sdc.txt
sdc               0.00     0.00    0.00  295.00     0.00 25714.00   174.33     0.50    1.69    0.00    1.69   1.40  41.20
sdc               0.00     0.00    0.00  640.00     0.00 59921.00   187.25     1.22    1.90    0.00    1.90   1.33  85.20
sdc               0.00     0.00    0.00  509.00     0.00 53581.50   210.54     1.00    1.96    0.00    1.96   1.70  86.40
sdc               0.00     0.00    0.00  512.00     0.00 41799.00   163.28     0.95    1.85    0.00    1.85   1.48  75.60
sdc               0.00     0.00    0.00  538.00     0.00 48975.50   182.07     1.09    2.03    0.00    2.03   1.50  80.80
sdc               0.00     0.00    0.00  601.00     0.00 50568.00   168.28     1.36    2.25    0.00    2.25   1.50  90.40
sdc               0.00     0.00    0.00  395.00     0.00 38467.00   194.77     1.03    2.61    0.00    2.61   2.14  84.40
sdc               0.00     0.00    0.00  540.00     0.00 49031.00   181.60     1.03    1.91    0.00    1.91   1.45  78.40
sdc               0.00     0.00    0.00  580.00     0.00 53762.50   185.39     1.28    2.21    0.00    2.21   1.49  86.40
sdc               0.00     0.00    0.00  648.00     0.00 57146.50   176.38     1.19    1.83    0.00    1.83   1.31  85.20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These files contain the output of &amp;ldquo;iostat -dxy&amp;rdquo; for each device in the
ZFS pool used for the specific test configuration. In this case, the
pool consisted of 2 disks, &amp;ldquo;sdb&amp;rdquo; and &amp;ldquo;sdc&amp;rdquo;. Each line in the file
represents a 1 second interval.&lt;/p&gt;

&lt;p&gt;For completeness, since the iostat column headers are not included in
these files, here&amp;rsquo;s what they are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, we can see that usually an IO request sent to these disks was
serviced in less than 2ms, judging by the &lt;code&gt;svctm&lt;/code&gt; column.&lt;/p&gt;

&lt;h2 id=&#34;analyzing-results-data-with-pandas-and-jupyter&#34;&gt;Analyzing Results Data with Pandas and Jupyter&lt;/h2&gt;

&lt;p&gt;Now that the data had been gathered from &amp;ldquo;fio&amp;rdquo; and &amp;ldquo;iostat&amp;rdquo; for all of
the test configurations, it was time to parse and analyze the data.&lt;/p&gt;

&lt;h3 id=&#34;parsing-fio-results-with-pandas&#34;&gt;Parsing &amp;ldquo;fio&amp;rdquo; Results with Pandas&lt;/h3&gt;

&lt;p&gt;To parse the &amp;ldquo;fio&amp;rdquo; results files, I used a combination of the previously
used &amp;ldquo;sh&amp;rdquo; Python module, and the &amp;ldquo;jq&amp;rdquo; command. Here&amp;rsquo;s some code that
would parse the the IOPs reported by fio for each test configuration,
and print the results as a table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ python3
Python 3.6.2 (default, Sep  6 2017, 21:23:54)
[GCC 5.4.0 20160609] on linux
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import pandas
&amp;gt;&amp;gt;&amp;gt; import sh
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; numjobs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
&amp;gt;&amp;gt;&amp;gt; numdisks = [1, 2, 3]
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; jq = sh.jq.bake(&#39;-M&#39;, &#39;-r&#39;)
&amp;gt;&amp;gt;&amp;gt; iops = pandas.DataFrame()
&amp;gt;&amp;gt;&amp;gt; for i in numdisks:
...   tmp = []
...   for j in numjobs:
...     data = jq(&#39;.jobs[0].write.iops&#39;,
...               &#39;results/{:d}-disks/{:d}-jobs/fio.json&#39;.format(i, j))
...     tmp.append(float(data.strip()))
...   iops[&#39;{:d} disks&#39;.format(i)] = pandas.Series(tmp, numjobs)
...
&amp;gt;&amp;gt;&amp;gt; print(iops)
          1 disks       2 disks       3 disks
1      693.355111    539.923004    399.353355
2      946.218459    817.014473    465.543389
4     1487.733742   1367.766810    817.556196
8     2597.526832   2714.935671    902.661356
16    3958.774225   4132.298054   3854.193054
32    6192.515329   5906.296339   5730.420583
64    7470.722522   8216.463923   8146.322770
128   8649.000333   9370.233395   9525.552187
256   8601.175804  10942.662150  10868.330780
512   7327.968610  11595.399661  11321.241803
1024  7792.668597  11137.351190  11391.167610
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, here&amp;rsquo;s code that would report the average write latency (in
nanoseconds) reported by fio for each test configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ python3
Python 3.6.2 (default, Sep  6 2017, 21:23:54)
[GCC 5.4.0 20160609] on linux
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import pandas
&amp;gt;&amp;gt;&amp;gt; import sh
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; numjobs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
&amp;gt;&amp;gt;&amp;gt; numdisks = [1, 2, 3]
&amp;gt;&amp;gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; jq = sh.jq.bake(&#39;-M&#39;, &#39;-r&#39;)
&amp;gt;&amp;gt;&amp;gt; lat = pandas.DataFrame()
&amp;gt;&amp;gt;&amp;gt; for i in numdisks:
...   tmp = []
...   for j in numjobs:
...     data = jq(&#39;.jobs[0].write.lat_ns.mean&#39;,
...               &#39;results/{:d}-disks/{:d}-jobs/fio.json&#39;.format(i, j))
...     tmp.append(float(data.strip()))
...   lat[&#39;{:d} disks&#39;.format(i)] = pandas.Series(tmp, numjobs)
...
&amp;gt;&amp;gt;&amp;gt; print(lat)
           1 disks       2 disks       3 disks
1     1.438905e+06  1.848592e+06  2.500436e+06
2     2.110580e+06  2.444135e+06  4.292531e+06
4     2.685592e+06  2.919950e+06  4.889253e+06
8     3.076803e+06  2.943452e+06  8.858595e+06
16    4.038235e+06  3.868267e+06  4.148222e+06
32    5.163324e+06  5.412691e+06  5.580323e+06
64    8.561707e+06  7.783084e+06  7.850346e+06
128   1.478921e+07  1.364914e+07  1.342665e+07
256   2.973660e+07  2.333695e+07  2.352923e+07
512   6.972930e+07  4.406545e+07  4.511673e+07
1024  1.305812e+08  9.137294e+07  8.931305e+07
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;visualizing-fio-results-with-jupyter&#34;&gt;Visualizing &amp;ldquo;fio&amp;rdquo; Results with Jupyter&lt;/h3&gt;

&lt;p&gt;While the text-based tables shown in the previous section are better
than nothing, Jupyter can be used to execute this parsing code, and
visualize the data using Python&amp;rsquo;s &amp;ldquo;matplotlib&amp;rdquo; graphing module.&lt;/p&gt;

&lt;p&gt;The Jupyter notebook software is easy to start up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(jupyter-example) $ jupyter notebook --ip=0.0.0.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then one can navigate to the server running the notebook using a web
browser; e.g. I would enter &lt;code&gt;http://ps-jupyter.dcenter.delphix.com:8888&lt;/code&gt;
into my browser.&lt;/p&gt;

&lt;p&gt;If the &lt;code&gt;jupyter&lt;/code&gt; command is run from a local shell (e.g. on one&amp;rsquo;s
workstation), the &lt;code&gt;--ip&lt;/code&gt; option can be ommitted, and the command will
automatically attempt to open a new browser window with the notebook&amp;rsquo;s
URL already populated.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an &lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/visualizing-fio-results-with-jupyter.ipynb
&#34;&gt;example&lt;/a&gt; Jupyter notebook, migrating the parsing
code from the prior section into the notebook, and adding some more logic
to generate graphs rather than text-based tables.&lt;/p&gt;

&lt;h3 id=&#34;visualizing-iostat-results-with-jupyter&#34;&gt;Visualizing &amp;ldquo;iostat&amp;rdquo; Results with Jupyter&lt;/h3&gt;

&lt;p&gt;Similarly, the data from &amp;ldquo;iostat&amp;rdquo; can also be parsed and visualized just
like the &amp;ldquo;fio&amp;rdquo; data. Rather than repeat the explanations from the prior
sections, I&amp;rsquo;ll simply link directly to the &lt;a href=&#34;https://nbviewer.jupyter.org/github/prakashsurya/prakashsurya.github.io/blob/src/static/post/2017-09-07-using-python-and-jupyter-for-performance-testing-and-analysis/visualizing-iostat-results-with-jupyter.ipynb
&#34;&gt;example&lt;/a&gt;
Jupyter notebook; which contains the code for both parsing the &amp;ldquo;iostat&amp;rdquo;
data files, as well as generating graphs from that data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building and Using &#34;crash&#34; on Ubuntu 16.04</title>
      <link>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</link>
      <pubDate>Tue, 05 Sep 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-09-05-building-and-using-crash-on-ubuntu-16-04/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve been working on the ZFS on Linux project recently, and had a need
to use &lt;code&gt;crash&lt;/code&gt; on the Ubuntu 16.04 based VM I was using. The following
is some notes regarding the steps I had to take, in order to build,
install, and ultimately run the utility against the &amp;ldquo;live&amp;rdquo; system.&lt;/p&gt;

&lt;h2 id=&#34;build-and-install-crash&#34;&gt;Build and Install &amp;ldquo;crash&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;First, I had to install the build dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install -y \
    git build-essential libncurses5-dev zlib1g-dev bison
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I could checkout the source code, build, and install:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/crash-utility/crash
$ cd crash
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;install-kernel-debug-symbols&#34;&gt;Install Kernel Debug Symbols&lt;/h2&gt;

&lt;p&gt;In order to run the &lt;code&gt;crash&lt;/code&gt; utility against the live system, we need
access to the uncompressed kernel image. We can obtain this by
installing the corresponding &lt;code&gt;-dbgsym&lt;/code&gt; package for the running kernel.&lt;/p&gt;

&lt;p&gt;First, we have to enable the APT repositories that contain these
packages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo tee /etc/apt/sources.list.d/ddebs.list &amp;lt;&amp;lt;EOF
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)          main restricted universe multiverse
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)-updates  main restricted universe multiverse
deb http://ddebs.ubuntu.com/ $(lsb_release -c -s)-proposed main restricted universe multiverse
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then add the correct key for these repositories:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-key adv \
    --keyserver keyserver.ubuntu.com \
    --recv-keys 428D7C01 C8CAB6595FDFF622
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now we can refresh the APT cache and install the kernel&amp;rsquo;s &lt;code&gt;-dbgsym&lt;/code&gt;
package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get update -y
$ sudo apt-get install -y linux-image-$(uname -r)-dbgsym
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;run-crash-against-live-system&#34;&gt;Run &amp;ldquo;crash&amp;rdquo; Against Live System&lt;/h2&gt;

&lt;p&gt;Finally, with all of the above done, I could run the &lt;code&gt;crash&lt;/code&gt; utility
against the running system like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo crash /usr/lib/debug/boot/vmlinux-$(uname -r)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a quick example for dumping the kernel backtrace for a specific
process (the process I needed to debug):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crash&amp;gt; bt 28365
PID: 28365  TASK: ffff887b7b978e00  CPU: 5   COMMAND: &amp;quot;fio&amp;quot;
 #0 [ffff887cd2167870] __schedule at ffffffff8183e9ee
 #1 [ffff887cd21678c0] schedule at ffffffff8183f0d5
 #2 [ffff887cd21678d8] spl_panic at ffffffffc02d70ca [spl]
 #3 [ffff887cd2167a60] zil_commit at ffffffffc1271daa [zfs]
 #4 [ffff887cd2167b50] zfs_write at ffffffffc1260d6d [zfs]
 #5 [ffff887cd2167d48] zpl_write_common_iovec at ffffffffc128b391 [zfs]
 #6 [ffff887cd2167dc0] zpl_iter_write at ffffffffc128b4e1 [zfs]
 #7 [ffff887cd2167e30] new_sync_write at ffffffff8120ec0b
 #8 [ffff887cd2167eb8] __vfs_write at ffffffff8120ec76
 #9 [ffff887cd2167ec8] vfs_write at ffffffff8120f5f9
#10 [ffff887cd2167f08] sys_pwrite64 at ffffffff81210465
#11 [ffff887cd2167f50] entry_SYSCALL_64_fastpath at ffffffff818431f2
    RIP: 00007f08274fbda3  RSP: 00007ffc1eae8a70  RFLAGS: 00000293
    RAX: ffffffffffffffda  RBX: 00007f080f6d8240  RCX: 00007f08274fbda3
    RDX: 0000000000002000  RSI: 0000000000f9b660  RDI: 0000000000000004
    RBP: 00007f080f6d8240   R8: 0000000000000000   R9: 0000000000000001
    R10: 0000000000374000  R11: 0000000000000293  R12: 000000000009bea3
    R13: 0000000000000001  R14: 0000000000002000  R15: 0000000000372000
    ORIG_RAX: 0000000000000012  CS: 0033  SS: 002b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And another example showing information about the running system:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;crash&amp;gt; sys
      KERNEL: /usr/lib/debug/boot/vmlinux-4.4.0-93-generic
    DUMPFILE: /proc/kcore
        CPUS: 32
        DATE: Tue Sep  5 22:55:59 2017
      UPTIME: 06:58:42
LOAD AVERAGE: 512.32, 512.25, 512.26
       TASKS: 1035
    NODENAME: ubuntu-16-04
     RELEASE: 4.4.0-93-generic
     VERSION: #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017
     MACHINE: x86_64  (2199 Mhz)
      MEMORY: 512 GB
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Using BCC&#39;s &#34;trace&#34; Instead of &#34;printk&#34;</title>
      <link>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-28-using-bccs-trace-instead-of-printk/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Recently I&amp;rsquo;ve been working on porting some changes that I made to the
OpenZFS ZIL over to the ZFS on Linux codebase; see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt;
for the OpenZFS pull request, and &lt;a href=&#34;https://github.com/zfsonlinux/zfs/pull/6566&#34;&gt;here&lt;/a&gt; for the ZFS on
Linux pull request.&lt;/p&gt;

&lt;p&gt;In my initial port, I was running into a problem where the automated
tests would trigger a &amp;ldquo;hang&amp;rdquo; as a result of the &lt;code&gt;readmmap&lt;/code&gt; program
calling &lt;code&gt;msync&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ pstree -p 2337
test-runner.py(2337)-+-sudo(3183)---mmap_read_001_p(3185)---readmmap(3198)
                     `-{test-runner.py}(3184)

$ sudo cat /proc/3198/stack
[&amp;lt;ffffffff9bdafb68&amp;gt;] wait_on_page_bit_common+0x118/0x1d0
[&amp;lt;ffffffff9bdafd34&amp;gt;] __filemap_fdatawait_range+0x114/0x190
[&amp;lt;ffffffff9bdafdc4&amp;gt;] filemap_fdatawait_range+0x14/0x30
[&amp;lt;ffffffff9bdb2477&amp;gt;] filemap_write_and_wait_range+0x57/0x90
[&amp;lt;ffffffffc08f049d&amp;gt;] zpl_fsync+0x3d/0x110 [zfs]
[&amp;lt;ffffffff9be7b93b&amp;gt;] vfs_fsync_range+0x4b/0xb0
[&amp;lt;ffffffff9bdf6af2&amp;gt;] SyS_msync+0x182/0x200
[&amp;lt;ffffffff9c4d453b&amp;gt;] entry_SYSCALL_64_fastpath+0x1e/0xad
[&amp;lt;ffffffffffffffff&amp;gt;] 0xffffffffffffffff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this state was reached, the &lt;code&gt;msync&lt;/code&gt; call would never return.&lt;/p&gt;

&lt;p&gt;Without diving too far into the technical details, my hunch was that
the &lt;code&gt;zfs_putpage_commit_cb&lt;/code&gt; function was not being called properly. At
this point, I wanted to verify this, so I could then revisit the code
with concrete data to support my suspicision.&lt;/p&gt;

&lt;p&gt;If I hit this issue on illumos, I would have quickly jumped to using
either &lt;code&gt;dtrace&lt;/code&gt; or &lt;code&gt;mdb&lt;/code&gt; to help verify and debug the situation. Since
I was on Linux, I had neither of these tools at my disposal. Thankfully
though, I did have a test case that would reliably reproduce
the issue in a matter of minutes.&lt;/p&gt;

&lt;p&gt;I thought about adding some &lt;code&gt;printk&lt;/code&gt; or &lt;code&gt;zfs_dbgmsg&lt;/code&gt; statments to the
code and re-compiling to gather some data, but after having used
&lt;code&gt;dtrace&lt;/code&gt; on illumos, I resented that idea. I had previously read about
&lt;a href=&#34;http://www.brendangregg.com/blog/2016-03-05/linux-bpf-superpowers.html&#34;&gt;Linux BPF&lt;/a&gt; and &lt;a href=&#34;https://github.com/iovisor/bcc&#34;&gt;Linux BCC&lt;/a&gt;, so this felt like a
good opportunity try and experiment with those to see if I could use it
to gain better insight into my problem, without making any code changes.&lt;/p&gt;

&lt;h2 id=&#34;building-and-installing-bcc-from-source&#34;&gt;Building and Installing BCC from Source&lt;/h2&gt;

&lt;p&gt;First off, I wanted to build and install BCC from source, rather than
use a pre-built package. This decision was strictly for educational
purposes; I wanted to learn how easy or difficult this would be, in case
I ever wanted to make modifications to it in the future.&lt;/p&gt;

&lt;p&gt;The project contains easy to follow instructions for doing this in its
&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/INSTALL.md&#34;&gt;Install.md&lt;/a&gt; document. Since I was running on Ubuntu 17.04,
the process was documented and works as described:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Trusty and older
VER=trusty
$ echo &amp;quot;deb http://llvm.org/apt/$VER/ llvm-toolchain-$VER-3.7 main
$ deb-src http://llvm.org/apt/$VER/ llvm-toolchain-$VER-3.7 main&amp;quot; | \
  sudo tee /etc/apt/sources.list.d/llvm.list
$ wget -O - http://llvm.org/apt/llvm-snapshot.gpg.key | sudo apt-key add -
$ sudo apt-get update

# All versions
$ sudo apt-get -y install bison build-essential cmake flex git libedit-dev \
  libllvm3.7 llvm-3.7-dev libclang-3.7-dev python zlib1g-dev libelf-dev

# For Lua support
$ sudo apt-get -y install luajit luajit-5.1-dev

$ git clone https://github.com/iovisor/bcc.git
$ mkdir bcc/build; cd bcc/build
$ cmake .. -DCMAKE_INSTALL_PREFIX=/usr
$ make
$ sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running those commands, I found the various BCC tools, examples,
and manpages installed in &lt;code&gt;/usr/share/bcc&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;using-bcc-s-trace&#34;&gt;Using BCC&amp;rsquo;s &amp;ldquo;trace&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Now that I had the BCC tools installed, I could use them to gather some
data about my issue. While there&amp;rsquo;s quite a few tools in the BCC
repository to help trace the various Linux subsystems, what I needed was
targeted specifically at the &lt;code&gt;zfs_putpage_commit_cb&lt;/code&gt; function. I wanted
to see if that function was getting called at all; and for that, the &lt;code&gt;trace&lt;/code&gt;
command was just what I needed.&lt;/p&gt;

&lt;p&gt;First I used this tool while running the ZFS modules without my changes,
to provide a baseline:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/share/bcc/tools/trace -K zfs_putpage_commit_cb 2&amp;gt;/dev/null
PID    TID    COMM         FUNC
27605  27605  readmmap     zfs_putpage_commit_cb
        zfs_putpage_commit_cb+0x1 [kernel]
        zil_commit+0x17 [kernel]
        zpl_writepages+0xd6 [kernel]
        do_writepages+0x1e [kernel]
        __filemap_fdatawrite_range+0xc6 [kernel]
        filemap_write_and_wait_range+0x41 [kernel]
        zpl_fsync+0x3d [kernel]
        vfs_fsync_range+0x4b [kernel]
        sys_msync+0x182 [kernel]
        entry_SYSCALL_64_fastpath+0x1e [kernel]
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the reproducer running in another shell, this told me that this
function definitely was being called when my changes were not applied,
and even provided a stack trace that lead to the function call.&lt;/p&gt;

&lt;p&gt;Now that I verified the behavior without my changes, it was time to run
the same test, but with my modified version of ZFS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# /usr/share/bcc/tools/trace -K zfs_putpage_commit_cb 2&amp;gt;/dev/null
PID    TID    COMM         FUNC
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just like before, I had the reproducer running in another shell, but
this time the &lt;code&gt;trace&lt;/code&gt; command didn&amp;rsquo;t produce any output, which means the
function wasn&amp;rsquo;t called.&lt;/p&gt;

&lt;p&gt;With these observations in mind, I was able to re-visit the code and
ultimately track down my problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Isolating ZIL Disk Activity</title>
      <link>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-08-04-openzfs-isolating-zil-disk-activity/</guid>
      <description>&lt;p&gt;I recently completed a project to improve the performance of the OpenZFS
ZIL (see &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/447&#34;&gt;here&lt;/a&gt; for more
details); i.e. improving the performance of synchronous activity on
OpenZFS, such as writes using the &lt;code&gt;O_SYNC&lt;/code&gt; flag.  As part of that work,
I had to run some performance testing and benchmarking of my code
changes (and the system as a whole), to ensure the system was behaving
as I expected.&lt;/p&gt;

&lt;p&gt;Early on in my benchmarking exercises, I became confused by the data
that I was gathering. I was expecting only a certain number of writes to
be active on the underlying storage devices at any given time, based on
the known workload that I was applying to the zpool, and based on my
understanding of the ZIL&amp;rsquo;s mechanics. When running these known workloads
and inspecting the &lt;code&gt;actv&lt;/code&gt; column from &lt;code&gt;iostat&lt;/code&gt; though, I was
consistently seeing more write activity on the devices than I expected.&lt;/p&gt;

&lt;p&gt;At this point, I was starting to question my understanding of the code
that I had written, and my understanding of the ZIL&amp;rsquo;s mechanics as a
whole. Since I knew exactly the IO workload that was being applied to
the system, why wasn&amp;rsquo;t it behaving as I had predicted?&lt;/p&gt;

&lt;p&gt;After scratching my head and consulting the code numerous times, I asked
Matt Ahrens if he had any clues as to what might be going on. Matt was
quick to remind me that I was failing to incorporate the IO that would
occur as part of &lt;code&gt;spa_sync()&lt;/code&gt; into my mental model. Additionally, he
suggested that since it would be difficult to know exactly how many IOs
to expect from &lt;code&gt;spa_sync()&lt;/code&gt;, which then makes it difficult to verify the
&lt;code&gt;actv&lt;/code&gt; column from &lt;code&gt;iostat&lt;/code&gt; w.r.t. my code changes, I should configure
the system to effectively disable &lt;code&gt;spa_sync()&lt;/code&gt; altogether. This way, all
of the IOs that would be active on the disk would be a result of a ZIL
write, which is exactly what I was previously expecting.&lt;/p&gt;

&lt;p&gt;To acheive this configuration, Matt pointed me at the following kernel
perameters: &lt;code&gt;zfs_dirty_data_max&lt;/code&gt;, &lt;code&gt;zfs_dirty_data_sync&lt;/code&gt;, and
&lt;code&gt;zfs_txg_timeout&lt;/code&gt;. Basically, I had to set all of these values such that
the dirty limit would never be reached, and thus, a TXG sync would never
trigger as a result of the amount of dirty data my workload generated.
Since my test system had 128 GB of RAM, I used the following
commands/values to achieve this configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -kwe &#39;zfs_dirty_data_max/z 0t68719476736&#39;
$ sudo mdb -kwe &#39;zfs_dirty_data_sync/z 0t34359738368&#39;
$ sudo mdb -kwe &#39;zfs_txg_timeout/z 0t3600&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also important to note that these values are dependent on the rate
at which my workload would dirty data (i.e. the workload&amp;rsquo;s write
throughput), and the duration of the test. I ensured that the workload
would not be able to dirty enough data to cause TXG sync prior to the
test completing. With all of this configured correctly, the only way
writes would get issued to disk would be via the ZIL, which is exactly
what I wanted.&lt;/p&gt;

&lt;p&gt;Additionally, I further tuned the system to disable the IO aggregation
that ZFS may do when there&amp;rsquo;s sufficient write activity to warrant it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -kwe &#39;zfs_vdev_aggregation_limit/z 0t0&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While this setting didn&amp;rsquo;t help my workload&amp;rsquo;s throughput, it did help me
validate the correctness of my code changes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running `sshd` on Windows using Cygwin</title>
      <link>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-16-running-sshd-on-windows-using-cygwin/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As part of our effort to support &lt;a href=&#34;https://www.delphix.com/&#34;&gt;Delphix&lt;/a&gt; in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Microsoft_Azure&#34;&gt;Azure&lt;/a&gt; cloud
environment, we&amp;rsquo;re writing some automation to convert our &lt;code&gt;.iso&lt;/code&gt; install
media into a &lt;a href=&#34;https://en.wikipedia.org/wiki/VHD_(file_format)&#34;&gt;VHD&lt;/a&gt; image, leveraging &lt;a href=&#34;https://en.wikipedia.org/wiki/Jenkins_(software)&#34;&gt;Jenkins&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Packer_(software)&#34;&gt;Packer&lt;/a&gt; in
the process.&lt;/p&gt;

&lt;p&gt;Essentially, we want to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Microsoft_Windows&#34;&gt;Windows&lt;/a&gt; server as a Jenkins &amp;ldquo;slave&amp;rdquo;,
and run Packer from within a Jenkins job that will run on that Windows
system.&lt;/p&gt;

&lt;p&gt;In order to do that, the Jenkins &amp;ldquo;master&amp;rdquo; needs to connect with the
Windows system, such that it can configure the system to act as a
Jenkins slave. Additionally, due to some of our existing Jenkins
infrastructure, this requires the Jenkins &amp;ldquo;master&amp;rdquo; using &lt;code&gt;ssh&lt;/code&gt; to
connect to the Windows box. Thus, &lt;code&gt;sshd&lt;/code&gt; is needed on the Windows
system.&lt;/p&gt;

&lt;h2 id=&#34;running-sshd-on-windows-using-cygwin&#34;&gt;Running &lt;code&gt;sshd&lt;/code&gt; on Windows using Cygwin&lt;/h2&gt;

&lt;p&gt;The following is a quick run-down of the steps I needed to perform, to
install and configure &lt;code&gt;sshd&lt;/code&gt; on our Windows server, enabling &lt;code&gt;ssh&lt;/code&gt;
clients (and our Jenkins master) to connect to it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Install Cygwin on the Windows server:

&lt;ul&gt;
&lt;li&gt;I used RDP to connect to the server as the &lt;code&gt;Administrator&lt;/code&gt; user&lt;/li&gt;
&lt;li&gt;Used a web browser to access the &lt;a href=&#34;https://cygwin.com/install.html&#34;&gt;installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Downloaded the &lt;code&gt;setup-x86_64.exe&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;Executed &lt;code&gt;setup-x86_64.exe&lt;/code&gt;, and followed the setup instructions&lt;/li&gt;
&lt;li&gt;When reaching the package selection screen of the setup wizard, I
explicitly selected the following packages:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cygrunsrv&lt;/code&gt; from the &lt;code&gt;Admin&lt;/code&gt; group&lt;/li&gt;
&lt;li&gt;&lt;code&gt;openssh&lt;/code&gt; from the &lt;code&gt;Net&lt;/code&gt; group&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;After the Cygwin setup finished, I used the instructions found
 &lt;a href=&#34;http://www.noah.org/ssh/cygwin-sshd.html&#34;&gt;here&lt;/a&gt; to configure and start the &lt;code&gt;sshd&lt;/code&gt; service. This involved
 running the following commands in a Cygwin shell:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ssh-host-config -y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cygrunsrv -S sshd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;At this point, the &lt;code&gt;sshd&lt;/code&gt; service was running, but the Windows
 server&amp;rsquo;s firewall was blocking incoming connections to port 22. So,
 I had to edit the server&amp;rsquo;s firewall rules to allow incomming
 connections on port 22 (the port &lt;code&gt;sshd&lt;/code&gt; was running on). For this,
 I followed the instructions that I found &lt;a href=&#34;https://techtorials.me/cygwin/configure-windows-firewall/&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;installing-additional-software&#34;&gt;Installing Additional Software&lt;/h2&gt;

&lt;p&gt;In addition to installing Cygwin and running &lt;code&gt;sshd&lt;/code&gt;, we also needed to
perform a little more configuration to the Windows system, as our
Jenkins job had these additional dependencies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To allow ssh access to other systems in our environment from the
Jenkins job, the necessary &lt;code&gt;ssh&lt;/code&gt; keys had to be installed in the
&lt;code&gt;Administrator&lt;/code&gt; user&amp;rsquo;s home directory.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git&lt;/code&gt; was installed using the Cygwin installer.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;packer&lt;/code&gt; was downloaded from the project&amp;rsquo;s &lt;a href=&#34;https://www.packer.io/downloads.html&#34;&gt;download page&lt;/a&gt; and
installed simply by placing the binary at the correct location.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pv&lt;/code&gt; was manually compiled and installed following the directions on
the &lt;a href=&#34;https://sourceforge.net/projects/pvforcygwin/&#34;&gt;project&amp;rsquo;s website&lt;/a&gt;; i.e. using these commands:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;wget http://www.ivarch.com/programs/sources/pv-1.3.4.tar.bz2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tar -xf pv-1.3.4.tar.bz2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd pv-1.3.4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./configure --prefix=&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Notes on ZIL Transactions</title>
      <link>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-03-15-notes-on-zil-transactions/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;http://open-zfs.org&#34;&gt;OpenZFS&lt;/a&gt; &lt;a href=&#34;http://nex7.blogspot.com/2013/04/zfs-intent-log.html&#34;&gt;Intent Log&lt;/a&gt; (ZIL) is used to ensure &lt;a href=&#34;https://en.wikipedia.org/wiki/POSIX&#34;&gt;POSIX&lt;/a&gt;
compliance of certain system calls (that modify the state of a ZFS
dataset), and protect against data loss in the face of failure scenarios
such as: an operating system crash, power loss, etc. Specifically, it&amp;rsquo;s
used as a performance optimization so that applications can be assured
that their given system call, and any &amp;ldquo;user data&amp;rdquo; associated with it,
will not be &amp;ldquo;lost&amp;rdquo;, without having to wait for an entire &lt;a href=&#34;https://www.delphix.com/blog/delphix-engineering/zfs-fundamentals-transaction-groups&#34;&gt;transaction
group&lt;/a&gt; (TXG) to be synced out (which can take on the order of
seconds, on a moderately loaded system).&lt;/p&gt;

&lt;p&gt;So how does this process work? How are these system calls tracked? This
post will attempt to explain how the system calls enter the ZIL, and how
they&amp;rsquo;re tracked by the in-memory portion of the ZIL. What isn&amp;rsquo;t covered
in this post is how the in-memory representation of the system calls get
written to disk, nor how the on-disk ZIL is used to &amp;ldquo;replay&amp;rdquo; the system
calls after a failure event.&lt;/p&gt;

&lt;h2 id=&#34;zfs-intent-log-transactions&#34;&gt;ZFS Intent Log Transactions&lt;/h2&gt;

&lt;p&gt;Much like how ZFS&amp;rsquo;s DMU layer operates on the notion of &amp;ldquo;transactions&amp;rdquo;,
so does the ZIL. For each system call that modifies a dataset&amp;rsquo;s state, a
ZIL transaction is created; referred to as an &lt;code&gt;itx&lt;/code&gt; in the code and this
document. It&amp;rsquo;s important to note, these &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s are created for
&amp;ldquo;synchronous&amp;rdquo; system calls, as well as &amp;ldquo;asynchronous&amp;rdquo; ones. For example,
an &lt;code&gt;itx&lt;/code&gt; will be generated by an application calling &lt;code&gt;write&lt;/code&gt; on a ZFS
dataset, whether the &lt;code&gt;O_SYNC&lt;/code&gt; flag is used or not. Additionally, &lt;em&gt;all&lt;/em&gt;
system calls that modify a dataset will cause an &lt;code&gt;itx&lt;/code&gt; to be generated;
e.g. &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;rename&lt;/code&gt;, &lt;code&gt;setattr&lt;/code&gt;, etc. all will generate an &lt;code&gt;itx&lt;/code&gt;
unique to that system call.&lt;/p&gt;

&lt;h3 id=&#34;in-memory-representation-of-zil-transactions&#34;&gt;In-Memory Representation of ZIL Transactions&lt;/h3&gt;

&lt;p&gt;Each &lt;code&gt;itx&lt;/code&gt; is composed of an &lt;code&gt;itx_t&lt;/code&gt;, as well as a system call specific
component. The &lt;code&gt;itx_t&lt;/code&gt; portion looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct itx {
	list_node_t	itx_node;	/* linkage on zl_itx_list */
	void		*itx_private;	/* type-specific opaque data */
	itx_wr_state_t	itx_wr_state;	/* write state */
	uint8_t		itx_sync;	/* synchronous transaction */
	uint64_t	itx_sod;	/* record size on disk */
	uint64_t	itx_oid;	/* object id */
	lr_t		itx_lr;		/* common part of log record */
	/* followed by type-specific part of lr_xx_t and its immediate data */
} itx_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for a &lt;code&gt;write&lt;/code&gt; system call, the &lt;code&gt;lr_write_t&lt;/code&gt; structure would be
tacked onto the end of the &lt;code&gt;itx_t&lt;/code&gt;; this structure looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;typedef struct {
	lr_t		lr_common;	/* common portion of log record */
	uint64_t	lr_foid;	/* file object to write */
	uint64_t	lr_offset;	/* offset to write to */
	uint64_t	lr_length;	/* user data length to write */
	uint64_t	lr_blkoff;	/* no longer used */
	blkptr_t	lr_blkptr;	/* spa block pointer for replay */
	/* write data will follow for small writes */
} lr_write_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, the in-memory representation for a &lt;code&gt;write&lt;/code&gt; &lt;code&gt;itx&lt;/code&gt; actually looks
like the following (assuming a small amount of data is being written):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct {
	itx_t common;
	lr_write_t uncommon;
	void *data;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The portion of the structure that&amp;rsquo;s common to all &lt;code&gt;itx&lt;/code&gt; types starts at
offset 0, the write specific portion immediately follows the &lt;code&gt;itx_t&lt;/code&gt;,
and then finally the user-data that is being written.&lt;/p&gt;

&lt;h3 id=&#34;determing-the-type-of-a-zil-transaction&#34;&gt;Determing the Type of a ZIL Transaction&lt;/h3&gt;

&lt;p&gt;Since the last structure depicted in the previous section doesn&amp;rsquo;t
actually exist, it&amp;rsquo;s important to understand how the code is able to
determine which structure follows the &lt;code&gt;itx_t&lt;/code&gt; portion of an &lt;code&gt;itx&lt;/code&gt;; since
it could be any one of the many options (e.g. &lt;code&gt;lr_write_t&lt;/code&gt;,
&lt;code&gt;lr_rename_t&lt;/code&gt;, &lt;code&gt;lr_setattr_t&lt;/code&gt;, etc).&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;lr_t&lt;/code&gt; contained within the &lt;code&gt;itx_t&lt;/code&gt; is used for this purpose. It
contains an &lt;code&gt;lrc_txtype&lt;/code&gt; field, which can be used to determine the
&lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s type. For example, if the &lt;code&gt;itx&lt;/code&gt; was for a &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;lrc_tx_type&lt;/code&gt;
would equal &lt;code&gt;TX_WRITE&lt;/code&gt;, and the &lt;code&gt;lr_write_t&lt;/code&gt; could be obtained like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ASSERT(itx-&amp;gt;itx_lr.lrc_txtype == TX_WRITE);
lr_write_t *lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;zil-transactions-by-example&#34;&gt;ZIL Transactions by Example&lt;/h2&gt;

&lt;p&gt;Now that we have a brief understanding of what an &lt;code&gt;itx&lt;/code&gt; is, lets look at
how these are generated and added to the ZIL; we&amp;rsquo;ll be using
&lt;code&gt;zfs_write&lt;/code&gt; as the example code path.&lt;/p&gt;

&lt;h3 id=&#34;accessing-the-in-memory-zil-structure&#34;&gt;Accessing the In-Memory ZIL Structure&lt;/h3&gt;

&lt;p&gt;The first line of code within &lt;code&gt;zfs_write&lt;/code&gt; that pertains to the ZIL&amp;rsquo;s
machinery is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zilog = zfsvfs-&amp;gt;z_log;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to manipulate the ZIL, a pointer to the ZIL structure (i.e. the
&lt;code&gt;zilog_t&lt;/code&gt;) is needed. This is obtained using the &lt;code&gt;vnode_t&lt;/code&gt; (passed in as
a parameter to &lt;code&gt;zfs_write&lt;/code&gt;) to extract a pointer to the corresponding
&lt;code&gt;znode_t&lt;/code&gt;; the &lt;code&gt;v_data&lt;/code&gt; field of the &lt;code&gt;vnode_t&lt;/code&gt; holds a pointer to the
&lt;code&gt;znode_t&lt;/code&gt;. Once we have the &lt;code&gt;znode_t&lt;/code&gt; for this specific file, it&amp;rsquo;s
trivial to use its &lt;code&gt;z_zfsvfs&lt;/code&gt; field to access the &lt;code&gt;zfsvfs_t&lt;/code&gt;, and then
the &lt;code&gt;z_log&lt;/code&gt; field to access the &lt;code&gt;zilog_t&lt;/code&gt;. It&amp;rsquo;s worth noting that the
&lt;code&gt;zilog_t&lt;/code&gt; is shared across all files in the same dataset (i.e. there&amp;rsquo;s a
single ZIL per ZFS dataset).&lt;/p&gt;

&lt;h3 id=&#34;zil-transaction-creation-and-assignment&#34;&gt;ZIL Transaction Creation and Assignment&lt;/h3&gt;

&lt;p&gt;Once the &lt;code&gt;zilog_t&lt;/code&gt; structure is obtained, the next line within
&lt;code&gt;zfs_write&lt;/code&gt; pertaining to the ZIL is this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;zfs_log_write(zilog, tx, TX_WRITE, zp, woff, tx_bytes, ioflag);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;zfs_log_write&lt;/code&gt; function is used to create new &lt;code&gt;itx&lt;/code&gt;s, as well as
insert them into one of the ZIL&amp;rsquo;s list of transactions.&lt;/p&gt;

&lt;h4 id=&#34;zil-itx-create&#34;&gt;&lt;code&gt;zil_itx_create&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Inside of &lt;code&gt;zil_log_write&lt;/code&gt;, the function &lt;code&gt;zil_itx_create&lt;/code&gt; is used to
allocate one or more &lt;code&gt;itx_t&lt;/code&gt; structures to represent the &lt;code&gt;write&lt;/code&gt; system
within the in-memory portion of the ZIL. That can be seen here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (resid) {
	itx_t *itx;
	lr_write_t *lr;
	ssize_t len;

	/*
	 * If the write would overflow the largest block then split it.
	 */
	if (write_state != WR_INDIRECT &amp;amp;&amp;amp; resid &amp;gt; ZIL_MAX_LOG_DATA)
		len = SPA_OLD_MAXBLOCKSIZE &amp;gt;&amp;gt; 1;
	else
		len = resid;

	// ... &amp;lt;snip&amp;gt; ...

	itx = zil_itx_create(txtype, sizeof (*lr) +
	    (write_state == WR_COPIED ? len : 0));

	// ... &amp;lt;snip&amp;gt; ...

	resid -= len;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;zil_itx_create&lt;/code&gt; simply allocates and initializes the in-memory
&lt;code&gt;itx_t&lt;/code&gt; structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;itx_t *
zil_itx_create(uint64_t txtype, size_t lrsize)
{
	itx_t *itx;

	lrsize = P2ROUNDUP_TYPED(lrsize, sizeof (uint64_t), size_t);

	itx = kmem_alloc(offsetof(itx_t, itx_lr) + lrsize, KM_SLEEP);
	itx-&amp;gt;itx_lr.lrc_txtype = txtype;
	itx-&amp;gt;itx_lr.lrc_reclen = lrsize;
	itx-&amp;gt;itx_sod = lrsize; /* if write &amp;amp; WR_NEED_COPY will be increased */
	itx-&amp;gt;itx_lr.lrc_seq = 0;	/* defensive */
	itx-&amp;gt;itx_sync = B_TRUE;		/* default is synchronous */

	return (itx);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, for a single &lt;code&gt;write&lt;/code&gt; (and single call to &lt;code&gt;zfs_log_write&lt;/code&gt;), 1 or
more &lt;code&gt;itx&lt;/code&gt;s will be created to represent the system call.&lt;/p&gt;

&lt;h4 id=&#34;zil-itx-assign&#34;&gt;&lt;code&gt;zil_itx_assign&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;At this point, the &lt;code&gt;itx&lt;/code&gt;s representing the &lt;code&gt;write&lt;/code&gt; will have been
allocated and initialized, but they&amp;rsquo;re not yet part of the ZIL. In order
to add the &lt;code&gt;itx&lt;/code&gt;s to the ZIL, the &lt;code&gt;zil_itx_assign&lt;/code&gt; function is used.
Expanding the previous code snippet from &lt;code&gt;zfs_write&lt;/code&gt;, we can see that
&lt;code&gt;zil_itx_assign&lt;/code&gt; is called immediately after the individual &lt;code&gt;itx&lt;/code&gt;s are
created and initialized:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while (resid) {
	itx_t *itx;
	lr_write_t *lr;
	ssize_t len;

	/*
	 * If the write would overflow the largest block then split it.
	 */
	if (write_state != WR_INDIRECT &amp;amp;&amp;amp; resid &amp;gt; ZIL_MAX_LOG_DATA)
		len = SPA_OLD_MAXBLOCKSIZE &amp;gt;&amp;gt; 1;
	else
		len = resid;

	itx = zil_itx_create(txtype, sizeof (*lr) +
	    (write_state == WR_COPIED ? len : 0));
	lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
	if (write_state == WR_COPIED &amp;amp;&amp;amp; dmu_read(zp-&amp;gt;z_zfsvfs-&amp;gt;z_os,
	    zp-&amp;gt;z_id, off, len, lr + 1, DMU_READ_NO_PREFETCH) != 0) {
		zil_itx_destroy(itx);
		itx = zil_itx_create(txtype, sizeof (*lr));
		lr = (lr_write_t *)&amp;amp;itx-&amp;gt;itx_lr;
		write_state = WR_NEED_COPY;
	}

	itx-&amp;gt;itx_wr_state = write_state;
	if (write_state == WR_NEED_COPY)
		itx-&amp;gt;itx_sod += len;
	lr-&amp;gt;lr_foid = zp-&amp;gt;z_id;
	lr-&amp;gt;lr_offset = off;
	lr-&amp;gt;lr_length = len;
	lr-&amp;gt;lr_blkoff = 0;
	BP_ZERO(&amp;amp;lr-&amp;gt;lr_blkptr);

	itx-&amp;gt;itx_private = zp-&amp;gt;z_zfsvfs;

	if (!(ioflag &amp;amp; (FSYNC | FDSYNC)) &amp;amp;&amp;amp; (zp-&amp;gt;z_sync_cnt == 0) &amp;amp;&amp;amp;
	    (fsync_cnt == 0))
		itx-&amp;gt;itx_sync = B_FALSE;

	zil_itx_assign(zilog, itx, tx);

	off += len;
	resid -= len;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The purpose of &lt;code&gt;zil_itx_assign&lt;/code&gt; is to insert the new &lt;code&gt;itx&lt;/code&gt; into one of
the ZIL&amp;rsquo;s list of transactions; the details of that process is covered
next (it&amp;rsquo;s complicated enough to warrant a new section).&lt;/p&gt;

&lt;h3 id=&#34;zl-itxg-4-i-sync-list-i-async-tree-and-ia-list&#34;&gt;&lt;code&gt;zl_itxg[4]&lt;/code&gt;, &lt;code&gt;i_sync_list&lt;/code&gt;, &lt;code&gt;i_async_tree&lt;/code&gt;, and &lt;code&gt;ia_list&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;In order to fully understand how a given &lt;code&gt;itx&lt;/code&gt; gets inserted into the
in memory representation of the ZIL (via &lt;code&gt;zil_itx_assign&lt;/code&gt;), one has to
understand a few things about the &lt;code&gt;zilog_t&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;zl_itxg&lt;/code&gt; field of the &lt;code&gt;zilog_t&lt;/code&gt; contains 4 unique &lt;code&gt;itxg_t&lt;/code&gt;s,
 and each &lt;code&gt;itxg_t&lt;/code&gt; maps to a particular DMU transaction group. Each
 &lt;code&gt;itx&lt;/code&gt; created is applicable to a specific DMU transaction (&lt;code&gt;tx&lt;/code&gt;),
 which is then applicable to a specific DMU transaction group
 (&lt;code&gt;txg&lt;/code&gt;). Thus, the &lt;code&gt;tx&lt;/code&gt; is used to determine the &lt;code&gt;txg&lt;/code&gt; for this
 specific &lt;code&gt;itx&lt;/code&gt;, and the &lt;code&gt;txg&lt;/code&gt; is used to determine which of the 4
 different &lt;code&gt;itxg_t&lt;/code&gt;s (from the &lt;code&gt;zl_itxg&lt;/code&gt; array) will be used when
 inserting the &lt;code&gt;itx&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each of the 4 &lt;code&gt;itxg_t&lt;/code&gt;&amp;rsquo;s maintain a:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i_sync_list&lt;/code&gt; which is a simple linked list of &lt;code&gt;itx_t&lt;/code&gt; structures.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i_async_tree&lt;/code&gt; which is an AVL tree of &lt;code&gt;itx_async_node_t&lt;/code&gt;
structures (indexed by DMU object ID), where each node in the tree
(each &lt;code&gt;itx_async_node_t&lt;/code&gt;) maintains its own &lt;code&gt;ia_list&lt;/code&gt;, which is a
linked list of &lt;code&gt;itx_t&lt;/code&gt; structures.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Below is a diagram that attempts to illustrate this (it&amp;rsquo;s not as
complicated as the above explanation might sound):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;zil-itx-lists.png&#34; alt=&#34;ZIL Transaction Lists&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;insertion-of-a-zil-transaction&#34;&gt;Insertion of a ZIL Transaction&lt;/h3&gt;

&lt;p&gt;So, when &lt;code&gt;zil_itx_assign&lt;/code&gt; is called in the context of &lt;code&gt;zfs_log_write&lt;/code&gt;,
the &lt;code&gt;itx&lt;/code&gt; will be inserted into one of &lt;code&gt;itxg_t&lt;/code&gt;&amp;rsquo;s lists described in the
previous section. The following logic/criteria is used to decide which
list to use, when inserting the new &lt;code&gt;itx&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First, which of the 4 &lt;code&gt;itxg_t&lt;/code&gt;s needs to be chosen. The &lt;code&gt;txg&lt;/code&gt; that
 the given &lt;code&gt;itx&lt;/code&gt; is associated with is used to make this decision.
 &lt;code&gt;dmu_tx_get_txg&lt;/code&gt; is used to obtain the &lt;code&gt;txg&lt;/code&gt; from the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s &lt;code&gt;tx&lt;/code&gt;,
 and then the &lt;code&gt;txg&lt;/code&gt; is bit-wise AND-ed with &lt;code&gt;TXG_MASK&lt;/code&gt; to determine
 which &lt;code&gt;itxg_t&lt;/code&gt; to use.&lt;/li&gt;
&lt;li&gt;Now that the &lt;code&gt;itxg_t&lt;/code&gt; is chosen, the linked list that will be used
 when inserting the &lt;code&gt;itx&lt;/code&gt; needs to be decided. This depends on the
 value of the &lt;code&gt;itx&lt;/code&gt;&amp;rsquo;s &lt;code&gt;itx_sync&lt;/code&gt; field (i.e. based on if the write
 is synchronous or not):

&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;itx_sync&lt;/code&gt; is &lt;code&gt;TRUE&lt;/code&gt;, the &lt;code&gt;itx&lt;/code&gt; is appended to the
&lt;code&gt;i_sync_list&lt;/code&gt; of the &lt;code&gt;itxg_t&lt;/code&gt; chosen in (1).&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;itx_sync&lt;/code&gt; is &lt;code&gt;FALSE&lt;/code&gt;, &lt;code&gt;avl_find&lt;/code&gt; is used to find the
&lt;code&gt;itx_asnyc_node_t&lt;/code&gt; for the specific file being written, from
the &lt;code&gt;i_async_tree&lt;/code&gt; of the &lt;code&gt;itxg_t&lt;/code&gt; chosen in (1). Once the
&lt;code&gt;itx_async_node_t&lt;/code&gt; is found, the &lt;code&gt;itx&lt;/code&gt; is appended to that
structure&amp;rsquo;s &lt;code&gt;ia_list&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the &lt;code&gt;itx&lt;/code&gt; has been assigned to either the &lt;code&gt;i_sync_list&lt;/code&gt; or one of
the &lt;code&gt;ia_list&lt;/code&gt; lists (contained in the &lt;code&gt;itx_async_node_t&lt;/code&gt;), the work of
&lt;code&gt;zfs_log_write&lt;/code&gt; is finished. This specific &lt;code&gt;write&lt;/code&gt; system call has been
recorded and inserted into the in-memory representation of the ZIL, but
this record will still be lost if a power loss were to happen at this
point.&lt;/p&gt;

&lt;p&gt;In order for the record of the &lt;code&gt;write&lt;/code&gt; to persist in the event of a
power loss (or an equivalent operating system crash), the &lt;code&gt;itx&lt;/code&gt;s need
to be written to the on-disk representation of the ZIL. Notes on that
process will be left for another post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Refresher on `zpool reguid` Using Examples</title>
      <link>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-22-openzfs-refresher-on-zpool-reguid-using-examples/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;zpool reguid&lt;/code&gt; command can be used to regenerate the &lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSGuids&#34;&gt;GUID&lt;/a&gt;
for an &lt;a href=&#34;http://www.open-zfs.org&#34;&gt;OpenZFS&lt;/a&gt; pool, which is useful when using device level
copies to generate multiple pools all with the same contents.&lt;/p&gt;

&lt;h2 id=&#34;example-using-file-vdevs&#34;&gt;Example using File VDEVs&lt;/h2&gt;

&lt;p&gt;As a contrived example, lets create a zpool backed by a single file
vdev:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mkdir /tmp/tank1
# mkfile -n 256m /tmp/tank1/vdev
# zpool create tank1 /tmp/tank1/vdev
# zpool list tank1
NAME    SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank1   240M    78K   240M         -     1%     0%  1.00x  ONLINE  -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can export this zpool, copy the file, and attempt to import two
identical zpools; one backed by the original file we created above, and
another zpool backed by the copy of the original file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank1
# mkdir /tmp/tank2
# cp /tmp/tank1/vdev /tmp/tank2/vdev
# zpool import -d /tmp/tank1 tank1
# zpool import -d /tmp/tank2 tank1 tank2
cannot import &#39;tank1&#39;: a pool with that name is already created/imported,
and no additional pools with that name were found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While the intention was to have two pools imported, &lt;code&gt;tank1&lt;/code&gt; and &lt;code&gt;tank2&lt;/code&gt;,
as one can see, the second &lt;code&gt;zpool import&lt;/code&gt; failed. The failure was due
to the two pools sharing the same GUID; we can verify the two pools
share the same GUID using &lt;code&gt;zdb&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank1
# zdb -e -p /tmp/tank1 -C tank1 | grep pool_guid
        pool_guid: 750003400893681264
# zdb -e -p /tmp/tank2 -C tank1 | grep pool_guid
        pool_guid: 750003400893681264
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is where &lt;code&gt;zpool reguid&lt;/code&gt; becomes useful; after importing &lt;code&gt;tank1&lt;/code&gt; we
can use &lt;code&gt;zpool reguid&lt;/code&gt; to change that zpool&amp;rsquo;s GUID such that we can then
import &lt;code&gt;tank2&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool import -d /tmp/tank1 tank1
# zpool reguid tank1
# zpool import -d /tmp/tank2 tank1 tank2
# zpool list tank1 tank2
NAME    SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank1   240M   110K   240M         -     3%     0%  1.00x  ONLINE  -
tank2   240M   114K   240M         -     2%     0%  1.00x  ONLINE  -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, we can use &lt;code&gt;zdb&lt;/code&gt; again to verify the GUID for the two
pools is different:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zdb -C tank1 | grep pool_guid
        pool_guid: 12195257967456241841
# zdb -C tank2 | grep pool_guid
        pool_guid: 750003400893681264
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: we don&amp;rsquo;t need the &lt;code&gt;-e&lt;/code&gt; and &lt;code&gt;-p&lt;/code&gt; options like before, because these
two pools are imported.&lt;/p&gt;

&lt;h2 id=&#34;examples-using-damaged-zpools&#34;&gt;Examples Using Damaged ZPOOLs&lt;/h2&gt;

&lt;p&gt;The manpage for the &lt;code&gt;zpool reguid&lt;/code&gt; command states this can only be used
on zpools that are &amp;ldquo;online and healthy&amp;rdquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     zpool reguid pool
             Generates a new unique identifier for the pool. You must ensure
             that all devices in this pool are online and healthy before
             performing this action.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, what happens when a device is damaged and this command is attempted
to be used? Let&amp;rsquo;s try some examples and find out.&lt;/p&gt;

&lt;h3 id=&#34;example-using-a-damaged-vdev-in-an-empty-zpool&#34;&gt;Example Using a Damaged VDEV in an Empty ZPOOL&lt;/h3&gt;

&lt;p&gt;For this example, we&amp;rsquo;ll create another zpool using file based VDEVs, but
this time the zpool will be striped across 3 files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# mkdir /tmp/tank
# mkfile -n 256m /tmp/tank/vdev1
# mkfile -n 256m /tmp/tank/vdev2
# mkfile -n 256m /tmp/tank/vdev3
# zpool create tank /tmp/tank/vdev{1,2,3}
# zpool list -v tank
NAME                SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank                720M    78K   720M         -     0%     0%  1.00x  ONLINE  -
  /tmp/tank/vdev1   240M    30K   240M         -     0%     0%
  /tmp/tank/vdev2   240M    18K   240M         -     0%     0%
  /tmp/tank/vdev3   240M    30K   240M         -     0%     0%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets damage the pool by writing random data over the most of one of
the VDEVs. We explicitly avoid writing to the first and last 1MB of the
file to leave the VDEV&amp;rsquo;s labels intact.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@oi-hipster:~# dd if=/dev/urandom of=/tmp/tank/vdev3 bs=1M seek=1 count=254
254+0 records in
254+0 records out
266338304 bytes transferred in 4.534832 secs (58731677 bytes/sec)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To verify the damage, we use &lt;code&gt;zpool scrub&lt;/code&gt; and &lt;code&gt;zpool status&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool scrub
# zpool status -v tank
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
        using &#39;zpool clear&#39; or replace the device with &#39;zpool replace&#39;.
   see: http://illumos.org/msg/ZFS-8000-9P
  scan: scrub repaired 28.5K in 0h0m with 0 errors on Wed Feb 22 22:34:18 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               DEGRADED     0     0     0
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  DEGRADED     0     0    24  too many errors

errors: No known data errors
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, when we attempt to run &lt;code&gt;zpool reguid&lt;/code&gt;, the command fails due to the
zpool being in an unhealthy state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool reguid tank
cannot reguid &#39;tank&#39;: one or more devices is currently unavailable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but if we use &lt;code&gt;zpool clear&lt;/code&gt; to bring the pool back into a healthy state,
we can issue the &lt;code&gt;zpool reguid&lt;/code&gt; command without issues:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool clear tank
# zpool reguid tank
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: remember that &lt;code&gt;zpool scrub&lt;/code&gt; will have corrected the damage
previously done by &lt;code&gt;dd&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;example-using-a-damaged-vdev-in-a-non-empty-zpool&#34;&gt;Example Using a Damaged VDEV in a Non-Empty ZPOOL&lt;/h3&gt;

&lt;p&gt;Exanding on the previous example, we&amp;rsquo;ll perform an almost identical
test, except we&amp;rsquo;ll now fill the zpool with a 256MB file instead of using
an empty zpool. So, again, we start by creating the zpool and our 256MB
file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool export tank
# rm /tmp/tank/*
# mkfile -n 256m /tmp/tank/vdev1
# mkfile -n 256m /tmp/tank/vdev2
# mkfile -n 256m /tmp/tank/vdev3
# zpool create tank /tmp/tank/vdev{1,2,3}
# dd if=/dev/urandom of=/tank/file bs=1M count=256
256+0 records in
256+0 records out
268435456 bytes transferred in 4.641438 secs (57834542 bytes/sec)
# zpool list -v tank
NAME                SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank                720M   256M   464M         -    10%    35%  1.00x  ONLINE  -
  /tmp/tank/vdev1   240M  85.3M   155M         -    11%    35%
  /tmp/tank/vdev2   240M  85.3M   155M         -    10%    35%
  /tmp/tank/vdev3   240M  85.8M   154M         -    10%    35%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now damaging the VDEV, and running &lt;code&gt;zpool scrub&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dd if=/dev/urandom of=/tmp/tank/vdev3 bs=1M seek=1 count=254
254+0 records in
254+0 records out
266338304 bytes transferred in 4.225401 secs (63032676 bytes/sec)
# zpool scrub tank
# zpool status -v tank
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://illumos.org/msg/ZFS-8000-8A
  scan: scrub repaired 87.5K in 0h0m with 686 errors on Wed Feb 22 22:58:40 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               DEGRADED     0     0   686
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  DEGRADED     0     0 1.37K  too many errors

errors: Permanent errors have been detected in the following files:

        /tank/file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, there&amp;rsquo;s &amp;ldquo;Permanent errors&amp;rdquo; detected to the file that we
created, but the pool itself should still be intact and reparied by the
&lt;code&gt;zpool scrub&lt;/code&gt;. Thus, we&amp;rsquo;ll use &lt;code&gt;zpool clear&lt;/code&gt; to clear the errors like we
did in the previous example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool clear tank
# zpool status -v tank
  pool: tank
 state: ONLINE
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://illumos.org/msg/ZFS-8000-8A
  scan: scrub repaired 87.5K in 0h0m with 686 errors on Wed Feb 22 22:58:40 2017
config:

        NAME               STATE     READ WRITE CKSUM
        tank               ONLINE       0     0     0
          /tmp/tank/vdev1  ONLINE       0     0     0
          /tmp/tank/vdev2  ONLINE       0     0     0
          /tmp/tank/vdev3  ONLINE       0     0     0

errors: Permanent errors have been detected in the following files:

        /tank/file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point, I would expect that we could now use &lt;code&gt;zpool reguid&lt;/code&gt;
on this zpool; even though there&amp;rsquo;s &amp;ldquo;Permanent errors&amp;rdquo; in the user data
file, the pool is online an healthy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zdb -C tank | grep pool_guid
        pool_guid: 6847294342266961459
# zpool reguid tank
# zdb -C tank | grep pool_guid
        pool_guid: 13248255241205296188
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and this is exactly the behavior seen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a Custom Amazon EC2 AMI from ISO (using OI Hipster)</title>
      <link>https://www.prakashsurya.com/post/2017-02-06-creating-a-custom-amazon-ec2-ami-from-iso/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-06-creating-a-custom-amazon-ec2-ami-from-iso/</guid>
      <description>

&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;

&lt;p&gt;In this post, I&amp;rsquo;ll pick up from where I left off
&lt;a href=&#34;https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/&#34;&gt;last time&lt;/a&gt;, and demonstrate one potential way to convert
the installation ISO media generated in that post, into an &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html&#34;&gt;AMI&lt;/a&gt;
that can be used to create new VMs in the &lt;a href=&#34;https://aws.amazon.com/documentation/ec2/&#34;&gt;Amazon EC2&lt;/a&gt; environment.
It&amp;rsquo;s important to note a couple things before we start:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;While I&amp;rsquo;ll be generating an AMI based on &lt;a href=&#34;https://wiki.openindiana.org/oi/Hipster&#34;&gt;OI Hipster&lt;/a&gt;,
 this process should be applicable to any Linux or FreeBSD based
 operating system as well (and quite possibly Windows too, but I
 don&amp;rsquo;t know much about that platform).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I make no gaurantees about whether the process that&amp;rsquo;ll be
 demonstrated is correct, efficient, or complete. I have little
 to no expertise in this area, and this is simply a write up of the
 notes that I took as I was teaching myself how to do this. Just
 because the process below worked for me, does not necessarily mean
 it will work for anybody else. Thus, if you&amp;rsquo;re using this as a
 guide to create your own custom AMI(s), be aware, YMMV.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With that out of the way, lets get started.&lt;/p&gt;

&lt;h2 id=&#34;step-0-create-template-for-root-volume-of-ami&#34;&gt;Step 0: Create Template for Root Volume of AMI&lt;/h2&gt;

&lt;p&gt;The first step that we need to perform, is to create a disk image
that&amp;rsquo;ll be used as the template for the root volume of the AMI that
we&amp;rsquo;ll be creating. If you already have a &lt;code&gt;raw&lt;/code&gt; VM disk image that you
intend to use as the AMI root volume template, than this step can be
skipped. For the sake of completeness, I&amp;rsquo;m including one possible way to
generate this &lt;code&gt;raw&lt;/code&gt; disk image, that&amp;rsquo;ll then be used in later steps.&lt;/p&gt;

&lt;p&gt;In order to generate this &lt;code&gt;raw&lt;/code&gt; disk image, we&amp;rsquo;ll create a VM that&amp;rsquo;ll be
used to execute the ISO installer, which will install our operating
system (OI Hipster, in my case) onto a &amp;ldquo;disk&amp;rdquo;, that we can then upload
to Amazon and use as our AMI&amp;rsquo;s root volume template.&lt;/p&gt;

&lt;p&gt;For this guide, I&amp;rsquo;ll be using a &lt;a href=&#34;https://www.debian.org/&#34;&gt;Debian&lt;/a&gt; server that&amp;rsquo;s
configured as a &lt;a href=&#34;https://wiki.debian.org/Xen&#34;&gt;Xen host&lt;/a&gt; to create the VM and disk image.
First, lets create a sparse file that&amp;rsquo;ll be attached as the VM&amp;rsquo;s only
virtual disk; after the ISO installation completes, this file will be
our &lt;code&gt;raw&lt;/code&gt; disk image that&amp;rsquo;ll be converted into the AMI&amp;rsquo;s root volume.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ truncate -s 64G ami-template.img
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to create the VM, I need to specify the hardware configuration to
use. I do this using the following Xen VM configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt;EOF &amp;gt; ami-template.cfg
&amp;gt; builder=&#39;hvm&#39;
&amp;gt; name=&#39;ami-template&#39;
&amp;gt; vcpus=4
&amp;gt; memory=4096
&amp;gt; vif=[&#39;bridge=xenbr0, type=ioemu&#39;]
&amp;gt; disk=[  &#39;file:/root/OpenIndiana_Text_X86.iso,hdb:cdrom,r&#39;,
&amp;gt;         &#39;file:/root/ami-template.img,xvda,w&#39; ]
&amp;gt; boot=&#39;d&#39;
&amp;gt; vnc=1
&amp;gt; vnclisten=&#39;0.0.0.0&#39;
&amp;gt; vncconsole=1
&amp;gt; on_crash=&#39;preserve&#39;
&amp;gt; xen_platform_pci=1
&amp;gt; serial=&#39;pty&#39;
&amp;gt; on_reboot=&#39;destroy&#39;
&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For those unfamilar with Xen&amp;rsquo;s configuration files, this states that the
VM will be given:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;4 CPUs&lt;/li&gt;
&lt;li&gt;4G of RAM&lt;/li&gt;
&lt;li&gt;The installation ISO will be attached as a CDROM&lt;/li&gt;
&lt;li&gt;The sparse file generated above will be used as the VM&amp;rsquo;s only disk&lt;/li&gt;
&lt;li&gt;The console will be displayed over &lt;a href=&#34;https://en.wikipedia.org/wiki/Virtual_Network_Computing&#34;&gt;VNC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;etc&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With that configuration file generated, we can go ahead and create/start
the VM with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ xm create ami-template.cfg
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the VM running and it&amp;rsquo;s console being displayed over VNC, I can
then connect to the console and manually walk through the installer&amp;rsquo;s
instructions. After the installer completes, we&amp;rsquo;ll have a disk image
(the &lt;code&gt;ami-template.img&lt;/code&gt; file) that&amp;rsquo;ll be uploaded to Amazon and used to
generate the AMI in later steps.&lt;/p&gt;

&lt;h2 id=&#34;step-1-convert-raw-disk-image-into-stream-optimized-vmdk-format&#34;&gt;Step 1: Convert Raw Disk Image into Stream Optimized VMDK Format&lt;/h2&gt;

&lt;p&gt;Now that we&amp;rsquo;ve run the OI Hipster installer to completion, we should
have a &lt;code&gt;ami-template.img&lt;/code&gt; file that contains a complete installation of
our operating system. While we &lt;em&gt;could&lt;/em&gt; upload directly to Amazon as-is,
we won&amp;rsquo;t do this as it&amp;rsquo;d be an inefficient use of space (and time).&lt;/p&gt;

&lt;p&gt;Remember that when we generated the file that we attached to the VM in
the &lt;a href=&#34;#step-0-create-template-for-root-volume-of-ami&#34;&gt;previous step&lt;/a&gt;, we used a sparse file; we can use &lt;code&gt;ls&lt;/code&gt; to
see how much space was saved by doing so (as opposed to a non-sparse
file):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lsh ami-template.img
3.6G -rw-r--r-- 1 root root 64G Feb  5 22:10 ami-template.img
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this output, we can see that while the file is 64G in size, it&amp;rsquo;s
only taking up 3.6G of space. Thus, it&amp;rsquo;d be great if we could upload
only 3.6G (or less) worth of data to Amazon, rather than the full 64G
(most of which would be zeros).&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s what this step is about; rather than uploading the
&lt;code&gt;ami-template.img&lt;/code&gt; file as-is, which would result in 64G of data being
transferred over the network, we&amp;rsquo;ll convert the file into a &amp;ldquo;stream
optimized&amp;rdquo; VMDK and upload this converted file. As a result, we&amp;rsquo;ll only
have to send a small fraction (of the original file&amp;rsquo;s size) of data over
the network, when compared to if we hadn&amp;rsquo;t done this conversion; which
is a huge benefit in terms of the latency required to perform the
upload.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use the &lt;code&gt;VMDKStream.py&lt;/code&gt; utility to do the conversion, so first we
need to download this tool:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget -O VMDK-stream-converter-0.2.tar.gz \
    https://github.com/imcleod/VMDK-stream-converter/archive/0.2.tar.gz

$ tar -xf VMDK-stream-converter-0.2.tar.gz

$ ls VMDK-stream-converter-0.2
COPYING  README  setup.py  VMDKstream.py
root@kvmserver1:~/psurya# ls -l VMDK-stream-converter-0.2
total 40
-rw-rw-r-- 1 root root 18092 Jun 29  2011 COPYING
-rw-rw-r-- 1 root root   576 Jun 29  2011 README
-rw-rw-r-- 1 root root   120 Jun 29  2011 setup.py
-rwxrwxr-x 1 root root 11454 Jun 29  2011 VMDKstream.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now we can use it to read the original &lt;code&gt;raw&lt;/code&gt; disk image, and convert
it into the stream optimized &lt;code&gt;vmdk&lt;/code&gt; that we want:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./VMDK-stream-converter-0.2/VMDKstream.py ami-template.img ami-template.vmdk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After this completes, we can use &lt;code&gt;ls&lt;/code&gt; again to see just how much less
data will be needed to transfer the disk image using this new format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lsh ami-template.img ami-template.vmdk
 3.6G -rw-r--r-- 1 root root   64G Feb  5 22:10 ami-template.img
1022M -rw-r--r-- 1 root root 1022M Feb  5 22:35 ami-template.vmdk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As one can see, the converted file is &lt;strong&gt;much&lt;/strong&gt; smaller than even the
sparse file that we started with; rather than sending 64G over the
network, we&amp;rsquo;ll only have to send about 1G.&lt;/p&gt;

&lt;p&gt;Now with our stream optimized VMDK file generated, we can move on to
actually performing the upload of this file to Amazon.&lt;/p&gt;

&lt;h2 id=&#34;step-2-download-ec2-tools-and-initialize-environment&#34;&gt;Step 2: Download EC2 Tools and Initialize Environment&lt;/h2&gt;

&lt;p&gt;To perform the upload of the VMDK file that we had just generated, as
well as the various other steps involved in creating the AMI, we&amp;rsquo;ll use
the EC2 API Tools provided by Amazon. Thus, the first thing we need to
do, is to download these tools (assuming they aren&amp;rsquo;t already installed).&lt;/p&gt;

&lt;h3 id=&#34;step-2-1-download-ec2-api-tools&#34;&gt;Step 2.1: Download EC2 API Tools&lt;/h3&gt;

&lt;p&gt;This is simple, using &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;unzip&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip
$ unzip ec2-api-tools.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tools will be extracted into a directory named &lt;code&gt;ec2-api-tools&lt;/code&gt;, and
postfixed with the version of the tools downloaded. In this example,
version &lt;code&gt;1.7.5.1&lt;/code&gt; was downloaded, so the directory containing the tools
will be &lt;code&gt;ec2-api-tools-1.7.5.1&lt;/code&gt;, and we can verify this using &lt;code&gt;ls&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -l ec2-api-tools-1.7.5.1/
total 100
drwxr-xr-x 2 root root 36864 Sep  7  2015 bin
drwxr-xr-x 2 root root  4096 Sep  7  2015 lib
-rw-r--r-- 1 root root  4852 Sep  7  2015 license.txt
-rw-r--r-- 1 root root   539 Sep  7  2015 notice.txt
-rw-r--r-- 1 root root 46468 Sep  7  2015 THIRDPARTYLICENSE.TXT
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-2-initialize-necessary-environment-variables&#34;&gt;Step 2.2: Initialize Necessary Environment Variables&lt;/h3&gt;

&lt;p&gt;In conjunction with the EC2 API Tools, we&amp;rsquo;ll also make use of the
following environment variables to configure the invocation of the
tools. The following is a list of the environment variables that we&amp;rsquo;ll
use in the following sections, as well as a brief description of each:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt;: This is required so the tools can authenticate as
&amp;ldquo;you&amp;rdquo; when making API request to Amazon. This needs to be set based
on each individual&amp;rsquo;s specific key. For more information, see
&lt;a href=&#34;https://aws.amazon.com/developers/access-keys/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_SECRET_KEY&lt;/code&gt;: In addition to the &amp;ldquo;access key&amp;rdquo; described above,
authenticating with the Amazon APIs also requires a &amp;ldquo;secret key&amp;rdquo;.
Exactly like the &amp;ldquo;access key&amp;rdquo;, this must be set based on the
individual&amp;rsquo;s specific key. See &lt;a href=&#34;https://aws.amazon.com/developers/access-keys/&#34;&gt;here&lt;/a&gt; for more
information.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_ZONE&lt;/code&gt;: This will specify the AWS availability zone that will be
used when generating the AMI. For more information about AWS
availability zones, see &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_S3_BUCKET&lt;/code&gt;: This will specify the AWS S3 bucket used when
uploading the disk image. The disk image will be uploaded to this
bucket, and then a snapshot of the generated volume will be taken,
and this snapshot used to create the root volume of the AMI. For
more information about Amazon S3, see &lt;a href=&#34;https://aws.amazon.com/documentation/s3/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_AMI_NAME&lt;/code&gt;: This will specify the name given to the AMI.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;AWS_AMI_DESC&lt;/code&gt;: This will specify the description given to the AMI.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is intended to serve as an example of the values that
could be used for these various environment variables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export AWS_ACCESS_KEY=AKIAIOSFODNN7EXAMPLE
$ export AWS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
$ export AWS_ZONE=us-west-1a
$ export AWS_S3_BUCKET=ivol-openindiana-hipster
$ export AWS_AMI_NAME=&amp;quot;OpenIndiana Hipster Testing 2017.02.06 (HVM)&amp;quot;
$ export AWS_AMI_DESC=&amp;quot;Hipster is a codename for rapidly moving \
&amp;gt; development branch of OpenIndiana and the development branch from \
&amp;gt; which major releases are made.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3-import-the-stream-optimized-vmdk-to-amazon-s3&#34;&gt;Step 3: Import the Stream Optimized VMDK to Amazon S3&lt;/h2&gt;

&lt;p&gt;Now that we have the EC2 API Tools installed, and the necessary
environment variables initialized, we can start the process of uploading
our disk image to Amazon, and converting that into an AMI.&lt;/p&gt;

&lt;p&gt;The first step of this proces is to upload our stream optimized VMDK
(generated in &lt;a href=&#34;#step-1-convert-raw-disk-image-into-stream-optimized-vmdk-format&#34;&gt;step 1&lt;/a&gt;) to our Amazon S3 bucket; to do this,
we&amp;rsquo;ll use the &lt;code&gt;ec2ivol&lt;/code&gt; utility:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./ec2-api-tools-1.7.5.1/bin/ec2ivol -o $AWS_ACCESS_KEY \
&amp;gt; -w $AWS_SECRET_KEY -b $AWS_S3_BUCKET -z $AWS_ZONE \
&amp;gt; -d &amp;quot;$AWS_AMI_DESCRIPTION&amp;quot; -f vmdk ami-template.vmdk
Requesting volume size: 64 GB
TaskType        IMPORTVOLUME    TaskId  import-vol-fghpxu3l     ExpirationTime  2017-02-13T20:39:29Z    Status  active  StatusMessage   Pending
DISKIMAGE       DiskImageFormat VMDK    DiskImageSize   1070948352      VolumeSize      64      AvailabilityZone        us-west-1a      ApproximateBytesConverted       0
Creating new manifest at ivol-openindiana-hipster/0af50aa3-ee80-492d-96a5-bab6ab63bda4/ami-template.vmdkmanifest.xml
Uploading the manifest file
Uploading 1070948352 bytes across 103 parts
----------------------------------------------------------------------------------------------------
   Upload progress              Estimated time      Estimated speed
 / 100% [====================&amp;gt;]                     56.056 MBps
********************* All 1070948352 Bytes uploaded in 19s  *********************
Done uploading.
Average speed was 56.053 MBps
The disk image for import-vol-fghpxu3l has been uploaded to Amazon S3
where it is being converted into an EBS volume.  You may monitor the
progress of this task by running ec2-describe-conversion-tasks.  When
the task is completed, you may use ec2-delete-disk-image to remove the
image from S3.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then use the &lt;code&gt;TaskId&lt;/code&gt; value provided by the output of &lt;code&gt;ec2ivol&lt;/code&gt;
(i.e. &lt;code&gt;import-vol-fghpxu3l&lt;/code&gt; in this instance), and feed that into
&lt;code&gt;ec2dct&lt;/code&gt; to retreive the status for the upload&amp;rsquo;s conversion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./ec2-api-tools-1.7.5.1/bin/ec2dct import-vol-fghpxu3l
TaskType        IMPORTVOLUME    TaskId  import-vol-fghpxu3l     ExpirationTime  2017-02-13T20:39:29Z    Status  completed
DISKIMAGE       DiskImageFormat VMDK    DiskImageSize   1070948352      VolumeId        vol-041fd3aa8c9435e7d   VolumeSize      64      AvailabilityZone        us-west-1a      ApproximateBytesConverted       1070945104
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As can be seen from the &lt;code&gt;Status&lt;/code&gt; section, this conversion has &lt;code&gt;completed&lt;/code&gt;,
so we can move on to the next step. If the conversion hadn&amp;rsquo;t &lt;code&gt;completed&lt;/code&gt;
yet, e.g. it was still &lt;code&gt;active&lt;/code&gt;, then we would need to wait (and poll
using &lt;code&gt;ec2dct&lt;/code&gt;) until it &lt;code&gt;completed&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;step-4-create-a-snapshot-of-imported-volume&#34;&gt;Step 4: Create a Snapshot of Imported Volume&lt;/h2&gt;

&lt;p&gt;The next step that we need to perform, is to create a snapshot from the
volume that we just created. Looking at the prior output from &lt;code&gt;ec2dct&lt;/code&gt;,
we can see from the &lt;code&gt;VolumeId&lt;/code&gt; field, that a volume was generated and
it&amp;rsquo;s ID is &lt;code&gt;vol-041fd3aa8c9435e7d&lt;/code&gt;. We will use the &lt;code&gt;ec2addsnap&lt;/code&gt; utility
to create a snapshot of that volume:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./ec2-api-tools-1.7.5.1/bin/ec2addsnap vol-041fd3aa8c9435e7d
SNAPSHOT        snap-01b80a642ef5f1f51  vol-041fd3aa8c9435e7d   pending 2017-02-06T20:49:11+0000                239688353806    64              Not Encrypted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just like when importing the volume, we can check the status of the
snapshot generation process using &lt;code&gt;ec2dsnap&lt;/code&gt; (the parameter passed to it
comes from the the output of &lt;code&gt;ec2addsnap&lt;/code&gt; above):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ while true; do
&amp;gt; ./ec2-api-tools-1.7.5.1/bin/ec2dsnap snap-01b80a642ef5f1f51
&amp;gt; sleep 60
&amp;gt; done
SNAPSHOT        snap-01b80a642ef5f1f51  vol-041fd3aa8c9435e7d   pending 2017-02-06T20:49:11+0000        0%      239688353806    64              Not Encrypted
SNAPSHOT        snap-01b80a642ef5f1f51  vol-041fd3aa8c9435e7d   pending 2017-02-06T20:49:11+0000        0%      239688353806    64              Not Encrypted
SNAPSHOT        snap-01b80a642ef5f1f51  vol-041fd3aa8c9435e7d   completed       2017-02-06T20:49:11+0000        100%    239688353806    64              Not Encrypted
^C
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-register-ami-using-previously-created-volume-snapshot&#34;&gt;Step 5: Register AMI Using Previously Created Volume Snapshot&lt;/h2&gt;

&lt;p&gt;Finally, now that we have the volume snapshot generated, we can use this
to register our new custom AMI using &lt;code&gt;ec2reg&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./ec2-api-tools-1.7.5.1/bin/ec2reg -s snap-01b80a642ef5f1f51 \
&amp;gt; -d &amp;quot;$AWS_AMI_DESC&amp;quot; -n &amp;quot;$AWS_AMI_NAME&amp;quot; -a x86_64 \
&amp;gt; --root-device-name /dev/xvda --virtualization-type hvm
IMAGE   ami-5b065a3b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success! The new custom built AMI is &lt;code&gt;ami-5b065a3b&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It should now be possible to use this AMI to create new Amazon EC2
instances.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Custom Installation Media for OI Hipster</title>
      <link>https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2017-02-01-creating-custom-istallation-media-for-oi-hipster/</guid>
      <description>

&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;

&lt;p&gt;This post is a write up of my notes for creating custom installation
media for &lt;a href=&#34;https://www.openindiana.org/overview/the-hipster-branch/&#34;&gt;OpenIndiana Hipster&lt;/a&gt;, using a custom/patched
version of &lt;a href=&#34;https://illumos.org&#34;&gt;illumos&lt;/a&gt;. It assumes that OI Hipster has already
been installed on a machine (e.g. installed on a VM using their
&lt;a href=&#34;https://www.openindiana.org/download/&#34;&gt;provided&lt;/a&gt; installation media); and this server will be
used to build our custom version of illumos, as well as the custom OI
installation media. The goal of this exercise is to create a &amp;ldquo;Live DVD&amp;rdquo;
that can be used to install our custom version of illumos.&lt;/p&gt;

&lt;h2 id=&#34;shoutouts&#34;&gt;Shoutouts&lt;/h2&gt;

&lt;p&gt;Before I get started, I want to thank &lt;code&gt;alp&lt;/code&gt; and &lt;code&gt;tsoome&lt;/code&gt; from the
&lt;code&gt;#oi-dev&lt;/code&gt; IRC channel on freenode for answering my various questions as
I was learning how to do this; their help definitely saved me hours of
frustration. Thanks again :)&lt;/p&gt;

&lt;h2 id=&#34;step-1-fetch-illumos&#34;&gt;Step 1: Fetch illumos&lt;/h2&gt;

&lt;p&gt;The illumos repository &lt;a href=&#34;https://github.com/illumos/illumos-gate&#34;&gt;is on GitHub&lt;/a&gt;, so we&amp;rsquo;ll use this to
obtain our copy. First, we must install &lt;code&gt;git&lt;/code&gt; on the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pkg install git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then we can use &lt;code&gt;git clone&lt;/code&gt; to fetch the code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/illumos/illumos-gate.git
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-2-prep-for-building-illumos&#34;&gt;Step 2: Prep for Building illumos&lt;/h2&gt;

&lt;p&gt;Now we can move on to compiling the illumos codebase; we&amp;rsquo;ll use the
&lt;a href=&#34;https://wiki.illumos.org/display/illumos/How+To+Build+illumos&#34;&gt;documented process&lt;/a&gt; as a guide. We need to
download and install all the required build tools (e.g. compilers, etc),
which can can be easily done using the &lt;code&gt;build-essentials&lt;/code&gt; meta-package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pkg install build-essential
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-1-download-and-unpack-closed-binaries&#34;&gt;Step 2.1: Download and Unpack Closed Binaries&lt;/h3&gt;

&lt;p&gt;After that completes, we have to download and unpack the &amp;ldquo;closed
binaries&amp;rdquo;; these are closed source components from Sun/Oracle that are
required to build illumos (and have not yet been replaced):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd illumos-gate/
$ wget -c \
    https://download.joyent.com/pub/build/illumos/on-closed-bins.i386.tar.bz2 \
    https://download.joyent.com/pub/build/illumos/on-closed-bins-nd.i386.tar.bz2
$ tar xjvpf on-closed-bins.i386.tar.bz2
$ tar xjvpf on-closed-bins-nd.i386.tar.bz2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-2-2-configure-environment-file-illumos-sh&#34;&gt;Step 2.2: Configure Environment File &amp;ndash; illumos.sh&lt;/h3&gt;

&lt;p&gt;The illumos build system is configured using an &amp;ldquo;environment&amp;rdquo; file. This
file contains various environment variables that the build uses to
determine what actions it will take; for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Should it run lint checks?&lt;/li&gt;
&lt;li&gt;Should it perform a &amp;ldquo;debug&amp;rdquo; or &amp;ldquo;non-debug&amp;rdquo; build? Or both?&lt;/li&gt;
&lt;li&gt;etc&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our purposes, we&amp;rsquo;ll leave all of the default values, and only make
the minimal modifications necessary:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp usr/src/tools/env/illumos.sh .
$ PKGVERS_BRANCH=$(pkg info  -r pkg://openindiana.org/SUNWcs | \
&amp;gt;                  awk &#39;$1 == &amp;quot;Branch:&amp;quot; {print $2}&#39;)
$ echo &amp;quot;export PKGVERS_BRANCH=&#39;$PKGVERS_BRANCH&#39;&amp;quot; &amp;gt;&amp;gt; illumos.sh
$ echo &amp;quot;export ONNV_BUILDNUM=&#39;$PKGVERS_BRANCH&#39;&amp;quot; &amp;gt;&amp;gt; illumos.sh
$ echo &amp;quot;export PERL_VERSION=&#39;5.22&#39;&amp;quot; &amp;gt;&amp;gt; illumos.sh
$ echo &amp;quot;export PERL_PKGVERS=&#39;-522&#39;&amp;quot; &amp;gt;&amp;gt; illumos.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3-build-illumos&#34;&gt;Step 3: Build illumos&lt;/h2&gt;

&lt;p&gt;Now we&amp;rsquo;re ready to start the build. This is expected to take a couple
hours to complete, so I&amp;rsquo;d recommend using a program such as &lt;code&gt;screen&lt;/code&gt; or
&lt;code&gt;tmux&lt;/code&gt;. That way, if the connection with the server performing the build
were to fail, the build process will remain running. For this guide I&amp;rsquo;ll
use &lt;code&gt;screen&lt;/code&gt;, since it&amp;rsquo;s already installed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ screen -S illumos-build
$ cp usr/src/tools/scripts/nightly.sh .
$ chmod +x nightly.sh
$ time ./nightly.sh illumos.sh

real    123m31.031s
user    178m14.443s
sys     42m54.293s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-4-prep-for-building-the-installation-media&#34;&gt;Step 4: Prep for Building the Installation Media&lt;/h2&gt;

&lt;p&gt;In order to build the installation media, we need to use the
&lt;code&gt;distro_const&lt;/code&gt; command, which is provided by the
&lt;code&gt;distribution-constructor&lt;/code&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pkg install distribution-constructor
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use &lt;code&gt;distro_const&lt;/code&gt;, we must pass it a &amp;ldquo;manifest&amp;rdquo; file, which is
nothing more than a specially formatted XML file. We&amp;rsquo;ll start with a
manifest template provided by the package, and then tweak the template
to suite our needs.&lt;/p&gt;

&lt;h3 id=&#34;step-4-1-copy-distro-const-manifest-template&#34;&gt;Step 4.1: Copy distro_const Manifest Template&lt;/h3&gt;

&lt;p&gt;First, we need to copy the template so we can make modifications to it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd ~
$ cp /usr/share/distro_const/text_install/text_mode_x86.xml .
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-4-1-replace-dev-ips-repository-with-hipster&#34;&gt;Step 4.1: Replace &amp;ldquo;dev&amp;rdquo; IPS Repository with &amp;ldquo;hipster&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;The first modification we need to make, is to point to the &amp;ldquo;hipster&amp;rdquo;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Image_Packaging_System&#34;&gt;IPS&lt;/a&gt; repository as opposed to the default &amp;ldquo;dev&amp;rdquo; repository. To do
this, we need to locate the &lt;code&gt;pkg_repo_default_authority&lt;/code&gt; and
&lt;code&gt;post_install_repo_default_authority&lt;/code&gt; sections, and replace
&lt;code&gt;http://pkg.openindiana.org/dev&lt;/code&gt; with
&lt;code&gt;http://pkg.openindiana.org/hipster&lt;/code&gt;. Here&amp;rsquo;s &lt;code&gt;diff&lt;/code&gt; of this change:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ diff -u text_mode_x86.xml text_mode_x86.xml.orig
--- text_mode_x86.xml.orig      Thu Feb  2 02:09:32 2017
+++ text_mode_x86.xml           Thu Feb  2 02:10:33 2017
@@ -192,7 +192,7 @@
                --&amp;gt;
                &amp;lt;pkg_repo_default_authority&amp;gt;
                        &amp;lt;main
-                               url=&amp;quot;http://pkg.openindiana.org/dev&amp;quot;
+                               url=&amp;quot;http://pkg.openindiana.org/hipster&amp;quot;
                                authname=&amp;quot;openindiana.org&amp;quot;/&amp;gt;
                        &amp;lt;!--
                             If you want to use one or more  mirrors that are
@@ -229,7 +229,7 @@
                --&amp;gt;
                &amp;lt;post_install_repo_default_authority&amp;gt;
                        &amp;lt;main
-                               url=&amp;quot;http://pkg.openindiana.org/dev&amp;quot;
+                               url=&amp;quot;http://pkg.openindiana.org/hipster&amp;quot;
                                authname=&amp;quot;openindiana.org&amp;quot;/&amp;gt;
                        &amp;lt;!--
                             Uncomment before using.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-4-2-point-to-ips-repository-from-custom-illumos-build&#34;&gt;Step 4.2: Point to IPS Repository from Custom illumos Build&lt;/h3&gt;

&lt;p&gt;As part of illumos build in &lt;a href=&#34;#step-3-build-illumos&#34;&gt;step 3&lt;/a&gt;, an IPS
repository was generated under the &lt;code&gt;packages/i386/nightly/repo.redist&lt;/code&gt;
directory. Since my illumos codebase is stored in the directory,
&lt;code&gt;/export/home/ps/illumos-gate&lt;/code&gt;, the full path to this IPS repository is
&lt;code&gt;/export/home/ps/illumos-gate/packages/i386/nightly/repo.redist&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In order for the installation media to use my custom built version of
illumos, the manifest needs to be modified to point to this IPS
repository, &lt;strong&gt;in addition to&lt;/strong&gt; the default OI &amp;ldquo;hipster&amp;rdquo; repository. It&amp;rsquo;s
important to note, that we&amp;rsquo;re not replacing the &amp;ldquo;hipster&amp;rdquo; repository
with the custom illumos build&amp;rsquo;s repository; rather, we want to use the
build&amp;rsquo;s repository as the &amp;ldquo;main&amp;rdquo; repository, and the &amp;ldquo;hipster&amp;rdquo;
repository as an &amp;ldquo;additional&amp;rdquo; repository.&lt;/p&gt;

&lt;p&gt;To do this, we&amp;rsquo;ll modify the &lt;code&gt;url&lt;/code&gt; specified in the
&lt;code&gt;pkg_repo_default_authority&lt;/code&gt; section to point to the illumos build&amp;rsquo;s
repository, using a &lt;code&gt;file://&lt;/code&gt; based URL, and we&amp;rsquo;ll modify the &lt;code&gt;url&lt;/code&gt; of
the &lt;code&gt;pkg_repo_addl_authority&lt;/code&gt; section to point to the original &amp;ldquo;hipster&amp;rdquo;
IPS repository. Here&amp;rsquo;s a diff of these changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ diff -u text_mode_x86.xml.orig text_mode_x86.xml
--- text_mode_x86.xml.orig      Thu Feb  2 02:18:19 2017
+++ text_mode_x86.xml           Thu Feb  2 02:19:45 2017
@@ -192,8 +192,8 @@
                --&amp;gt;
                &amp;lt;pkg_repo_default_authority&amp;gt;
                        &amp;lt;main
-                               url=&amp;quot;http://pkg.openindiana.org/hipster&amp;quot;
-                               authname=&amp;quot;openindiana.org&amp;quot;/&amp;gt;
+                               url=&amp;quot;file:///export/home/ps/illumos-gate/packages/i386/nightly/repo.redist&amp;quot;
+                               authname=&amp;quot;on-nightly&amp;quot;/&amp;gt;
                        &amp;lt;!--
                             If you want to use one or more  mirrors that are
                             setup for the authority, specify the urls here.
@@ -210,15 +210,12 @@
                     If you want to use one or more  mirrors that are
                     setup for the authority, specify the urls here.
                --&amp;gt;
-               &amp;lt;!--
-                    Uncomment before using.
                &amp;lt;pkg_repo_addl_authority&amp;gt;
                        &amp;lt;main
-                               url=&amp;quot;&amp;quot;
-                               authname=&amp;quot;&amp;quot;/&amp;gt;
+                               url=&amp;quot;http://pkg.openindiana.org/hipster&amp;quot;
+                               authname=&amp;quot;openindiana.org&amp;quot;/&amp;gt;
                        &amp;lt;mirror url=&amp;quot;&amp;quot; /&amp;gt;
                &amp;lt;/pkg_repo_addl_authority&amp;gt;
-               --&amp;gt;
                &amp;lt;!--
                     The default preferred authority to be used by the system
                     after it has been installed.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-4-3-use-oi-repository-for-ssh-and-ftp-packages&#34;&gt;Step 4.3: Use OI Repository for &amp;ldquo;ssh&amp;rdquo; and &amp;ldquo;ftp&amp;rdquo; Packages&lt;/h3&gt;

&lt;p&gt;I don&amp;rsquo;t fully understand why this step is necessary, but we need to
modify the manifest to force the use of the OpenIndiana&amp;rsquo;s IPS repository
when fetching the following packages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pkg:/network/ssh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/ftp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/ssh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is done by adding the package URL for these packages to the
&lt;code&gt;packages&lt;/code&gt; section of the manifest; the following &lt;code&gt;diff&lt;/code&gt; does this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ diff -u text_mode_x86.xml.orig text_mode_x86.xml
--- text_mode_x86.xml.orig      Thu Feb  2 03:38:37 2017
+++ text_mode_x86.xml           Thu Feb  2 03:56:46 2017
@@ -266,6 +266,9 @@
                        &amp;lt;pkg name=&amp;quot;pkg:/server_install&amp;quot;/&amp;gt;
                        &amp;lt;pkg name=&amp;quot;pkg:/system/install/text-install&amp;quot;/&amp;gt;
                        &amp;lt;pkg name=&amp;quot;pkg:/system/install/media/internal&amp;quot;/&amp;gt;
+                       &amp;lt;pkg name=&amp;quot;pkg://openindiana.org/network/ssh&amp;quot;/&amp;gt;
+                       &amp;lt;pkg name=&amp;quot;pkg://openindiana.org/service/network/ftp&amp;quot;/&amp;gt;
+                       &amp;lt;pkg name=&amp;quot;pkg://openindiana.org/service/network/ssh&amp;quot;/&amp;gt;
                &amp;lt;/packages&amp;gt;
 &amp;lt;!--
      Items below this line are rarely configured
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-4-4-remove-certain-packages-from-illumos-build-repository&#34;&gt;Step 4.4: Remove Certain Packages from illumos Build Repository&lt;/h3&gt;

&lt;p&gt;I understand even less about why this step is necessary, but we need to
remove certain packages from the IPS repository generated by the
previous illumos build (the build from &lt;a href=&#34;#step-3-build-illumos&#34;&gt;step 3&lt;/a&gt;),
otherwise the &lt;a href=&#34;#step-5-building-the-installation-media&#34;&gt;next step&lt;/a&gt; will
fail to build the installation media. The packages we need to remove
are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pkg:/network/ssh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/ssh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/ftp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/ssh-common&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pkg:/service/network/smtp/sendmail&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can use &lt;code&gt;pkgrepo&lt;/code&gt; to remove them like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ REPO=&amp;quot;/export/home/ps/illumos-gate/packages/i386/nightly/repo.redist&amp;quot;
$ for pkg in &amp;quot;pkg:/network/ssh&amp;quot; \
&amp;gt;            &amp;quot;pkg:/service/network/ssh&amp;quot; \
&amp;gt;            &amp;quot;pkg:/service/network/ftp&amp;quot; \
&amp;gt;            &amp;quot;pkg:/service/network/ssh-common&amp;quot; \
&amp;gt;            &amp;quot;pkg:/service/network/smtp/sendmail&amp;quot;; do
&amp;gt;     pkgrepo -s &amp;quot;$REPO&amp;quot; remove &amp;quot;$pkg&amp;quot;
&amp;gt; done
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-5-building-the-installation-media&#34;&gt;Step 5: Building the Installation Media&lt;/h2&gt;

&lt;p&gt;Now, with all of the above steps completed, we&amp;rsquo;re ready to use
&lt;code&gt;distro_const&lt;/code&gt; to build the installation media files; this part is
simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo distro_const build text_mode_x86.xml
... &amp;lt;snip&amp;gt; ...
Build completed Thu Feb  2 05:26:06 2017
Build is successful.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that completes, the installation media should be found in
&lt;code&gt;/rpool/dc/media&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -lh /rpool/dc/media/
total 2793956
-rw-r--r--   1 root     root        657M Feb  2 05:25 OpenIndiana_Text_X86.iso
-r--r--r--   1 root     root        793M Feb  2 05:25 OpenIndiana_Text_X86.usb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success!&lt;/p&gt;

&lt;p&gt;Those files can now be used to install our custom OpenIndiana Hipster
distribution onto any machine of our choosing (e.g. a VM, bare metal,
etc), and the installed operating system will be our custom version of
illumos, plus all additional packages packages that constitute OI
Hipster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Artificial Disk Latency Using zinject</title>
      <link>https://www.prakashsurya.com/post/2015-12-07-openzfs-artificial-disk-latency-using-zinject/</link>
      <pubDate>Mon, 07 Dec 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-12-07-openzfs-artificial-disk-latency-using-zinject/</guid>
      <description>

&lt;p&gt;About a year ago I had the opportunity to work on a small extension to
the &lt;a href=&#34;http://www.open-zfs.org&#34;&gt;OpenZFS&lt;/a&gt; &lt;code&gt;zinject&lt;/code&gt; command with colleagues Matt Ahrens and
Frank Salzmann, during one of our &lt;a href=&#34;http://www.delphix.com&#34;&gt;Delphix&lt;/a&gt; engineering wide
hackathon events. Now that it&amp;rsquo;s in the &lt;a href=&#34;https://github.com/openzfs/openzfs/pull/39&#34;&gt;process of landing&lt;/a&gt; in the
upstream &lt;a href=&#34;https://github.com/openzfs/openzfs&#34;&gt;OpenZFS repository&lt;/a&gt;, I wanted to take a minute to show
it off.&lt;/p&gt;

&lt;p&gt;To describe the new functionality, I&amp;rsquo;ll defer to the help message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ zinject -h
... &amp;lt;snip&amp;gt; ...

zinject -d device -D latency:lanes pool

        Add an artificial delay to IO requests on a particular
        device, such that the requests take a minimum of &#39;latency&#39;
        milliseconds to complete. Each delay has an associated
        number of &#39;lanes&#39; which defines the number of concurrent
        IO requests that can be processed.

        For example, with a single lane delay of 10 ms (-D 10:1),
        the device will only be able to service a single IO request
        at a time with each request taking 10 ms to complete. So,
        if only a single request is submitted every 10 ms, the
        average latency will be 10 ms; but if more than one request
        is submitted every 10 ms, the average latency will be more
        than 10 ms.

        Similarly, if a delay of 10 ms is specified to have two
        lanes (-D 10:2), then the device will be able to service
        two requests at a time, each with a minimum latency of
        10 ms. So, if two requests are submitted every 10 ms, then
        the average latency will be 10 ms; but if more than two
        requests are submitted every 10 ms, the average latency
        will be more than 10 ms.

        Also note, these delays are additive. So two invocations
        of &#39;-D 10:1&#39;, is roughly equivalent to a single invocation
        of &#39;-D 10:2&#39;. This also means, one can specify multiple
        lanes with differing target latencies. For example, an
        invocation of &#39;-D 10:1&#39; followed by &#39;-D 25:2&#39; will
        create 3 lanes on the device; one lane with a latency
        of 10 ms and two lanes with a 25 ms latency.

... &amp;lt;snip&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given that explanation, lets dive in and give this a test drive with a
few examples.&lt;/p&gt;

&lt;h2 id=&#34;setup-and-baseline&#34;&gt;Setup and Baseline&lt;/h2&gt;

&lt;p&gt;First, lets start with a basic example using a pool with a single disk:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool create tank c1t1d0
# zfs create tank/foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s do some IO to the pool and use &lt;code&gt;dtrace&lt;/code&gt; to examine the latency
of the requests to get a baseline expectation for the IO latency of
writes to this pool.&lt;/p&gt;

&lt;p&gt;Running in one terminal I have a simple, single threaded write workload
running in an infinite loop. This will continually copy some random data
to a file in the pool, remove that file, and repeat.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dd if=/dev/urandom of=/tmp/urandom-cache bs=1M count=1024 &amp;amp;&amp;gt;/dev/null
# while true; do
&amp;gt; cp /tmp/urandom-cache /tank/foo/urandom-cache
&amp;gt; sync
&amp;gt; rm /tank/foo/urandom-cache
&amp;gt; sync
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, lets look at the latency of the write requests issued to disk by
the filesystem, using &lt;code&gt;dtrace&lt;/code&gt; (see &lt;a href=&#34;#zio-latency&#34;&gt;here&lt;/a&gt; for the source
of this &lt;code&gt;dtrace&lt;/code&gt; script):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs zio-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

           value  ------------- Distribution ------------- count
             128 |                                         0
             256 |                                         248
             512 |@@@@@                                    6204
            1024 |@@@@@@@@@@@@@@@@@@@@@@                   26218
            2048 |@@@@@@@@@@@                              13070
            4096 |@@                                       2474
            8192 |                                         416
           16384 |                                         56
           32768 |                                         7
           65536 |                                         0

avg latency (us)   stddev (us)   iops   throughput (KB/s)
2025               1609          811    99623
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is showing us a histogram of the latency of all writes that were
submitted and completed to our pool during that specific interval.
Additionally, it&amp;rsquo;s showing us an average and standard deviation of the
latency over all the requests, number of request issued and completed
per second, and the throughput achieved by these writes.&lt;/p&gt;

&lt;p&gt;This sets the baseline performance that we expect to get out of this
storage configuration; most requests take less than 8 ms to be serviced,
with the average being about 2ms with a standard deviation of roughly
1.5ms.&lt;/p&gt;

&lt;h2 id=&#34;first-experiment-single-disk-single-lane&#34;&gt;First Experiment: Single Disk, Single Lane&lt;/h2&gt;

&lt;p&gt;Now that we have some baseline numbers to compare against, lets use the
new &lt;code&gt;zinject&lt;/code&gt; functionality to slow down the requests. Let&amp;rsquo;s say we want
to delay all requests such that they take a minimum of 10ms to complete.&lt;/p&gt;

&lt;p&gt;Remembering that we created this pool in the previous section using a
single disk named &lt;code&gt;c1t1d0&lt;/code&gt;, we can set the minimum latency of this disk
to be our desired 10ms:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zinject -d c1t1d0 -D 10:1 tank
Added handler 1 with the following properties:
  pool: tank
  vdev: c138fa4fbf3d8f2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And to double check that this is the only delay registered on our pool,
we can run &lt;code&gt;zinject&lt;/code&gt; without any parameters to dump a list of all
registered delays:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zinject
 ID  POOL             DELAY (ms)       LANES            GUID
---  ---------------  ---------------  ---------------  ----------------
  1  tank             10               1                c138fa4fbf3d8f2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;ldquo;lanes&amp;rdquo; column describes the number of requests that can be serviced
simultaneous by each registered delay; thus this delay specifies that
only a single request can be serviced at a time, and each request will
take a minimum of 10ms to complete.&lt;/p&gt;

&lt;p&gt;Now, with that delay in place, and the same write workload from the
previous section still running, we can again use &lt;code&gt;dtrace&lt;/code&gt; to inspect the
write request latencies, IOPs, and throughput values. Given this delay,
we should expect to see no request complete sooner than 10ms, and as a
result, the IOPs and throughput should take a significant hit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs zio-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

           value  ------------- Distribution ------------- count
            4096 |                                         0
            8192 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@             4177
           16384 |@@@@@@                                   894
           32768 |@@@@@                                    807
           65536 |                                         1
          131072 |                                         0

avg latency (us)   stddev (us)   iops   throughput (KB/s)
17080              12448         97     12518
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! This matches our expectation.&lt;/p&gt;

&lt;p&gt;Since we&amp;rsquo;re using a logarithmic histogram, we can&amp;rsquo;t actually
differentiate between requests in the 8-16ms range, but judging by the
fact that there aren&amp;rsquo;t any request in the 0-8ms range, it&amp;rsquo;s safe to
assume all the request that landed in the 8-16ms range, actually took
10-16ms to complete (as they should have).&lt;/p&gt;

&lt;h2 id=&#34;wait-what-about-the-average-latency&#34;&gt;Wait, What About the Average Latency?&lt;/h2&gt;

&lt;p&gt;At this point, you might be looking at the average latency output from
the previous experiment, and be wondering why it&amp;rsquo;s so much larger than
our target latency of 10ms. Without the delay, most requests would
complete in under 8ms, so why isn&amp;rsquo;t the average latency with the delay
closer to the desired 10ms?&lt;/p&gt;

&lt;p&gt;Remember that when we created the delay we specified it to have a single
&amp;ldquo;lane&amp;rdquo;. What this means is only a single request can be serviced at a
time. If more than a single write is submitted every 10ms to the
underlying disk, then each new write request will get backed up by any
existing requests.&lt;/p&gt;

&lt;p&gt;In other words, each lane represents a &lt;a href=&#34;https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)&#34;&gt;FIFO&lt;/a&gt; queue. When a new
IO request is submitted, the FIFO queue with the shortest wait time will
be selected (e.g. if there are multiple lanes available to choose from),
and then the request is placed in the queue and waits its turn until
it can be processed and eventually completed. Thus, in our example
above, if a request is submitted while another request is currently
being processed and still has 7ms to finish, this new request&amp;rsquo;s latency
will be a minimum of 17ms; 7ms spent waiting for the prior request to be
processed, and 10ms spent being processed itself. Due to the way OpenZFS
issues writes, multiple requests will certainly be submitted
simultaneously, so it&amp;rsquo;s common for this situation to occur and explains
the average latency in the previous experiment.&lt;/p&gt;

&lt;h2 id=&#34;second-experiment-single-disk-multiple-lanes&#34;&gt;Second Experiment: Single Disk, Multiple Lanes&lt;/h2&gt;

&lt;p&gt;If the previously described behavior isn&amp;rsquo;t actually what is desired, we
can simulate an implementation where requests pay no attention to prior
requests by adding a delay with enough lanes such that there will always
be an open lane to accept new requests. As an example, let&amp;rsquo;s create a
delay with the same 10ms minimum latency as before, but instead of 1
lane, we&amp;rsquo;ll create 16 lanes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zinject -c all
removed all registered handlers

# zinject -d c1t1d0 -D 10:16 tank
Added handler 2 with the following properties:
  pool: tank
  vdev: c138fa4fbf3d8f2

# zinject
 ID  POOL             DELAY (ms)       LANES            GUID
---  ---------------  ---------------  ---------------  ----------------
  2  tank             10               16               c138fa4fbf3d8f2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming the magic number of 16 is sufficient to prevent new requests
from piling up behind older, in-progress requests, we should now see the
average latency trend much closer to the specified 10ms:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs zio-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

           value  ------------- Distribution ------------- count
            4096 |                                         0
            8192 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 8836
           16384 |                                         39
           32768 |                                         11
           65536 |                                         1
          131072 |                                         1
          262144 |                                         0

avg latency (us)   stddev (us)   iops   throughput (KB/s)
10261              3022          148    18463
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perfect! As expected, the average latency is now much closer to our
desired 10ms. Unfortunately though, there were 52 requests that actually
took longer than 16ms to be completed by the underlying storage (2 of
which took longer than 64ms!), which pulls the average slightly above
the specified target of 10ms.&lt;/p&gt;

&lt;h2 id=&#34;third-experiment-setup-and-baseline&#34;&gt;Third Experiment: Setup and Baseline&lt;/h2&gt;

&lt;p&gt;Another interesting example to demonstrate is simulating a pool with
disks of different latency characteristics. This can happen for a
variety of reasons, none of which I&amp;rsquo;ll touch on in this article. To
simulate this case, we&amp;rsquo;ll need a pool with two disks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool create tank c1t1d0 c1t2d0
# zfs create tank/foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll use a slightly modified version of the write workload used in
previous sections; instead of a single thread, we&amp;rsquo;ll use 8 threads to
increase the amount of data being written and increase the amount of
simultaneous write requests occurring:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# while true; do
&amp;gt; PIDS=&amp;quot;&amp;quot;
&amp;gt; for i in $(seq 1 8); do
&amp;gt;     cp /tmp/urandom-cache /tank/foo/urandom-cache-$i &amp;amp;&amp;gt;/dev/null &amp;amp;
&amp;gt;     PIDS=&amp;quot;$PIDS $!&amp;quot;
&amp;gt; done
&amp;gt; sync
&amp;gt; for pid in $PIDS; do
&amp;gt;     wait $pid
&amp;gt; done
&amp;gt;
&amp;gt; PIDS=&amp;quot;&amp;quot;
&amp;gt; for i in $(seq 1 8); do
&amp;gt;     rm /tank/foo/urandom-cache-$i &amp;amp;&amp;gt;/dev/null &amp;amp;
&amp;gt;     PIDS=&amp;quot;$PIDS $!&amp;quot;
&amp;gt; done
&amp;gt; sync
&amp;gt; for pid in $PIDS; do
&amp;gt;     wait $pid
&amp;gt; done
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And with a slight variation to the previous &lt;code&gt;dtrace&lt;/code&gt; script, we can show
the same statistics as the prior sections, but individually for each
vdev in the pool (see &lt;a href=&#34;#vdev-latency&#34;&gt;here&lt;/a&gt; for the source of this
&lt;code&gt;dtrace&lt;/code&gt; script):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs vdev-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

f1d32d9b9514d7a3
           value  ------------- Distribution ------------- count
             128 |                                         0
             256 |                                         43
             512 |                                         665
            1024 |@@                                       3231
            2048 |@                                        1848
            4096 |@@@@@@@@@@@@@                            17886
            8192 |@@@@@@@@@@@@@@@@@@@@                     26511
           16384 |@@@                                      3493
           32768 |                                         310
           65536 |                                         4
          131072 |                                         2
          262144 |                                         0

f4eb4119d7170682
           value  ------------- Distribution ------------- count
             128 |                                         0
             256 |                                         41
             512 |                                         627
            1024 |@@                                       3207
            2048 |@                                        1865
            4096 |@@@@@@@@@@@@@                            17809
            8192 |@@@@@@@@@@@@@@@@@@@@                     26416
           16384 |@@@                                      3570
           32768 |                                         312
           65536 |                                         9
          131072 |                                         1
          262144 |                                         0

GUID               avg latency (us)   stddev (us)   iops   throughput (KB/s)
f1d32d9b9514d7a3   9475               5292          899    111305
f4eb4119d7170682   9496               5298          897    111116
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, without any delays injected with &lt;code&gt;zinject&lt;/code&gt; each vdev
performs nearly identically.&lt;/p&gt;

&lt;h2 id=&#34;third-experiment-multiple-disks-different-latencies&#34;&gt;Third Experiment: Multiple Disks, Different Latencies&lt;/h2&gt;

&lt;p&gt;Now that we&amp;rsquo;ve verified the vdevs to be performing similarly in the
absence of any artificial delays, we can simulate a case where one disk
performs half as slow as the other by specifying two different &lt;code&gt;zinject&lt;/code&gt;
delays for the different disks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zinject -d c1t1d0 -D 16:16 tank
Added handler 3 with the following properties:
  pool: tank
  vdev: f4eb4119d7170682

# zinject -d c1t2d0 -D 32:16 tank
Added handler 4 with the following properties:
  pool: tank
  vdev: f1d32d9b9514d7a3

# zinject
 ID  POOL             DELAY (ms)       LANES            GUID
---  ---------------  ---------------  ---------------  ----------------
  3  tank             16               16               f4eb4119d7170682
  4  tank             32               16               f1d32d9b9514d7a3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this scenario we&amp;rsquo;d expect the total throughput to take a significant
hit, resulting from the large delays, but we&amp;rsquo;d also expect the faster
disk to service twice the number of write requests in any given
interval. Thus, we should see the faster disk demonstrate roughly twice
the throughput when inspected with &lt;code&gt;dtrace&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs vdev-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

f4eb4119d7170682
           value  ------------- Distribution ------------- count
            4096 |                                         0
            8192 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  29964
           16384 |@                                        1012
           32768 |                                         117
           65536 |                                         4
          131072 |                                         0

f1d32d9b9514d7a3
           value  ------------- Distribution ------------- count
            4096 |                                         0
            8192 |                                         26
           16384 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ 16104
           32768 |                                         162
           65536 |                                         5
          131072 |                                         0

GUID               avg latency (us)   stddev (us)   iops   throughput (KB/s)
f4eb4119d7170682   16391              2128          518    64651
f1d32d9b9514d7a3   32310              1680          271    33630
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is exactly what I&amp;rsquo;ve measure above! Disk &lt;code&gt;f4eb4119d7170682&lt;/code&gt; was
given the 16ms delay and was measured as having nearly double the IOPs
and nearly double the throughput during the measured interval, success!&lt;/p&gt;

&lt;h2 id=&#34;dtrace-scripts&#34;&gt;DTrace Scripts&lt;/h2&gt;

&lt;p&gt;In the spirit of full disclosure, here&amp;rsquo;s the source for the &lt;code&gt;dtrace&lt;/code&gt;
scripts that I used in the previous sections.&lt;/p&gt;

&lt;h3 id=&#34;zio-latency&#34;&gt;ZIO Latency&lt;/h3&gt;

&lt;p&gt;Here&amp;rsquo;s the script used in the first two experiments which does not break
down the statistics based on vdev:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat zio-rw-latency.d
BEGIN
{
        start = timestamp;
}

fbt:zfs:vdev_disk_io_start:entry
/this-&amp;gt;zio = args[0],
 this-&amp;gt;zio-&amp;gt;io_type == ($$2 == &amp;quot;read&amp;quot; ? 1 : 2) &amp;amp;&amp;amp;
 stringof(this-&amp;gt;zio-&amp;gt;io_spa-&amp;gt;spa_name) == $$1/
{
        ts[this-&amp;gt;zio] = timestamp;
}

fbt:zfs:zio_interrupt:entry
/this-&amp;gt;zio = args[0], ts[this-&amp;gt;zio]/
{
        delta = (timestamp - ts[this-&amp;gt;zio])/1000;

        @i = count();
        @a = avg(delta);
        @s = stddev(delta);
        @l = quantize(delta);
        @t = sum(this-&amp;gt;zio-&amp;gt;io_size);

        ts[this-&amp;gt;zio] = 0;
}

END
{
        printf(&amp;quot;\nhistogram of latencies (us):&amp;quot;);
        printa(@l);

        normalize(@i, (timestamp - start) / 1000000000);
        normalize(@t, (timestamp - start) / 1000000000 * 1024);

        printf(&amp;quot;%-16s   %-11s   %-4s   %-17s\n&amp;quot;,
            &amp;quot;avg latency (us)&amp;quot;, &amp;quot;stddev (us)&amp;quot;, &amp;quot;iops&amp;quot;, &amp;quot;throughput (KB/s)&amp;quot;);
        printa(&amp;quot;%@-16u   %@-11u   %@-4u   %@-17u\n&amp;quot;, @a, @s, @i, @t);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This script is intended to take two parameters: &lt;code&gt;$$1&lt;/code&gt; is the pool name,
i.e. &lt;code&gt;tank&lt;/code&gt; in our examples, and &lt;code&gt;$$2&lt;/code&gt; is either &lt;code&gt;&amp;quot;read&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;write&amp;quot;&lt;/code&gt;,
we only used &lt;code&gt;&amp;quot;write&amp;quot;&lt;/code&gt; in the examples in this article.&lt;/p&gt;

&lt;h3 id=&#34;vdev-latency&#34;&gt;VDEV Latency&lt;/h3&gt;

&lt;p&gt;Here&amp;rsquo;s the script used in the last experiment, which differentiates the
statistics based on the individual vdevs in the pool:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat vdev-rw-latency.d
BEGIN
{
        start = timestamp;
}

fbt:zfs:vdev_disk_io_start:entry
/this-&amp;gt;zio = args[0],
 this-&amp;gt;zio-&amp;gt;io_type == ($$2 == &amp;quot;read&amp;quot; ? 1 : 2) &amp;amp;&amp;amp;
 stringof(this-&amp;gt;zio-&amp;gt;io_spa-&amp;gt;spa_name) == $$1/
{
        ts[this-&amp;gt;zio] = timestamp;
}

fbt:zfs:zio_interrupt:entry
/this-&amp;gt;zio = args[0], ts[this-&amp;gt;zio]/
{
        delta = (timestamp - ts[this-&amp;gt;zio])/1000;

        @i[this-&amp;gt;zio-&amp;gt;io_vd-&amp;gt;vdev_guid] = count();
        @a[this-&amp;gt;zio-&amp;gt;io_vd-&amp;gt;vdev_guid] = avg(delta);
        @s[this-&amp;gt;zio-&amp;gt;io_vd-&amp;gt;vdev_guid] = stddev(delta);
        @l[this-&amp;gt;zio-&amp;gt;io_vd-&amp;gt;vdev_guid] = quantize(delta);
        @t[this-&amp;gt;zio-&amp;gt;io_vd-&amp;gt;vdev_guid] = sum(this-&amp;gt;zio-&amp;gt;io_size);

        ts[this-&amp;gt;zio] = 0;
}

END
{
        printf(&amp;quot;\nhistogram of latencies (us):\n\n&amp;quot;);
        printa(&amp;quot;%x %@u\n&amp;quot;, @l);

        normalize(@i, (timestamp - start) / 1000000000);
        normalize(@t, (timestamp - start) / 1000000000 * 1024);

        printf(&amp;quot;%-16s   %-16s   %-11s   %-4s   %-17s\n&amp;quot;, &amp;quot;GUID&amp;quot;,
            &amp;quot;avg latency (us)&amp;quot;, &amp;quot;stddev (us)&amp;quot;, &amp;quot;iops&amp;quot;, &amp;quot;throughput (KB/s)&amp;quot;);
        printa(&amp;quot;%-16x   %@-16u   %@-11u   %@-4u   %@-17u\n&amp;quot;, @a, @s, @i, @t);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This script takes the same &lt;code&gt;$$1&lt;/code&gt; and &lt;code&gt;$$2&lt;/code&gt; parameters as the previous
&lt;code&gt;zio-rw-latency.d&lt;/code&gt; script described above.&lt;/p&gt;

&lt;h3 id=&#34;why-not-use-dtrace-s-io-provider&#34;&gt;Why Not Use DTrace&amp;rsquo;s &amp;ldquo;io&amp;rdquo; Provider?&lt;/h3&gt;

&lt;p&gt;Inevitably there will be at least one curious reader that looks at the
above &lt;code&gt;dtrace&lt;/code&gt; scripts and wonders why I didn&amp;rsquo;t use the &lt;a href=&#34;http://dtrace.org/guide/chp-io.html&#34;&gt;stable &lt;code&gt;io&lt;/code&gt;
provider&lt;/a&gt; (i.e. &lt;code&gt;io:::start&lt;/code&gt; and &lt;code&gt;io:::done&lt;/code&gt;). This is intentional
because the delays injected using &lt;code&gt;zinject&lt;/code&gt; occur above the level of
the &lt;code&gt;io&lt;/code&gt; provider.&lt;/p&gt;

&lt;p&gt;Essentially, what&amp;rsquo;s happening is the underlying storage will complete
the request as fast as it can, and the &lt;code&gt;io&lt;/code&gt; provider will report on this
latency. After the storage has completed the request, it will return the
result back up to OpenZFS, and it is at this point that the request may
be delayed further if using this new &lt;code&gt;zinject&lt;/code&gt; feature. Once the result
is passed back to OpenZFS, the latency has already been recorded when
using the &lt;code&gt;io&lt;/code&gt; provider, so any additional latency introduced by
&lt;code&gt;zinject&lt;/code&gt; will go unnoticed.&lt;/p&gt;

&lt;p&gt;I can easily demonstrate this with the following example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# zpool create tank c1t1d0
# zfs create tank/foo

# while true; do
&amp;gt; cp /tmp/urandom-cache /tank/foo/urandom-cache
&amp;gt; sync
&amp;gt; rm /tank/foo/urandom-cache
&amp;gt; sync
&amp;gt; done &amp;amp;
[1] 9388

# zinject -d c1t1d0 -D 10:1 tank
Added handler 5 with the following properties:
  pool: tank
  vdev: e2503467ad6a2e62

# zinject
 ID  POOL             DELAY (ms)       LANES            GUID
---  ---------------  ---------------  ---------------  ----------------
  5  tank             10               1                e2503467ad6a2e62
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, we have a pool with a single disk and a delay of 10ms with a single
lane. Lets look at the write request latency on the system as reported
by the &lt;code&gt;io&lt;/code&gt; provider:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs io-rw-latency.d -c &#39;sleep 60&#39; write


           value  ------------- Distribution ------------- count
             128 |                                         0
             256 |                                         26
             512 |@@                                       266
            1024 |@@@@@@@@@@@@@@@@@@@@                     2208
            2048 |@@@@@@@@@@                               1079
            4096 |@@@@                                     461
            8192 |@@                                       206
           16384 |@                                        54
           32768 |                                         10
           65536 |                                         1
          131072 |                                         0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s clear that the latency reported here does not match the 10ms
minimum latency requested by the &lt;code&gt;zinject&lt;/code&gt; command; and to verify that
the delay is actually working correctly, let&amp;rsquo;s use the &lt;code&gt;dtrace&lt;/code&gt; script
from the prior sections (&lt;a href=&#34;#zio-latency&#34;&gt;this one&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# dtrace -qs zio-rw-latency.d -c &#39;sleep 60&#39; tank write

histogram of latencies (us):

           value  ------------- Distribution ------------- count
            4096 |                                         0
            8192 |@@@@@@@@@@@@@@@@@@                       2606
           16384 |@@@@@@                                   898
           32768 |@@@@@@@@@                                1308
           65536 |@@@@@@@                                  1095
          131072 |                                         0

avg latency (us)   stddev (us)   iops   throughput (KB/s)
33768              26629         98     12573
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clearly, the delay is in place, it&amp;rsquo;s just that the &lt;code&gt;io&lt;/code&gt; provider doesn&amp;rsquo;t
detect the additional latency introduced by &lt;code&gt;zinject&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And finally, here&amp;rsquo;s the source for the &lt;code&gt;dtrace&lt;/code&gt; script used above, which
uses the &lt;code&gt;io&lt;/code&gt; provider:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# cat io-rw-latency.d
io:::start
/args[0]-&amp;gt;b_flags &amp;amp; ($$1 == &amp;quot;read&amp;quot; ? B_READ : B_WRITE)/
{
        ts[args[0]-&amp;gt;b_edev, args[0]-&amp;gt;b_lblkno] = timestamp;
}

io:::done
/ts[args[0]-&amp;gt;b_edev, args[0]-&amp;gt;b_lblkno]/
{
        @ = quantize((timestamp -
            ts[args[0]-&amp;gt;b_edev, args[0]-&amp;gt;b_lblkno]) / 1000);
        ts[args[0]-&amp;gt;b_edev, args[0]-&amp;gt;b_lblkno] = 0;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS: Reducing ARC Lock Contention</title>
      <link>https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/</link>
      <pubDate>Mon, 23 Mar 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-03-23-openzfs-reducing-arc-lock-contention/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;tl;dr;&lt;/strong&gt; Cached random read performance of 8K blocks was improved by
225% by reducing internal lock contention within the OpenZFS ARC on
illumos.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Locks are a pain. Even worse, is a single lock serializing all of the
page cache accesses for a filesystem. While that&amp;rsquo;s not &lt;em&gt;quite&lt;/em&gt; the
situation the OpenZFS ARC was in earlier this year, it was pretty close.&lt;/p&gt;

&lt;p&gt;For those unfamiliar, the OpenZFS file system is built atop its very own
page cache based on the paper by Nimrod Megiddo and Dharmendra S. Modha,
&amp;ldquo;ARC: A Self-Tuning, Low Overhead Replacement Cache&amp;rdquo;. While the paper
provides a very good basis from which actual code can be written to
implement its concepts, there&amp;rsquo;s a number of real world problems it fails
to address. For this article, we&amp;rsquo;ll be looking at one of these problems:
concurrency.&lt;/p&gt;

&lt;h2 id=&#34;real-world-problems-with-the-arc&#34;&gt;Real World Problems with the ARC&lt;/h2&gt;

&lt;p&gt;The paper describes a global data structure from which all page accesses
are performed; the problem is, it doesn&amp;rsquo;t describe a mechanism to do
this safely in a highly concurrent environment. Since any modern file
system obviously needs to support multi-threaded concurrent operations,
locks had to be added to protect the global data used to manage and
implement the ARC. How might one do this?&lt;/p&gt;

&lt;p&gt;One way is to is to wrap all operations modifying the global structures
with a single mutually exclusive lock. This satisfies the requirement
that all accesses are safe to perform concurrently from many threads,
while also be fairly simple to understand and implement. To my surprise,
this is how the OpenZFS ARC was structured for nearly the past decade&lt;/p&gt;

&lt;h2 id=&#34;performance-before&#34;&gt;Performance: Before&lt;/h2&gt;

&lt;p&gt;As one might expect, the main drawback with this simplistic approach is
performance. While safe, on systems with many CPUs, this approach
causes terrible lock contention for multi-threaded cached workloads. To
highlight this performance issue, I used FIO to perform a cached random
read workload using 32 threads. Each thread would use its own unique
file, reading 8K blocks at random 8K aligned offsets from its file
[&lt;a href=&#34;#appendix-1&#34;&gt;Appendix 1&lt;/a&gt;]. The aggregate bandwidth was measured while
varying the number of virtual CPUs allocated to the virtual machine,
and the results are as follows:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;# of CPUS&lt;/th&gt;
&lt;th&gt;Bandwidth&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;840893 KB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1559.5 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3022.1 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;5104.9 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;3854.3 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;3036.2 MB/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;See [&lt;a href=&#34;#appendix-2&#34;&gt;Appendix 2&lt;/a&gt;] for more detailed performance metrics.&lt;/p&gt;

&lt;p&gt;As one can see, the performance scales terribly when increasing the
number of CPUs in the system. Not only does performance fail to increase
with more CPUs, it actually gets much worse with more CPUs! Clearly this
is a badsituation to be in;especially so,withlarge CPU count systems
becomingincreasinglymore common.&lt;/p&gt;

&lt;p&gt;Looking at a snippet of lockstat profiling data, it&amp;rsquo;s clear which locks
are to blame (the following was taken from a run on a system with 32
CPUs):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Count indv cuml rcnt     nsec Lock                   Caller
-------------------------------------------------------------------------------
4191777  47%  47% 0.00    67709 ARC_mfu+0x58           remove_reference+0x63
4103160  46%  92% 0.00    75366 ARC_mfu+0x58           add_reference+0x8d
   1877   0%  92% 0.00   436330 buf_hash_table+0x750   arc_buf_add_ref+0x6a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two locks causing the bottleneck are used to protect the ARC&amp;rsquo;s
internal lists. These lists contain all ARC buffers residing in the
cache that aren&amp;rsquo;t actively used by consumers of the ARC (i.e. buffers
with a reference count of zero). Every time a buffer is added or removed
from one of these lists, the list&amp;rsquo;s lock must be held. This means the
lists&amp;rsquo; lock must be acquired for all of the following operations: a read
from disk, an ARC cache hit, when evicting a buffer, when dirtying a
buffer, when all references to a buffer are released by consumers, etc.&lt;/p&gt;

&lt;h2 id=&#34;remove-the-lists&#34;&gt;Remove the Lists?&lt;/h2&gt;

&lt;p&gt;Before I dive into the solution that was taken, I should make it clear
why the lists are needed in the first place. These lists allow for
iterating over the buffers from the least recently accessed buffer to
the most recently accessed buffer, or vice versa. The ability to do this
iteration is critical to implementing the ARC as described by the paper.
It allows for the least recently accessed buffer to be selected in
constant-time during the eviction process [&lt;a href=&#34;#appendix-3&#34;&gt;Appendix 3&lt;/a&gt;].&lt;/p&gt;

&lt;h2 id=&#34;solution-use-more-locks&#34;&gt;Solution: Use More Locks!&lt;/h2&gt;

&lt;p&gt;To alleviate the issue, we really needed a way to perform this iteration
without causing so much locking overhead to the performance critical
code paths (e.g. during a ARC cache hit). The solution we decided on is
to split each of the ARC&amp;rsquo;s internal lists into a variable number of
sublists, each sublist having its own lock and maintaining a time
ordering within itself (i.e. the buffers in each sublist are ordered
based on access time, but there&amp;rsquo;s no ordering of buffers between the
different sublists). Thus, a new data structure was created to
encapsulate these semantics, a &amp;ldquo;multilist&amp;rdquo; [&lt;a href=&#34;#appendix-4&#34;&gt;Appendix 4&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;On the surface this seems like a pretty trivial change, but there&amp;rsquo;s a
key aspect that make it more complex to implement than one might expect.
Due to the sublists not maintaining a strict ordering between each
other, selecting the least (or most) recently accessed buffer is no
longer a constant-time operation. In general, it becomes a linear-time
operation as each sublist must be iterated, comparing the elements in
each sublists with each other.&lt;/p&gt;

&lt;p&gt;A linear-time lookup of the least recently accessed buffer is
insufficient, so a clever domain-specific workaround was used to allow
for this lookup to maintain its constant-time complexity. Rather than
selecting the absolute least recently accessed buffer during eviction,
a buffer that is &amp;ldquo;old enough&amp;rdquo; is selected instead; this selection
happening in constant-time.&lt;/p&gt;

&lt;p&gt;We do this by evenly inserting buffers into each sublist, and then
during eviction, a randomly selected sublist is chosen. This sublist&amp;rsquo;s
least recently accessed buffer is selected and evicted. This process
will select a buffer that&amp;rsquo;s &amp;ldquo;old enough&amp;rdquo; based on the fact that buffers
are evenly inserted into each of the sublists, so each sublist&amp;rsquo;s least
recently accessed buffer will have an access time proportional to all
other sublists&amp;rsquo; least recently accessed buffer [&lt;a href=&#34;#appendix-5&#34;&gt;Appendix
5&lt;/a&gt;].&lt;/p&gt;

&lt;h2 id=&#34;performance-after&#34;&gt;Performance: After&lt;/h2&gt;

&lt;p&gt;As with any performance related change, empirical results are necessary
to give the change any sort of credibility. So, the workload that was
previously used to exemplify the problem [&lt;a href=&#34;#appendix-1&#34;&gt;Appendix 1&lt;/a&gt;],
was used again to ensure the expected performance gains were achieved.
The following table contains aggregate bandwidth numbers with the fix in
place (&amp;ldquo;Bandwidth After&amp;rdquo; column), as well as the aggregate bandwidth
numbers without the fix (&amp;ldquo;Bandwidth Before&amp;rdquo; column, the same results
previously shown):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;# of CPUS&lt;/th&gt;
&lt;th&gt;Bandwidth After&lt;/th&gt;
&lt;th&gt;Bandwidth Before&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;833163 KB/s&lt;/td&gt;
&lt;td&gt;840893 KB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1520.2 MB/s&lt;/td&gt;
&lt;td&gt;1559.5 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2985.3 MB/s&lt;/td&gt;
&lt;td&gt;3022.1 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;5913.2 MB/s&lt;/td&gt;
&lt;td&gt;5104.9 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;8938.9 MB/s&lt;/td&gt;
&lt;td&gt;3854.3 MB/s&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;9896.6 MB/s&lt;/td&gt;
&lt;td&gt;3036.2 MB/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As expected, performance scales much better as more CPUs are added to
the system, and the overall performance saw a 225% improvement when
using 32 CPUs (the largest CPU count I had available to test). Also,
the performance difference when using a small number of CPUs was
negligible.&lt;/p&gt;

&lt;p&gt;Lockstat was used again, and contention is clearly much less than before
(as before, this snippet was taken from a run on a system with 32 CPUs):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Count indv cuml rcnt     nsec Lock                   Caller
------------------------------------------------------------------------------
142396   3%   3% 0.00     2785 0xffffff09168112b0     rrw_exit+0x23
136499   3%   6% 0.00     3012 0xffffff09168112b0     rrw_enter_read+0x2
 82009   2%   8% 0.00     3359 0xffffff09168110d0     rrw_enter_read+0x27
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success!&lt;/p&gt;

&lt;h2 id=&#34;commit-details&#34;&gt;Commit Details&lt;/h2&gt;

&lt;p&gt;For those interested, this fix was committed to illumos on January 12th,
2015 via this commit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;commit 244781f10dcd82684fd8163c016540667842f203
Author:     Prakash Surya
AuthorDate: Mon Jan 12 19:52:19 2015 -0800
Commit:     Christopher Siden
CommitDate: Mon Jan 12 19:52:19 2015 -0800

    5497 lock contention on arcs_mtx
    Reviewed by: George Wilson
    Reviewed by: Matthew Ahrens
    Reviewed by: Richard Elling
    Approved by: Dan McDonald
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s also in the process of being ported and merged into FreeBSD and
OpenZFS on Linux (at least, it was at the time of this writing).&lt;/p&gt;

&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a name=&#34;appendix-1&#34;&gt;&lt;/a&gt;The FIO script used to run the performance
benchmarks is below. Keep in mind, this script was run a number of
times prior to gathering the performance data, as I was specifically
targeting a fully cached workload and needed to ensure the files
were cached in the ARC.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[global]
group_reporting=1
fallocate=none
ioengine=psync
numjobs=32
iodepth=1
bs=8k
filesize=512m
size=512m
randrepeat=0
use_os_rand=1

[32threads]
rw=randread
time_based
runtime=1m
directory=/tank/fish
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a name=&#34;appendix-2&#34;&gt;&lt;/a&gt;In addition to the run used to gather the
aggregate bandwidth numbers, the FIO script was run a couple more
times to gather more useful debugging and performance metrics.
Specifically, it was run with lockstat profiling enabled, which was
used to pinpoint the lock(s) causing the poor scaling. As well as
another iteration with flame graph profiling enabled to pinpoint the
specific functions of interest (additionally to provide
corroborating evidence to back up the lockstat data). Unfortunately,
it looks like I can&amp;rsquo;t upload SVG or TAR files; so if anybody is
interested in the full lockstat output or the flamegraphs, drop me
an email and we can work something out (probably just email the TAR
archive directly as it&amp;rsquo;s only 227K compressed).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a name=&#34;appendix-3&#34;&gt;&lt;/a&gt;The lists can also be iterated starting
from the most recently accessed buffer to the least recently
accessed buffer too. This iteration direction is used when
populating the L2ARC device prior to the cache being &amp;ldquo;warmed up&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a name=&#34;appendix-4&#34;&gt;&lt;/a&gt;This concept is not original to me, as
FreeBSD has included a similar fix to the ARC in their platform for
some time. FreeBSD&amp;rsquo;s usage of this concept actually gave me
confidence that it would work out in practice, prior to actually
having a working solution.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a id=&#34;appendix-5&#34;&gt;&lt;/a&gt;I realize &amp;ldquo;old enough&amp;rdquo; is vague, but
essentially what we&amp;rsquo;re doing here is using a heuristic to determine
what buffer would be the best candidate to be evicted from the
cache. So, we&amp;rsquo;re modifying the previous heuristic from always
selecting the absolutely oldest buffer, to being a randomly selected
buffer out of a group of &amp;ldquo;old&amp;rdquo; buffers. The group of old buffers
being composed of the tails of all the sublists.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>OpenZFS Developer Summit 2014: OpenZFS on illumos</title>
      <link>https://www.prakashsurya.com/post/2015-01-06-openzfs-developer-summit-2014-openzfs-on-illumos/</link>
      <pubDate>Tue, 06 Jan 2015 00:00:00 -0800</pubDate>
      
      <guid>https://www.prakashsurya.com/post/2015-01-06-openzfs-developer-summit-2014-openzfs-on-illumos/</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;http://open-zfs.org/&#34;&gt;OpenZFS&lt;/a&gt; project is growing! The &lt;a href=&#34;http://www.open-zfs.org/wiki/OpenZFS_Developer_Summit_2014&#34;&gt;second annual OpenZFS developer
summit&lt;/a&gt; concluded just under two months ago, and overall I thought it
went very well. There was roughly 70 attendees, twice as many as the
previous year, and the talks given were very engaging and interesting.&lt;/p&gt;

&lt;p&gt;I gave a short talk about ZFS on the &lt;a href=&#34;http://www.illumos.org/&#34;&gt;illumos&lt;/a&gt; platform, and also
touched briefly on some of my subjective opinions coming from a
&lt;a href=&#34;http://zfsonlinux.org/&#34;&gt;ZFS on Linux&lt;/a&gt; background. While a video recording of &lt;a href=&#34;http://youtu.be/UtlGt3ag0o0&#34;&gt;my talk&lt;/a&gt; is
available, I thought it&amp;rsquo;d be good to also present some of that in written
format, which might be a more approachable format for those who weren&amp;rsquo;t
able to attend the conference. So here goes&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;openzfs-history&#34;&gt;OpenZFS History&lt;/h2&gt;

&lt;p&gt;As most readers will already know, ZFS was originally designed and
developed at Sun Microsystems starting around 2001. It was later
released as open source in 2005 via OpenSolaris. In 2010 illumos was
born as a fork of OpenSolaris, and in 2013 the OpenZFS was created.&lt;/p&gt;

&lt;p&gt;As a result of this lineage, OpenZFS really feels &amp;ldquo;at home&amp;rdquo; within the
illumos project; and what I mean by that, is it has a very polished
integration with the illumos operating system and the code base as a
whole. Just to state a couple explicit examples of this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s very easy to boot and run ZFS as the root filesystem (even in
the face of on-disk feature changes) as it and the boot loader both
live and change within the same repository.&lt;/li&gt;
&lt;li&gt;The debugging tools on illumos (e.g. mdb) have knowledge of
ZFS-specific structures and have ZFS-specific features to allow
efficient debugging and development.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But, as OpenZFS is growing, its integration on other operating systems
is continually getting better.&lt;/p&gt;

&lt;h2 id=&#34;openzfs-on-illumos-development-process&#34;&gt;OpenZFS on illumos Development Process&lt;/h2&gt;

&lt;p&gt;The development model for OpenZFS on illumos follows the same
conventions as all other illumos development. Access to commit to the
repository is granted to &amp;ldquo;advocates&amp;rdquo;, and these advocates rely on the
community (possibly themselves as well) to review any proposed changes,
and ensure the changes are correct and are of high quality. In practice,
each proposed change must reviewed and approved by a subject matter
expert (e.g. for OpenZFS, this is typically Matt Ahrens or George
Wilson) prior to an advocate committing the change.&lt;/p&gt;

&lt;p&gt;Since the illumos project doesn&amp;rsquo;t have any specific releases, this
implies that each commit to the repository must be of high quality as
each commit is effectively a &amp;ldquo;release&amp;rdquo;. There isn&amp;rsquo;t the option of
merging code known to be defective, with the intention of fixing the
defect in a follow up commit prior to the &amp;ldquo;next release&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;openzfs-on-illumos-collaboration&#34;&gt;OpenZFS on illumos Collaboration&lt;/h2&gt;

&lt;p&gt;Since OpenZFS on illumos is the &amp;ldquo;upstream&amp;rdquo; platform for OpenZFS, it&amp;rsquo;s
important that there is open communication and collaboration between
the illumos developers and developers of the other platforms. As
changes from one platform are merged into illumos, all other platforms
will (ideally) pull these changes into their own version of OpenZFS
which is mutually beneficial for a number of reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Less time spent on merge conflicts when pulling changes in between
OpenZFS platforms. If each platform is active in pushing to illumos
and pulling from illumos, then we&amp;rsquo;ll all be using as close to the
same source code as possible.&lt;/li&gt;
&lt;li&gt;Less duplication of effort. There&amp;rsquo;s been a number of times where
I&amp;rsquo;ve personally spent effort investigating or solving a specific
problem, only to find out that it&amp;rsquo;s already been solved on another
OpenZFS platform. If each platform was more actively kept in sync
with each other, this situation would be far less likely.&lt;/li&gt;
&lt;li&gt;More varied testing. If all platforms are running the same code,
then we all benefit from the variety of workloads each platform
brings to the table; and as a result, we all test each others&amp;rsquo; code.&lt;/li&gt;
&lt;li&gt;Proliferation of new OpenZFS features. Currently, most new ZFS
features are first developed on, and committed to illumos. Thus, if
other platforms are active in pushing changes into illumos, these
new features are inherently developed around and incorporate the
most recent changes in each platform. This makes it much easier to
pull the new features into the downstream platforms, allowing all
OpenZFS consumers to benefit from the new features as soon as
possible.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to achieve this goal, the process of building and testing
changes to the OpenZFS on illumos code base needs to be simplified and
streamlined. Work is on-going on this front, and my hope is that the
necessary improvements will happen over the next year, and we&amp;rsquo;ll be
able to sing a different tune for next year&amp;rsquo;s developer summit.&lt;/p&gt;

&lt;h2 id=&#34;developer-debugging-tools-on-illumos&#34;&gt;Developer/Debugging Tools on illumos&lt;/h2&gt;

&lt;p&gt;One of the nice things about OpenZFS development on illumos is that I
get to make use of the outstanding developer tools available on the
platform. Having spent much of the last few years working within the
Linux kernel, I&amp;rsquo;ve been implicitly trained to rely almost entirely on
the source code and stack traces for debugging most production and
development problems. Now that I&amp;rsquo;ve jumped to illumos, I can see
first hand what I&amp;rsquo;ve been missing with tools like dtrace, kmdb,
and &lt;strong&gt;especially&lt;/strong&gt; mdb.&lt;/p&gt;

&lt;p&gt;In my short time working on the illumos platform, I&amp;rsquo;ve found mdb to be
easily one of the best parts about the jump from ZFS on Linux. Certain
tasks that would taken hours, or have been prohibitively difficult, on
Linux can be done with ease using mdb&amp;rsquo;s dcmds and pipelines.&lt;/p&gt;

&lt;p&gt;For example, while working on the ZFS on Linux port I would often be
puzzled by what I found when looking at the ARC kstats. To try and make
some sense of it, I wanted to know exactly what was contained in the ARC
and what was contained in the dbuf cache; this eventually led to the
implementation of the &amp;ldquo;dbufs&amp;rdquo; proc handler on the Linux port. On
illumos, there&amp;rsquo;s already a mdb dcmd to do exactly that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Lines ending with &#39;...&#39; have been truncated to fit in 80 columns.
$ sudo mdb -k
&amp;gt; ::dbufs | ::dbuf ! head
        addr object lvl blkid holds os
ffffff160e924b10     114f 0        19  0 rpool/versions/2014.12.2014.1 ...
ffffff160e924c30     1c50 0        35  0 rpool/versions/2014.12.2014.1 ...
ffffff160e924d50     1c4d 0        33  0 rpool/versions/2014.12.2014.1 ...
ffffff160e9092f8     164b 0        24  0 rpool/versions/2014.12.2014.1 ...
ffffff160e8cc050       a0 0         3  0 mos
ffffff160e8c04e0     177b 0        3a  0 rpool/versions/2014.12.2014.1 ...
ffffff160e8bcde8     1c4d 0        1a  0 rpool/versions/2014.12.2014.1 ...
ffffff160e8b2bb8       74 0         0  0 mos
ffffff160e8b2cd8     1c4c 0         f  0 rpool/versions/2014.12.2014.1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And better yet, I can print the full contents for each buffer if that
level of detail is needed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -k
&amp;gt; ::dbufs | ::print dmu_buf_impl_t ! head
{
    db = {
        db_object = 0x114f
        db_offset = 0x320000
        db_size = 0x20000
        db_data = 0xffffff012e283000
    }
    db_objset = 0xffffff096dd82500
    db_dnode_handle = 0xffffff160e858d20
    db_parent = 0xffffff160e63cb70
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or filter out the specific dbufs of interest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Lines ending with &#39;...&#39; have been truncated to fit in 80 columns.
$ sudo mdb -k
&amp;gt; ::dbufs -o 0x90 | ::dbuf ! head
        addr object lvl blkid holds os
ffffff1609deb210       90 0         1  0 mos
ffffff0980966790       90 0         2  0 mos
ffffff0982633030       90 0         0  0 mos
ffffff096e53c9a8       90 1         0  3 mos
ffffff15edcd7bf0       90 0     bonus  1 tank/fish
ffffff097340dab8       90 0     bonus  1 rpool/versions/2014.12.2014.1 ...
ffffff096e118450       90 0         0  0 rpool/versions/2014.12.2014.1 ...
ffffff09179a79e8       90 0     bonus  1 mos
ffffff09157b6cc0       90 0     bonus  0 rpool/versions/2014.12.2014.1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But what if one wants to filter on something that isn&amp;rsquo;t supported by the
&lt;code&gt;::dbufs&lt;/code&gt; command? Well, here&amp;rsquo;s a more general approach, filtering out
only the buffers with a reference count of 3, using &lt;code&gt;::grep&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Lines ending with &#39;...&#39; have been truncated to fit in 80 columns.
# Also, mdb doesn&#39;t actually support continuation of input lines
# using the &amp;quot;\&amp;quot; character; but it helps me format the command within
# 80 columns for this post, and resembles a bash line continuation.

$ sudo mdb -k
&amp;gt; ::dbufs | ::print dmu_buf_impl_t db_holds.rc_count | \
  ::grep &#39;.==0x3&#39; | ::eval &amp;amp;lt;dbuf=K | ::dbuf ! head
        addr object lvl blkid holds os
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
ffffff1607afe080      5f2 1         0  3 rpool/versions/2014.12.2014.1 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, the ability to get stack traces detailing where objects
were allocated using &lt;code&gt;kmem_flags=0x1&lt;/code&gt; is hugely beneficial. Using it,
things like the &lt;code&gt;::refcount&lt;/code&gt; dcmd can not only provide information about
the current number of holds, but it can also provide the full kernel
stack trace of the thread that took the hold when the hold was taken.&lt;/p&gt;

&lt;p&gt;Likewise, the &lt;code&gt;::whatis&lt;/code&gt; dcmd also makes use of this information and
displays it to the user. Imagine a case where you have a &amp;ldquo;random&amp;rdquo;
pointer to an object (e.g. a pointer to an &lt;code&gt;arc_buf_t&lt;/code&gt;), and wanted to
know where this object came from. If using &lt;code&gt;kmem_flags=0x1&lt;/code&gt; on illumos,
this is as simple as the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mdb -k
&amp;gt; ::walk dmu_buf_impl_t ! head -n 1
0xffffff0ab4001048
&amp;gt; 0xffffff0ab4001048::whatis
ffffff0ab4001048 is allocated from dmu_buf_impl_t:
            ADDR          BUFADDR        TIMESTAMP           THREAD
                            CACHE          LASTLOG         CONTENTS
ffffff0ab4059128 ffffff0ab4001048      157ca42dbaf ffffff09e088e440
                 ffffff0958a5a008 ffffff090b0ebbc0 ffffff0932c80430
                 kmem_cache_alloc_debug+0x2e0
                 kmem_cache_alloc+0xdd
                 dbuf_create+0x79
                 dbuf_hold_impl+0x192
                 dbuf_hold+0x27
                 dmu_buf_hold_array_by_dnode+0x129
                 dmu_read_uio_dnode+0x5a
                 dmu_read_uio_dbuf+0x51
                 zfs_read+0x1a3
                 fop_read+0x5b
                 pread64+0x25d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I could go on and on about the cool things mdb allows, but this post is
already long enough, so I&amp;rsquo;ll cut myself off here. It would be great to
see an mdb port to Linux! (or even a more feature rich version of
&lt;a href=&#34;http://people.redhat.com/anderson/crash_whitepaper/&#34;&gt;crash&lt;/a&gt; with OpenZFS support)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>